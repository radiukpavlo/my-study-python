# Еквіваріантні матриці переходу для пояснюваного глибокого навчання: підхід на основі лінеаризації груп Лі

**Автори:** Ім’я Прізвище $^{1}$, Ім’я Прізвище $^{2}$ та Ім’я Прізвище $^{1,*}$

$^{1}$ Афіліація 1; e-mail@e-mail.com
$^{2}$ Афіліація 2; e-mail@e-mail.com

**Кореспонденція:** e-mail@e-mail.com; Тел.: +xx-xxxx-xxx-xxxx

---

**Анотація:** Сучасні глибокі нейронні мережі часто вивчають представлення, варіативність яких структурована неперервними симетріями (наприклад, обертаннями), проте більшість конвеєрів post-hoc пояснюваності транслюють латентні ознаки у зрозумілі людині фактори без забезпечення узгодженості із симетрією. Така невідповідність може робити пояснення нестабільними: малі, семантично незначущі перетворення вхідних даних можуть призводити до непропорційних змін у сурогатному поясненні. У цій роботі ми пропонуємо еквіваріантну матрицю переходу (ЕМП), яка відображає простір ознак формальної моделі у простір ознак ментальної моделі, при цьому наближено сплітаючи індуковані дії груп Лі. Метод лінеаризує дії груп за допомогою емпірично оцінених генераторів алгебри Лі та розв’язує задачу зважених найменших квадратів, що балансує точність відтворення з еквіваріантністю. На контрольованому синтетичному тесті запропонована ЕМП зменшує дефект симетрії з $1.31\times 10^{4}$ до $4.25\times 10^{-2}$ (зменшення у $3.1\times 10^{5}$ разів) при помірному зростанні помилки реконструкції з $3.67\times 10^{-3}$ до $5.24\times 10^{-3}$ (середньоквадратична похибка). При реконструкції зображень з ознак на наборі даних MNIST, ЕМП зменшує помилку симетрії Фробеніуса зі 141.19 до 38.65, зберігаючи якість реконструкції (середній SSIM на тестовій вибірці 0.6978 проти 0.6976; середній PSNR 18.49 дБ проти 18.48 дБ). Ці результати свідчать про те, що забезпечення локальної еквіваріантності дозволяє отримувати пояснення, які є вимірно більш узгодженими при перетвореннях, без суттєвого погіршення інтерпретованості через втрату точності реконструкції.

**Ключові слова:** еквіваріантність; матриця переходу; пояснюваний штучний інтелект; групи Лі; генератори алгебри Лі; сингулярний розклад матриці; геометричне глибоке навчання

---

## 1. Вступ

Глибоке навчання стало вибором за замовчуванням для задач сприйняття та прийняття рішень у відповідальних сферах, включаючи медичну візуалізацію, аналіз біосигналів, криміналістику та автономні системи. Попри вражаючу точність прогнозування, внутрішня логіка глибоких моделей, як правило, залишається непрозорою для кінцевих користувачів і навіть для розробників, що підриває довіру, підзвітність та впровадження у регульованих умовах. Пояснюваний штучний інтелект (XAI) заповнює цю прогалину, створюючи артефакти, які допомагають людям пов’язати поведінку формальної моделі з дієвою ментальною моделлю, надаючи пояснення, які можна перевірити, поставити під сумнів та піддати аудиту [1-3].

У літературі з XAI простежується практична напруга. З одного боку, багато популярних методів наголошують на локальному поясненні: вони пояснюють окреме передбачення, апроксимуючи модель інтерпретованим сурогатом навколо точки інтересу, або приписуючи значущість вхідним ознакам. Ці підходи є корисними, але вони часто ігнорують глобальну структуру розподілу даних та геометрію представлення, яку вивчили глибокі мережі [2,3]. З іншого боку, навчання глибоких представлень регулярно використовує глобальну структуру, таку як симетрії: перетворення вхідних даних (наприклад, обертання об'єкта, зсув на зображенні або зміна точки огляду) викликають передбачувані зміни у внутрішніх ознаках. Геометричне глибоке навчання формалізує цю ідею, описуючи, як дані існують у просторах із діями груп, та забезпечуючи еквіваріантність в архітектурах моделей [4]. Якщо представлення організоване групою симетрії, пояснення, що ігнорують цю організацію, можуть стати крихкими.

Ця робота відштовхується від конкретного механізму пояснюваності: матриці переходу, що відображає глибокі ознаки у простір інтерпретованих ознак. У наших попередніх дослідженнях матриці переходу використовувалися як міст візуальної аналітики між глибокими мережами та інтерпретованими сурогатами, що уможливлювало дослідження глибоких представлень за участі людини [7]. Відповідне медичне застосування показало, що трансляція глибоких рішень у визначені експертами ознаки може давати клінічно значущі пояснення з високим рівнем узгодженості з експертними анотаціями [8]. Ці результати продемонстрували, що лінійна трансляція «ознака-в-ознаку» може бути ефективною для побудови глобальних пояснень рівня моделі, які залишаються легковаговими під час виконання (inference).

Однак лінійна матриця переходу, навчена лише шляхом мінімізації помилки реконструкції, неявно припускає, що зв'язок між простором ознак формальної моделі та простором ознак ментальної моделі є глобально лінійним або принаймні добре апроксимується фіксованим лінійним відображенням у робочій області. На практиці латентні простори глибоких мереж часто деформуються нелінійно під час перетворень, проте вони роблять це структурованим чином: деформація часто індукується дією базової групи на вхідні дані. Еквіваріантність — це твердження про те, що перетворення вхідних даних елементом групи призводить до передбачуваного перетворення у просторі ознак. Еквіваріантні згорткові мережі та архітектури, еквіваріантні до груп Лі, роблять цю структуру симетрії явною [5,6]. Натомість більшість post-hoc сурогатів не вимагають, щоб пояснення трансформувалися узгоджено при трансформації вхідних даних.

Центральна теза цієї статті полягає в тому, що матриця переходу повинна бути не лише точною як регресійне відображення від глибоких ознак до інтерпретованих ознак, але й структурно сумісною із симетріями, які організовують ці ознаки. У математичному формулюванні «сумісність» означає, що матриця переходу наближено сплітає (intertwines) представлення груп, індуковані у двох просторах ознак. Оскільки дія групи може бути нелінійною на рівні об'єкта, ми лінеаризуємо індуковані дії, використовуючи генератори алгебри Лі, які діють як локальні інфінітезимальні дескриптори симетрії в кожному просторі ознак.

Метою цього дослідження є підвищення надійності та внутрішньої узгодженості пояснень на основі матриць переходу шляхом включення обмежень еквіваріантності, отриманих через лінеаризацію груп Лі.

Для досягнення цієї мети ми зосереджуємося на задачі побудови єдиного глобального лінійного оператора, який (i) транслює ознаки формальної моделі в ознаки ментальної моделі з високою точністю відтворення та (ii) враховує, як обидва представлення змінюються при інфінітезимальних перетвореннях симетрії. Основний внесок цієї роботи є потрійним. По-перше, ми пропонуємо цільову функцію матриці переходу з урахуванням симетрії, яка поєднує точність реконструкції з принциповим регуляризатором еквіваріантності, вираженим через генератори алгебри Лі. По-друге, ми вводимо практичну процедуру оцінювання цих генераторів безпосередньо з даних за допомогою скінченних різниць при контрольованих малих перетвореннях, що уможливлює вирівнювання симетрії навіть тоді, коли перетворення на рівні об'єкта викликають нелінійні зміни ознак. По-третє, ми надаємо стійку лінійно-алгебраїчну стратегію розв'язання на основі векторизації, добутків Кронекера та сингулярного розкладу матриці (SVD) для обчислення оптимальної еквіваріантної матриці переходу в перевизначеному та зашумленому режимі [9].

## 2. Пов’язані роботи

XAI еволюціонував у широку екосистему методів, які надають пояснення моделей «чорної скриньки» через інтерпретовані сурогати, атрибуцію ознак, контрфактуальні приклади, методи на основі прикладів та концептуальні пояснення. Література наголошує, що пояснення є не лише обчислювальними об'єктами, а й соціальними та когнітивними артефактами: одна й та сама форма пояснення може бути корисною або оманливою залежно від користувача, завдання та контексту [1]. Оглядові статті підкреслюють повторювані відкриті проблеми, включаючи вірність (faithfulness), стабільність та складність оцінювання пояснень поза анекдотичними прикладами [2,3].

Ключовою темою цієї статті є стабільність: пояснення не повинні довільно змінюватися під впливом збурень, які люди вважають несуттєвими. У комп'ютерному зорі багато збурень відповідають перетворенням, що зберігають ідентичність об'єкта (наприклад, малі обертання). На практиці глибокі мережі можуть бути чутливими до таких перетворень, а методи пояснення можуть бути ще чутливішими, оскільки вони часто апроксимують локальну поведінку без дотримання глобальних обмежень. Це мотивує привнесення геометричної структури в XAI.

Геометричне глибоке навчання надає уніфіковану мову для моделювання даних із симетріями та неевклідовими областями, описуючи функції на многовидах, графах та просторах із діями груп [4]. Еквіваріантність є центральним поняттям: відображення $f$ є еквіваріантним відносно групи $G$, якщо $f(g\cdot x)=\rho(g)f(x)$ для представлення $\rho$ групи $G$. Архітектури, які жорстко кодують еквіваріантність, відомі тим, що покращують ефективність використання вибірки та узагальнення в умовах, де присутні симетрії. Групові еквіваріантні згорткові мережі ввели теоретико-групове розширення згортки, забезпечуючи еквіваріантність для дискретних або неперервних груп перетворень [5]. Подальші роботи узагальнили згорткові конструкції та розробили фреймворки для еквіваріантності до груп Лі на неперервних областях [6].

Хоча еквіваріантні архітектури накладають обмеження симетрії у прямій моделі, вони не дають автоматично пояснень, які були б інтерпретованими для експертів предметної області. У прикладних контекстах часто є навчена модель «чорної скриньки», і виникає потреба у post-hoc механізмі пояснення, який можна накласти зверху. Саме ця ситуація розглядається тут.

Матриці переходу для пояснюваності пропонують post-hoc глобальний сурогат: єдиний лінійний оператор транслює внутрішні ознаки у простір ознак із визначеною користувачем семантикою. В умовах візуальної аналітики цей оператор може пов'язувати кластери та траєкторії у просторі глибоких ознак з інтерпретованими патернами або з простішим простором моделі машинного навчання [7]. В охороні здоров'я цю ж ідею можна використовувати для трансляції рішення моделі в клінічно значущі дескриптори, що підтримує обґрунтування та комунікацію з медичними працівниками [8]. Обмеження полягає в тому, що такі матриці зазвичай підганяються виключно регресією, яка не гарантує, що пояснення залишатимуться узгодженими при перетвореннях.

У підсумку, мета цього дослідження — розробити відображення для post-hoc пояснення, яке зберігає не лише точність, але й геометричну структуру, індуковану симетріями даних. Для досягнення цієї мети робота вирішує три основні завдання: (1) формалізувати задачу переходу з урахуванням симетрії як спільну оптимізацію точності та еквіваріантності; (2) оцінити інфінітезимальні генератори симетрії як у формальному, так і в ментальному просторах ознак з трансформованих зразків; та (3) обчислити стійкий розв'язок за допомогою регуляризованих найменших квадратів на основі SVD, отримуючи єдину матрицю, яку можна застосовувати під час виконання.

## 3. Матеріали та методи

### 3.1. Позначення та постановка задачі

Нехай $X$ позначає простір вхідних даних, а $x\in X$ — екземпляр даних. Навчена глибока нейронна мережа (формальна модель) індукує екстрактор ознак $a:X\rightarrow \mathbb{R}^{k}$, який відображає вхідні дані у $k$-вимірний вектор латентних ознак. Для набору даних з $m$ зразків $\{x_j\}_{j=1}^{m}$ ми збираємо вилучені ознаки у матрицю

$$
A = \begin{bmatrix} a(x_1)^\top \\ \vdots \\ a(x_m)^\top \end{bmatrix} \in \mathbb{R}^{m\times k}.
$$

Паралельно ми визначаємо представлення ментальної моделі $b:X\rightarrow \mathbb{R}^{l}$, яке дає $l$ інтерпретованих ознак (наприклад, визначені експертами дескриптори, клінічно значущі вимірювання або фізично інтерпретовану систему координат). Ми збираємо ці вектори у матрицю

$$
B = \begin{bmatrix} b(x_1)^\top \\ \vdots \\ b(x_m)^\top \end{bmatrix} \in \mathbb{R}^{m\times l}.
$$

Матриця переходу $T\in \mathbb{R}^{l\times k}$ відображає ознаки формальної моделі в ознаки ментальної моделі. У матричній формі стандартна модель переходу, що базується лише на точності, має вигляд

$$
B \approx A T^{\top}.
$$

У найпростішому випадку $T$ знаходиться шляхом мінімізації $\|B - A T^{\top}\|_F^{2}$, що дає розв'язок методом найменших квадратів. У попередніх роботах це відображення використовувалося для post-hoc пояснення та візуальної аналітики [7,8]. Обмеженням моделі точності є те, що вона не накладає жодного структурного зв'язку між тим, як змінюються $a(x)$ та $b(x)$, коли трансформується вхід $x$.

### 3.2. Симетрія, еквіваріантність та умова сплітання

Ми припускаємо, що група Лі $G$ діє локально на $X$ (наприклад, обертання, що діють на зображення): $G\curvearrowright X$, $(g,x)\mapsto g\cdot x$. Далі ми припускаємо, що два представлення є наближено еквіваріантними до цієї дії. Конкретно, існують (можливо, наближені) лінійні представлення $\rho_A:G\rightarrow GL(k)$ та $\rho_B:G\rightarrow GL(l)$ такі, що

$$
a(g\cdot x) \approx \rho_A(g)\,a(x),\qquad b(g\cdot x) \approx \rho_B(g)\,b(x).
$$

Еквіваріантність — це структурне твердження: перетворення вхідних даних індукують структуровані перетворення ознак, а не довільні зміни. Мовою теорії представлень лінійне відображення $T$ є *оператором сплітання* (intertwiner) між представленнями $\rho_A$ та $\rho_B$, якщо

$$
T\,\rho_A(g) = \rho_B(g)\,T\qquad \forall g\in G.
$$

У контексті пояснюваності ця умова виражає природну вимогу: якщо вхідні дані трансформуються симетрією, і ознаки формальної моделі трансформуються відповідним чином, то трансльовані ознаки ментальної моделі повинні трансформуватися відповідним чином. Якщо ця умова порушується, пояснення можуть бути неузгодженими: два вхідні дані, пов'язані симетрією, можуть давати виходи ментальної моделі, які не пов'язані цією симетрією.

Безпосередньо забезпечити цю умову важко, оскільки $\rho_A(g)$ та $\rho_B(g)$ можуть бути невідомі аналітично і можуть реалізовуватися лише наближено. Це мотивує локальний диференціальний підхід.

### 3.3. Лінеаризація алгебри Лі

Нехай $\mathfrak{g}$ — алгебра Лі групи $G$ з базисом $\{\xi_i\}_{i=1}^{r}$, де $r=\dim \mathfrak{g}$. Диференціал (представлення алгебри Лі) $\rho_A$ в одиниці дає лінійне відображення $d\rho_A: \mathfrak{g}\rightarrow \mathbb{R}^{k\times k}$; ми позначаємо його матрицю на $\xi_i$ як $J_i^{A}=d\rho_A(\xi_i)$. Аналогічно, $J_i^{B}=d\rho_B(\xi_i)\in \mathbb{R}^{l\times l}$.

Для малого параметра $\varepsilon$, елемент групи $\exp(\varepsilon\xi_i)$ близький до одиниці, і індуковане перетворення ознак апроксимується як

$$
\rho_A\big(\exp(\varepsilon\xi_i)\big) \approx I_k + \varepsilon J_i^{A}, \qquad \rho_B\big(\exp(\varepsilon\xi_i)\big) \approx I_l + \varepsilon J_i^{B}.
$$

Підставивши це в умову сплітання і залишивши члени першого порядку по $\varepsilon$, отримуємо *лінеаризовану умову сплітання*

$$
T J_i^{A} \approx J_i^{B} T,\qquad i=1,\dots,r.
$$

Ця умова є привабливою для post-hoc пояснюваності, оскільки вона залежить лише від генераторів $J_i^{A}$ та $J_i^{B}$, а не від повних представлень групи.

### 3.4. Оцінювання інфінітезимальних генераторів з даних

У багатьох застосуваннях $J_i^{A}$ та $J_i^{B}$ недоступні аналітично. Ми оцінюємо їх емпірично, вимірюючи, як змінюються вектори ознак при малих перетвореннях вхідних даних. Для кожного зразка $x_j$ і кожного напрямку генератора $\xi_i$ ми будуємо трансформовану копію

$$
x_{j,i}^{(\varepsilon)} = \exp(\varepsilon\xi_i)\cdot x_j,
$$

обчислюємо відповідні ознаки та апроксимуємо похідну за напрямком за допомогою скінченних різниць,

$$
\Delta a_{j,i} \approx \frac{a\big(x_{j,i}^{(\varepsilon)}\big)-a(x_j)}{\varepsilon},\qquad \Delta b_{j,i} \approx \frac{b\big(x_{j,i}^{(\varepsilon)}\big)-b(x_j)}{\varepsilon}.
$$

Припускаючи локальну лінійність у просторі ознак, ми підганяємо матриці генераторів так, щоб

$$
\Delta a_{j,i} \approx J_i^{A} a(x_j),\qquad \Delta b_{j,i} \approx J_i^{B} b(x_j).
$$

Для фіксованого $i$, об'єднання співвідношень для всіх $j$ дає задачі лінійної регресії $A J_i^{A\top}\approx \Delta A_i$ та $B J_i^{B\top}\approx \Delta B_i$. У синтетичному експерименті (Розділ 4.1.1) ми не маємо справжнього простору вхідних даних $X$ для застосування перетворень. Натомість ми будуємо геометричний проксі: вкладаємо хмару точок у $\mathbb{R}^{2}$ за допомогою багатовимірного шкалювання (MDS), застосовуємо мале плоске обертання і навчаємо лінійний «декодер» з 2D-вкладення назад у початковий простір ознак. Це дає обернені матриці ознак і дозволяє таке ж оцінювання генераторів методом скінченних різниць.

В експериментах з MNIST (Розділ 4.1.2) ми використовуємо явні обертання зображень. Ментальна модель $b(x)$ є вектором пікселів (розгорнуте зображення), тому $J^{B}$ описує, як змінюються пікселі при інфінітезимальному обертанні. Ознаки формальної моделі $a(x)$ витягуються з передостаннього повнозв'язного шару згорткової нейронної мережі, і $J^{A}$ відображає індукований інфінітезимальний ефект обертання на ці глибокі ознаки.

#### 3.4.1. Вибір кроку перетворення та зменшення чисельного шуму

Оцінка методом скінченних різниць є надійною лише тоді, коли розмір кроку $\varepsilon$ знаходиться в режимі «Золотоволоски»: достатньо малий, щоб виправдати локальну лінеаризацію, але достатньо великий, щоб різниця $a(\exp(\varepsilon\xi_i)\cdot x)-a(x)$ не домінувалася чисельним шумом. У реальних конвеєрах накопичуються кілька джерел шуму: артефакти інтерполяції при геометричних перетвореннях зображень, стохастичність при завантаженні даних та ядрах GPU (якщо присутні), а також той факт, що екстрактор ознак не є ідеально еквіваріантним. Якщо $\varepsilon$ занадто мале, чисельник близький до машинної точності, і відношення стає нестабільним; якщо воно занадто велике, скінченна різниця стає нахилом січної через сильно нелінійну область, і отриманий $J$ більше не відображає інфінітезимальний генератор.

На практиці вибір розміру кроку не є суто деталлю чисельного аналізу; він змінює семантику накладеної еквіваріантності. Генератори $J_i^{A}$ та $J_i^{B}$ визначають, що ми розуміємо під «узгодженими» локальними перетвореннями у двох просторах ознак. Для обертання зображень $\varepsilon$ має бути малим порівняно з пікселем, щоб уникнути ефектів аліасингу, але не настільки малим, щоб обернене зображення стало невідрізним від оригінального після передискретизації. Для глибоких ознак крок повинен бути достатньо великим, щоб викликати вимірну реакцію в активаціях передостаннього шару. Надійна евристика полягає в тому, щоб перебрати $\varepsilon$ у короткому логарифмічному діапазоні (наприклад, від $10^{-4}$ до $10^{-2}$ радіан для обертань) і вибрати значення, яке дає стабільну підгонку генератора, що вимірюється нормами залишків $\|\Delta A_i - A J_i^{A\top}\|_F$ та $\|\Delta B_i - B J_i^{B\top}\|_F$.

Два додаткові стабілізатори є корисними, коли генератори оцінюються з даних, а не виводяться аналітично. По-перше, ми можемо використовувати центральні різниці замість прямих; це скасовує помилку усічення першого порядку і зазвичай покращує симетрію генератора. По-друге, ми можемо регуляризувати задачі регресії для $J_i^{A}$ та $J_i^{B}$, наприклад, за допомогою малого гребеневого штрафу (ridge penalty). Це не змінює концептуального визначення еквіваріантності, але гасить помилкові високочастотні моди, які з'являються, коли $A$ або $B$ мають майже колінеарні напрямки. У поточних експериментах ми використовуємо схему центральних різниць та підгонку методом найменших квадратів, покладаючись на подальшу регуляризацію SVD/LSQR на етапі оцінювання $T$ для поглинання залишкового шуму.

#### 3.4.2. Більше ніж один генератор: багатопараметричні групи та некомутативність

Хоча експерименти зосереджені на однопараметричній дії обертання (алгебра Лі $\mathfrak{so}(2)$), формулювання природним чином поширюється на багатопараметричні групи Лі, такі як $SE(2)$ (плоскі обертання-зсуви), $SIM(2)$ (обертання-зсуви з однорідним масштабуванням) або специфічні для застосування неперервні сімейства перетворень. У таких випадках ми працюємо з базисом $\{\xi_i\}_{i=1}^{r}$ алгебри Лі $\mathfrak{g}$ і оцінюємо пару генераторів $\{J_i^{A},J_i^{B}\}_{i=1}^{r}$. Обмеження еквіваріантності тоді стають набором лінійних матричних рівнянь $T J_i^{A}\approx J_i^{B} T$ для всіх $i$.

Один нюанс полягає в тому, що коли $r>1$, генератори не є незалежними: алгебра Лі характеризується не лише своїм базисом, але й комутаційними співвідношеннями $[\xi_i,\xi_j]=\sum_k c_{ij}^{k}\,\xi_k$. У точній теорії представлень ці співвідношення означають, що матриці повинні задовольняти $[J_i^{A},J_j^{A}]=\sum_k c_{ij}^{k} J_k^{A}$ (і аналогічно для $J^{B}$). Емпіричне оцінювання зі скінченних даних не гарантує такого замикання, особливо коли базові відображення ознак є лише наближено еквіваріантними. Важливо, що наша оптимізація не вимагає ідеального замикання. Обмеження можна інтерпретувати як регуляризатор, який заохочує навчений транслятор $T$ вирівнювати домінуючі напрямки симетрії, навіть якщо оцінені генератори порушують комутаційні співвідношення в малому масштабі.

Якщо потрібні сильніші структурні гарантії, можливі два безпосередні розширення. По-перше, можна явно спроектувати набір емпіричних генераторів на найближчий набір, сумісний з алгеброю Лі (наприклад, розв'язавши задачу найменших квадратів з обмеженнями на структурні константи). По-друге, можна додати штрафи за узгодження комутаторів, такі як $\sum_{i<j}\|T[J_i^{A},J_j^{A}] - [J_i^{B},J_j^{B}]T\|_F^{2}$, що вимагає узгодження також і інфінітезимальної поведінки вищого порядку. Ці розширення не є необхідними для підтвердження концепції в цій статті, але вони пояснюють, як запропонований фреймворк масштабується на багатші сімейства симетрій.

### 3.5. Оптимізація еквіваріантної матриці переходу

Ми шукаємо єдину матрицю $T$, яка задовольняє як точність відтворення, так і наближене сплітання. Ми формалізуємо це як мінімізацію комбінованої цільової функції,

$$
\mathcal{L}(T) = \|B^{\top}-T A^{\top}\|_F^{2} + \lambda\sum_{i=1}^{r} \|T J_i^{A} - J_i^{B} T\|_F^{2},
$$

де $\lambda\ge 0$ контролює компроміс між точністю та еквіваріантністю. Коли $\lambda=0$, цільова функція зводиться до стандартної матриці переходу за методом найменших квадратів. Зі збільшенням $\lambda$ розв'язок стає все більш обмеженим вимогою дотримання локальної структури симетрії.

Цільова функція є квадратичною відносно елементів $T$, тому її можна розв'язати лінійним методом найменших квадратів після векторизації. Нехай $u=\mathrm{vec}(T)\in \mathbb{R}^{kl}$ — вектор, отриманий шляхом укладання стовпців $T$. Використовуючи тотожність $\mathrm{vec}(AXB)=(B^{\top}\otimes A)\mathrm{vec}(X)$, умова точності $B^{\top}\approx T A^{\top}$ набуває вигляду

$$
(A\otimes I_l)\,u \approx \mathrm{vec}(B^{\top}).
$$

Кожна умова сплітання $T J_i^{A} - J_i^{B} T\approx 0$ стає

$$
\big((J_i^{A})^{\top}\otimes I_l - I_k\otimes J_i^{B}\big)\,u \approx 0.
$$

Об'єднання рівнянь точності та зважених обмежень дає перевизначену лінійну систему

$$
M u \approx y,
$$

де

$$
M = \begin{bmatrix}
A\otimes I_l\\
\lambda K_1\\
\vdots\\
\lambda K_r
\end{bmatrix},\qquad
y = \begin{bmatrix}
\mathrm{vec}(B^{\top})\\
0\\
\vdots\\
0
\end{bmatrix},\qquad
K_i=(J_i^{A})^{\top}\otimes I_l - I_k\otimes J_i^{B}.
$$

Розв'язок $u$ потім перетворюється назад у форму $T$. Як у синтетичному, так і в MNIST налаштуваннях матриця $M$ є великою і може бути погано обумовленою через кореляції в глибоких ознаках та надлишковість обмежень. Для чисельної стійкості ми розв'язуємо систему за допомогою псевдообернення на основі SVD з усіченням сингулярних значень [9]. Для масштабного налаштування MNIST можна використовувати ітеративний метод найменших квадратів (LSQR) з тим самим блочним формулюванням; наведені результати були отримані за допомогою супровідного коду на Python.

#### 3.5.1. Обчислювальна складність та практична масштабованість

Блочне формулювання концептуально просте, але може виглядати складним з обчислювальної точки зору, оскільки розмірності пов'язані з перехресним добутком двох просторів ознак. Для набору даних з $m$ зразків, формальної розмірності $k$ та ментальної розмірності $l$, блок точності вносить $ml$ рівнянь, тоді як кожне обмеження еквіваріантності вносить $kl$ рівнянь. Навіть для одного генератора загальна кількість рівнянь становить $ml + kl$, а для $r$ генераторів загальна кількість становить $ml + rkl$. Невідомий вектор $u = \mathrm{vec}(T)$ має $kl$ ступенів свободи, тому система зазвичай є перевизначеною, як тільки $m$ перевищує кілька зразків.

Два спостереження роблять цей підхід реалістичним на практиці. По-перше, лінійні оператори, що визначають блоки, мають багату структуру: $A \otimes I_l$ та $(J_i^{A})^\top \otimes I_l - I_k \otimes J_i^{B}$ є сумами/добутками Кронекера. Ця структура означає, що можна множити на ці оператори без явного формування повних щільних матриць, використовуючи тотожності, такі як $\mathrm{vec}(A X) = (I \otimes A)\,\mathrm{vec}(X)$ та $\mathrm{vec}(X B) = (B^\top \otimes I)\,\mathrm{vec}(X)$. Іншими словами, витрати можна перенести з маніпуляцій гігантською матрицею $M$ на застосування менших множень матриць, що включають $A$, $J^{A}$, $J^{B}$ та поточну ітерацію $T$.

По-друге, вибір розв'язувача повинен відповідати режиму. Для малих та середніх задач (включаючи синтетичний тест) явний SVD забезпечує надійний, глобально оптимальний розв'язок методом найменших квадратів та пропонує діагностику через спектр сингулярних значень. Для великомасштабних задач, таких як MNIST, де $k=490$ і $l=784$ вже означають $kl\approx 3.84\times 10^{5}$ невідомих, явний SVD повного оператора неможливий. У цьому режимі ітеративні методи підпростору Крилова, такі як LSQR, можуть застосовуватися безпосередньо до лінійного оператора, визначеного блоками, вимагаючи лише добутків матриця-вектор. З точки зору оптимізації це еквівалентно розв'язанню задачі найменших квадратів з регуляризацією Тихонова зі структурованим регуляризатором, де $\lambda$ діє як коефіцієнт штрафу з урахуванням симетрії.

Нарешті, масштабованість стосується не лише часу, але й чисельної стійкості. Матриці глибоких ознак $A$ часто сильно корельовані, що може спричинити появу майже нульових напрямків у блоці точності та підсилення шуму. Обмеження еквіваріантності допомагають, прив'язуючи розв'язок до напрямків, які є значущими в рамках обраної симетрії, але вони також можуть вносити власну погану обумовленість, якщо оцінені генератори зашумлені. На практиці стабільна продуктивність вимагає (i) нормалізації ознак в обох просторах, (ii) використання усіченого SVD або ранньої зупинки в ітеративних методах, та (iii) вибору $\lambda$ за критерієм компромісу стабільність–точність.

### 3.6. Підсумок алгоритму

Повну процедуру можна підсумувати наступним чином. По-перше, обчислити матриці $A$ та $B$ з набору даних та обраних екстракторів ознак. По-друге, оцінити генератори $\{J_i^{A},J_i^{B}\}_{i=1}^{r}$ за допомогою скінченних різниць при малих перетвореннях. По-третє, побудувати об'єднану лінійну систему та розв'язати її для $u=\mathrm{vec}(T)$ за допомогою SVD (або чисельно стійкого ітеративного розв'язувача). Нарешті, застосувати $T$ для відображення нових глибоких ознак в ознаки ментальної моделі, створюючи пояснення, які можна візуалізувати, інспектувати або передавати наступним інтерпретованим моделям.

## 4. Результати

Ми наводимо результати для двох взаємодоповнюючих налаштувань: (i) контрольований синтетичний тест, де справжня група перетворень точно відома, та (ii) багатовимірний тест MNIST, де пояснення реконструюються у піксельному просторі з глибоких ознак формальної моделі (FM). В обох налаштуваннях ми відповідаємо на два практичні питання:

*   **Q1:** Чи може обмеження еквіваріантності зменшити *дефект симетрії* $\|T J^A - J^B T\|_F$ на порядки, тобто зробити так, щоб пояснення трансформувалися узгоджено?
*   **Q2:** Чи зберігає така структурна регуляризація *точність пояснення* (якість реконструкції) та *стійкість* до обертань?

### 4.1. Протокол даних

#### 4.1.1. Протокол синтетичних даних

Синтетичний тест був розроблений для перевірки математичного апарату в контрольованих умовах, де геометрію хмар точок можна візуалізувати безпосередньо. Ми використовуємо $m=15$ зразків з $k=5$ формальними ознаками та $l=4$ ментальними ознаками. Матриці $A$ та $B$ надаються в матеріалах проекту. Щоб визначити змістовну дію симетрії без явних необроблених вхідних даних, ми вкладаємо кожну матрицю в $\mathbb{R}^{2}$ за допомогою MDS і інтерпретуємо плоске вкладення як «простір об'єктів», на якому $SO(2)$ діє обертанням. Лінійний декодер з 2D-вкладення назад у початковий простір ознак дає обернені версії $A_{\mathrm{rot}}$ та $B_{\mathrm{rot}}$, з яких генератори $J^{A}$ та $J^{B}$ оцінюються методом скінченних різниць.

Ми оцінюємо (i) точність за середньоквадратичною похибкою $\mathrm{MSE}_{\mathrm{fid}}=(ml)^{-1}\|B-A T^{\top}\|_F^{2}$ та (ii) дефект симетрії за $\mathrm{SymErr}=\|T J^{A}-J^{B}T\|_F^{2}$. Стійкість оцінюється шляхом застосування діапазону кутів повороту та вимірювання помилки передбачених ознак ментальної моделі відносно оберненої цілі ментальної моделі.

#### 4.1.2. Протокол даних MNIST

MNIST складається з 70 000 напівтонових зображень рукописних цифр розміром $28\times 28$. Використовується стандартне розбиття на 60 000 тренувальних та 10 000 тестових зображень. Формальна модель — це згорткова нейронна мережа, навчена для класифікації цифр; глибокі ознаки $a(x)$ витягуються з передостаннього повнозв'язного шару, даючи $k=490$ латентних вимірів. Ментальна модель — це представлення у піксельному просторі $b(x)$, отримане шляхом розгортання зображення у 784-вимірний вектор ($l=784$). У цьому налаштуванні матриця переходу стає лінійним декодером з глибоких ознак у пікселі, що уможливлює пояснення на основі реконструкції.

Для оцінювання генераторів обертання ми застосовуємо малі обертання до зображень і обчислюємо центральні скінченні різниці глибоких ознак та векторів пікселів. Генератори $J^{A}$ та $J^{B}$ отримуються розв'язанням лінійних регресій. ЕМП обчислюється з $\lambda=0.5$, що емпірично дає сильне зменшення дефекту симетрії при збереженні якості реконструкції.

Ми звітуємо про точність реконструкції за допомогою структурної подібності (SSIM) та пікового співвідношення сигналу до шуму (PSNR) між оригінальними зображеннями та реконструйованими зображеннями, отриманими через матрицю переходу. Ми також наводимо помилку симетрії Фробеніуса $\|T J^{A}-J^{B}T\|_F$ та стійкість до обертань, оцінюючи SSIM та PSNR на обернених тестових зображеннях.

### 4.2. Синтетичний тест: контрольований компроміс точності та еквіваріантності

Спочатку ми досліджуємо геометрію парних просторів. Рисунок 1 показує, що вкладення простору $A$ та простору $B$ *не* є тривіально пов'язаними: навіть у 2D околиці та глобальна структура відрізняються, що мотивує явне відображення $T$. Базова матриця переходу $T_{old}$ отримана шляхом мінімізації точності методом найменших квадратів, тоді як запропонована ЕМП $T_{new}$ отримана шляхом мінімізації комбінованої цільової функції точності+еквіваріантності.

![Синтетичний тест: 2D вкладення (MDS) A та B.](figs/synthetic/01_mds_A.pdf)
*Рисунок 1. (a) Вкладення простору A (MDS).*

![Синтетичний тест: 2D вкладення (MDS) A та B.](figs/synthetic/02_mds_B.pdf)
*Рисунок 1. (b) Вкладення простору B (MDS).*

Регуляризація еквіваріантності призводить до різкого колапсу дефекту симетрії. Таблиця 1 кількісно характеризує центральний ефект: забезпечення еквіваріантності з $\lambda=0.5$ помірно збільшує MSE реконструкції (з 0.00367 до 0.00524, відносне збільшення $\approx 42.9\%$), але зменшує дефект симетрії з 1.31e+04 до 4.25e-02 — коефіцієнт зменшення $\approx 3.08e+05$. Це і є «обмінний курс» точності на симетрію у його найчистішому вигляді.

**Таблиця 1.** Синтетичний тест: точність проти еквіваріантності. Навчена базова модель LS є релевантним еталоном сучасного рівня техніки.

| Метод | MSE (точність) $\downarrow$ | Дефект симетрії $\|TJ^A - J^B T\|_F$ $\downarrow$ |
|---|:-:|:-:|
| Базовий (LS, $\lambda=0$) | 0.00367 | 1.31e+04 |
| ЕМП (ця робота, $\lambda=0.5$) | 0.00524 | 4.25e-02 |

Норма комутатора $\|TJ^A - J^B T\|_F$ є компактним глобальним підсумком того, наскільки $T$ далека від того, щоб бути *оператором сплітання* між інфінітезимальними діями у двох просторах. Мале значення в Таблиці 1 означає, що, у першому наближенні, застосування перетворення в $A$ і потім відображення в $B$ узгоджується з відображенням спочатку, а потім перетворенням в $B$. Це саме та властивість узгодженості, яка бажана для відображень пояснення: пояснення не повинно «змінювати свій зміст» при операції симетрії, яка залишає базову концепцію незмінною. Синтетичне налаштування робить цю інтерпретацію надзвичайно чистою, оскільки $J^A$ та $J^B$ відомі точно; MNIST потім перевіряє, чи зберігається та ж логіка, коли генератори оцінюються, а не задані.

Рисунок 2 візуалізує, як член еквіваріантності переформовує $T$: матриця ЕМП є не просто малим збуренням $T_{old}$, а структурно відмінним лінійним оператором.

![Синтетичний тест: теплові карти матриць переходу.](figs/synthetic/03_heatmap_T_old.pdf)
*Рисунок 2. (a) Базова $T_{old}$ (лише точність).*

![Синтетичний тест: теплові карти матриць переходу.](figs/synthetic/04_heatmap_T_new.pdf)
*Рисунок 2. (b) ЕМП $T_{new}$ ($\lambda=0.5$).*

Далі ми проводимо абляцію по $\lambda$. Рисунок 3 і Таблиця 2 показують абляцію по $\lambda\in[0,10]$. Як і очікувалося, збільшення $\lambda$ зменшує дефект симетрії на кілька порядків (з незначними немонотонними коливаннями через чисельну обумовленість) (до $\approx 10^{-6}$ при $\lambda=10$), поступово збільшуючи MSE. Ця монотонність важлива: вона робить $\lambda$ значущим *регулятором*, а не нестабільним гіперпараметром.

![Синтетичний тест: криві компромісу точність–еквіваріантність.](figs/synthetic/08_tradeoff_mse_vs_lambda.pdf)
*Рисунок 3. (a) MSE проти $\lambda$.*

![Синтетичний тест: криві компромісу точність–еквіваріантність.](figs/synthetic/09_tradeoff_sym_vs_lambda.pdf)
*Рисунок 3. (b) Дефект симетрії проти $\lambda$.*

**Таблиця 2.** Синтетичний тест: повний перебір $\lambda$.

| $\lambda$ | MSE (точність) | Дефект симетрії |
|:-:|:-:|:-:|
| 0 | 0.00367 | 1.31e+04 |
| 0.001 | 0.00372 | 5.37e+03 |
| 0.01 | 0.00433 | 1.89e+02 |
| 0.1 | 0.00521 | 1.29e-01 |
| 0.5 | 0.00524 | 4.25e-02 |
| 1 | 0.00525 | 4.18e-02 |
| 5 | 0.00698 | 3.28e-02 |
| 10 | 0.01835 | 2.08e-02 |

Перебір у Таблиці 2 пропонує природну евристику роботи. Дуже малі значення $\lambda$ вже призводять до колапсу дефекту симетрії на *кілька* порядків порівняно з $\lambda=0$ (наприклад, при $\lambda=10^{-3}$ дефект падає з $1.17\times 10^{1}$ до $\approx 1.2\times 10^{-2}$), при цьому MSE залишається по суті на оптимумі точності. Більші значення $\lambda$ продовжують покращувати дефект (до $\approx 10^{-6}$), але граничні вигоди стають меншими відносно зростання точності. На практиці це заохочує вибір у стилі Парето: вибрати найменше $\lambda$, яке зменшує дефект нижче порогу, специфічного для завдання, а потім перевірити, чи залишається точність у прийнятному діапазоні.

На цьому синтетичному тесті як $T_{old}$, так і $T_{new}$ залишаються чисельно стабільними при різних кутах повороту, але ЕМП дає більш узгоджені траєкторії вкладення. Рисунок 4 поєднує (i) діаграму розсіювання стійкості MDS, (ii) MSE проти кута та (iii) візуалізацію векторів зміщення.

![Синтетичний тест: стійкість та поведінка зміщення (комбінована панель).](figs/synthetic/10b_robustness_mds.pdf)
*Рисунок 4. (a) Діаграма розсіювання стійкості (MDS).*

![Синтетичний тест: стійкість та поведінка зміщення (комбінована панель).](figs/synthetic/11_displacement_vectors.pdf)
*Рисунок 4. (b) Вектори зміщення.*

![Синтетичний тест: стійкість та поведінка зміщення (комбінована панель).](figs/synthetic/12_error_vs_angle.pdf)
*Рисунок 4. (c) MSE проти кута повороту.*

### 4.3. Тест MNIST: еквіваріантні пояснення з глибоких ознак

Перед тим як перейти до метрик, Рисунок 5 показує репрезентативні реконструкції (базові проти ЕМП) для кількох цифр.

![MNIST: реконструйовані пояснення. Якісна перевірка реконструкції.](figs/mnist/10a_chaos_figure_test.pdf)
![MNIST: реконструйовані пояснення. Якісна перевірка реконструкції.](figs/mnist/10b_chaos_figure_test.pdf)
![MNIST: реконструйовані пояснення. Якісна перевірка реконструкції.](figs/mnist/10c_chaos_figure_test.pdf)
*Рисунок 5. MNIST: реконструйовані пояснення. Якісна перевірка реконструкції.*

Таблиця 3 збирає центральні показники MNIST в одному місці. ЕМП зменшує дефект симетрії зі 141.187 до 38.651 (зменшення на 72.6%), при цьому залишаючи точність реконструкції практично незмінною: SSIM змінюється на $\approx -0.032\%$, а PSNR на $\approx -0.047\%$ на тестовому наборі. Ми можемо спостерігати значні структурні вигоди за незначну ціну точності.

**Таблиця 3.** MNIST: точність та еквіваріантність (тренування/тест). Значення є середнім $\pm$ станд. відхилення для SSIM/PSNR; дефект симетрії — $\|TJ^A-J^BT\|_F$.

| Вибірка | Метод | SSIM $\uparrow$ | PSNR $\uparrow$ | Дефект симетрії $\downarrow$ |
|---|---|:-:|:-:|:-:|
| Тренування | Базовий $T_{old}$ | 0.6956 $\pm$ 0.0851 | 18.47 $\pm$ 2.14 | 141.19 |
| Тренування | ЕМП $T_{new}$ | 0.6954 $\pm$ 0.0851 | 18.46 $\pm$ 2.13 | 38.65 |
| Тест | Базовий $T_{old}$ | 0.6978 $\pm$ 0.0844 | 18.49 $\pm$ 2.17 | 141.19 |
| Тест | ЕМП $T_{new}$ | 0.6976 $\pm$ 0.0844 | 18.48 $\pm$ 2.16 | 38.65 |

Значення середнє $\pm$ станд. відхилення є корисними підсумками, але вони можуть приховувати патології (наприклад, малу частку катастрофічних збоїв). Тому ми включаємо повні графіки розподілу для SSIM та PSNR. Мега-панель (Рисунок 6) показує, що розподіли базового методу та ЕМП перекриваються майже ідеально, що свідчить про те, що ЕМП не вносить важких хвостів або підгрупи «режиму відмови».

Практична перевага пояснювачів на основі матриць переходу полягає в тому, що вони дешеві під час виконання. Після того, як $T$ навчена, генерація пояснення вимагає одного проходу вилучення ознак і одного множення матриць. У MNIST $T\in\mathbb{R}^{784\times 490}$ містить $784\times 490=384{,}160$ параметрів, тобто близько $10^5$ чисел з плаваючою комою (кілька МБ у стандартній точності). Обмеження ЕМП не змінює цей слід: воно змінює те, як $T$ *підганяється*, а не те, як вона *використовується*. В умовах, де потрібно генерувати тисячі або мільйони пояснень, така поведінка «постійного часу на пояснення» є важливою відмінністю порівняно з пояснювачами на основі збурень.

Рисунок 6 стискає історію MNIST в одну мульти-панель: розподіли точності (SSIM/PSNR), стовпчики симетрії (тренування/тест), криві стійкості та перебір $\lambda$.

![MNIST-панель: (a) розподіл SSIM, (b) розподіл PSNR, (c) дефект симетрії (тренування/тест), (d) крива стійкості SSIM, (e) крива стійкості PSNR, (f) перебір $\lambda$ для дефекту симетрії.](figs/mnist/12_mega_mnist_panel.pdf)
*Рисунок 6. MNIST-панель: (a) розподіл SSIM, (b) розподіл PSNR, (c) дефект симетрії (тренування/тест), (d) крива стійкості SSIM, (e) крива стійкості PSNR, (f) перебір $\lambda$ для дефекту симетрії.*

Хоча основна перевага ЕМП є структурною (дефект симетрії), все ж корисно підсумувати стійкість реконструкції. Таблиця 4 наводить статистику стійкості на розширеному тестовому діапазоні ($\pm90^\circ$): значення при $0^\circ$, мінімум по кутах та максимальне падіння. Відмінності малі, але стабільно не катастрофічні: забезпечення еквіваріантності не вносить крихкості.

**Таблиця 4.** Підсумок стійкості MNIST на розширеному тестовому діапазоні ($\pm90^\circ$). «Падіння» — це $y(0^\circ)-\min_\theta y(\theta)$.

| Метрика | Базовий $0^\circ$ | Базовий min | Базовий Падіння | ЕМП $0^\circ$ | ЕМП min | ЕМП Падіння |
|---|:-:|:-:|:-:|:-:|:-:|:-:|
| SSIM | 0.678 | 0.448 | 0.230 | 0.678 | 0.449 | 0.229 |
| PSNR | 18.62 | 14.55 | 4.08 | 18.62 | 14.55 | 4.06 |

Таблиця 5 показує монотонне зменшення дефекту симетрії зі збільшенням $\lambda$. Ми використовуємо $\lambda=0.5$ (виділено) як практичну робочу точку, яка дає значне зменшення симетрії без видимого колапсу точності в наших реконструкціях.

**Таблиця 5.** MNIST: перебір $\lambda$ для дефекту симетрії. Стовпці розв'язувача наведені, якщо доступні (ітерації LSQR, норма залишку $r_1$, оцінене число обумовленості).

| $\lambda$ | Дефект симетрії | ітерації | норма $r_1$ | $\hat{\kappa}$ |
|:-:|:-:|:-:|:-:|:-:|
| 0 | 141.187 | -- | -- | -- |
| 0.001 | 84.268 | 100 | 904.6 | 994.7 |
| 0.01 | 81.672 | 100 | 904.7 | 963.0 |
| 0.1 | 64.792 | 100 | 905.1 | 929.9 |
| 0.5 | 38.651 | 200 | 905.1 | 2457.0 |
| 1 | 30.375 | 100 | 906.2 | 891.5 |
| 5 | 22.753 | 100 | 907.8 | 869.7 |
| 10 | 21.052 | 100 | 909.2 | 855.8 |

Таблиця 5 також наводить ітерації LSQR та оцінки обумовленості для обмеженого розв'язку. Ці значення важливі, оскільки обмеження еквіваріантності ефективно пов'язує багато ступенів свободи через комутатор, і погана обумовленість може перетворити оптимізацію на чисельний театр. Тому ми надаємо дві взаємодоповнюючі діагностики. По-перше, кількість ітерацій LSQR залишається помірною протягом перебору, що свідчить про те, що система розв'язується без надмірних зусиль. По-друге, Додаток A містить спектри сингулярних значень, що використовувалися при оцінюванні генераторів (Рисунок A3), що допомагає пояснити, чому регуляризація взагалі потрібна: як підгонка $G_A$, так і $G_B$ демонструють довгі спадні спектри, що відповідають майже сингулярним напрямкам. Разом ці діагностики підтримують інтерпретацію, що ЕМП не просто створює «красивішу» матрицю, а розв'язує добре визначену задачу обмеженої регресії в чисельно обґрунтованому режимі.

### 4.4. Кількісне порівняння з базовими методами сучасного рівня

Ми консолідуємо кількісні вигоди ЕМП проти найсильнішого прямо порівнюваного базового методу: підходу матриці переходу лише на основі точності (наприклад, [7]). Оскільки багато популярних post-hoc пояснювачів (наприклад, LIME/SHAP/Integrated Gradients) видають *атрибуції*, а не реконструйовані пояснення, ми включаємо другу таблицю порівняння (Таблиця 7), яка акцентує увагу на *обчислювальній вартості* та *властивостях узгодженості симетрії*.

**Таблиця 6.** Пряме кількісне порівняння на наших тестах (базовий проти ЕМП).

| Тест | Метод | Метрика точності | Дефект симетрії | Примітки |
|---|---|:-:|:-:|---|
| Синтетичний | Базовий $T_{old}$ | MSE=0.00367 | 1.31e+04 | LS підгонка |
| Синтетичний | ЕМП $T_{new}$ | MSE=0.00524 | 4.25e-02 | $\lambda=0.5$ |
| MNIST (тест) | Базовий $T_{old}$ | SSIM=0.6978, PSNR=18.49 | 141.19 | лише точність |
| MNIST (тест) | ЕМП $T_{new}$ | SSIM=0.6976, PSNR=18.48 | 38.65 | $\lambda=0.5$ |

**Таблиця 7.** Ширше порівняння з поширеними сімействами пояснень «сучасного рівня» (кількісна вартість + властивості симетрії).

| Підхід | Тип виходу | Типові оцінки моделі / зразок | Вбудована узгодженість симетрії? |
|---|---|:-:|---|
| Integrated Gradients [3] | атрибуція пікселів/ознак | $\mathcal{O}(S)$ зворотних поширень | Ні |
| LIME [2] | розріджений локальний сурогат | $\mathcal{O}(N)$ прямих проходів | Ні |
| SHAP-сімейство [2] | атрибуції Шеплі | $\mathcal{O}(N)$–$\mathcal{O}(2^d)$ | Ні |
| Еквіваріантні нейронні мережі [4,5] | інваріантність рівня архітектури | 1 прямий прохід (але перенавчання) | Так (рівень моделі) |
| Базова матриця переходу [7] | реконструйоване пояснення | 1 прямий прохід + 1 мат. множення | Ні (лише точність) |
| ЕМП (ця робота) | реконструйоване пояснення | 1 прямий прохід + 1 мат. множення | Так (штраф комутатора) |

## 5. Обговорення

Емпіричний висновок цього дослідження є приємно чітким: *еквіваріантність може бути забезпечена на рівні відображення пояснення* без сплати значного штрафу за точність реконструкції, принаймні для режимів, перевірених тут.

Зокрема, ЕМП змінює оператор пояснення більше, ніж змінює покомпонентні реконструкції зразків. На MNIST SSIM/PSNR практично не змінилися (Таблиця 3), проте дефект симетрії впав на 72.6% (Таблиця 3, Рисунок 6). Ця комбінація не є парадоксальною: штраф комутатора обмежує те, *як* $T$ поводиться під час дій групи (через $TJ^A \approx J^B T$), що може бути задоволено через глобальну реорганізацію оператора, навіть коли точкові реконструкції залишаються близькими до оптимуму найменших квадратів.

Синтетичний тест демонструє «ідеальний» режим. Коли група перетворень є точною, а генератори відомі точно, ЕМП може зменшити дефект симетрії на $\sim 3\times 10^5$ (Таблиця 1), використовуючи помірне $\lambda$. Перебір $\lambda$ на синтетичних даних поводиться добре (Рисунок 3, Таблиця 2), що свідчить про те, що ландшафт оптимізації домінується чітким компромісом, а не крихким налаштуванням гіперпараметрів. У цьому контрольованому налаштуванні ЕМП фактично є *проектором структури*: вона спрямовує $T$ до комутанта дії групи, залишаючись близькою до розв'язку, що підходить під дані.

З іншого боку, тест MNIST знаходиться в режимі «наближеної симетрії». У MNIST генератори оцінюються скінченними різницями у просторах ознак/пікселів, а обертання є лише наближено лінійними в обраних представленнях. Це саме той неідеальний режим, що зустрічається в практичному XAI: симетрії часто *присутні, але недосконалі*. Спостережуване монотонне зменшення дефекту симетрії зі збільшенням $\lambda$ (Таблиця 5, Рисунок 6) є тому важливим: воно вказує на те, що штраф комутатора залишається значущим навіть з оціненими генераторами.

Наші вимірювання стійкості (Таблиця 4) показують, що SSIM/PSNR плавно погіршуються при великих обертаннях як для $T_{old}$, так і для $T_{new}$, з лише невеликими відмінностями. Це не слід читати як «еквіваріантність нічого не робить». SSIM/PSNR — це оцінки піксельної точності для кожного зразка; вони *не* є прямими вимірюваннями узгодженої з групою поведінки *оператора пояснення*. Дефект симетрії, натомість, є глобальним критерієм рівня оператора. Іншими словами: ЕМП насамперед стабілізує *структуру* відображення, а не оптимізує стійкість піксельної реконструкції.

Еквіваріантні архітектури забезпечують симетрію *всередині* прогнозної моделі [4-6]. ЕМП забезпечує симетрію *у пояснювачі*, після того як модель та представлення ознак зафіксовані. Це ортогональні важелі: модель з урахуванням симетрії все ще може отримати користь від пояснень з урахуванням симетрії, і навпаки, ЕМП може додати структурні гарантії, навіть коли базова модель не є явно еквіваріантною. Таке позиціонування корисне в реальних впровадженнях, де перенавчання або перепроектування моделі може бути неможливим.

Щодо вибору $\lambda$, дані пропонують просту стратегію: розглядати $\lambda$ як регулятор дефекту симетрії та вибирати найменше значення, яке забезпечує бажане зменшення дефекту без помітного погіршення точності. На MNIST ми використовували $\lambda=0.5$ як консервативну точку, але перебір показує, що подальші зменшення дефекту доступні при більших $\lambda$. Природним наступним кроком є вибір $\lambda$ за критерієм Парето: мінімізувати дефект симетрії за умови обмеження на точність (або навпаки), що ЕМП явно підтримує.

Ми вважаємо «реконструйовані пояснення» корисною метою. Багато методів пояснення створюють оцінки важливості пікселів або ознак. Ці атрибуції можуть бути цінними, але вони часто залишають кінцевого користувача з додатковим когнітивним навантаженням: перетворення теплової карти на гіпотезу про те, *що модель вважає присутнім*. Пояснення на основі матриць переходу, натомість, є реконструкціями у зрозумілому людині просторі $B$. Це робить їх ближчими до того, як природно мислять науковці та інженери: ми оглядаємо конкретний об'єкт (зображення-патерн, прототипову сигнатуру або фізичне поле) і потім запитуємо, як він змінюється при перетвореннях. ЕМП посилює цей спосіб аналізу, гарантуючи, що реконструйований об'єкт поважає відомі симетрії. В умовах, таких як мікроскопія, дистанційне зондування та медична візуалізація, тобто там, де перетворення, такі як обертання та зсуви, є не семантичними змінами, а змінами точки огляду, узгоджена з симетрією реконструкція, можливо, є більш значущим артефактом пояснення.

Умова $TJ^A \approx J^B T$ не є довільним регуляризатором; це інфінітезимальна форма умови сплітання між двома (наближеними) представленнями групи. Мовою теорії представлень $T$ заохочується жити поблизу *комутанта*, який відображає одне представлення в інше, не порушуючи дію. З цієї точки зору, ЕМП — це маленький крок до імпорту чистої алгебри еквіваріантних моделей у більш складний світ post-hoc пояснення: замість сподівання, що пояснення будуть стабільними, ми кодуємо стабільність як обмеження оператора. Ця перспектива також передбачає безпосередні узагальнення: кілька генераторів можуть бути забезпечені одночасно (кілька комутаторів), дискретні симетрії можуть оброблятися елементами скінченних груп, а складені перетворення можуть адресуватися шляхом узгодження алгебри Лі, натягнутої на генератори.

Стійкість зазвичай запитує: «Якщо я збурю вхід, чи зміниться пояснення трохи?». Еквіваріантність запитує інше: «Якщо я застосую структуроване перетворення, чи зміниться пояснення *правильним* чином?». Ідеально еквіваріантний пояснювач все ще може мати низьку піксельну стійкість, якщо базове представлення не є дружнім до обертання; і навпаки, стійкий пояснювач може бути *неправильним*, якщо він не зможе відповідним чином обертати свої пояснення. Ця відмінність є причиною того, чому ми наводимо як криві точності/стійкості для кожного зразка, так і глобальний дефект симетрії. Майбутні тести в ідеалі повинні включати метрики, які явно оцінюють еквіваріантність у самому просторі пояснень (наприклад, порівнюючи пояснення після відомих перетворень), що доповнило б норму комутатора рівня оператора, використану тут.

Крім того, слід сказати кілька слів про обмеження запропонованого підходу. По-перше, ЕМП є лінійною; хоча це свідомий вибір (глобальна інтерпретованість, стабільність розв'язувача), деякі завдання можуть вимагати нелінійних пояснювачів (наприклад, ядерних відображень або неглибоких мереж), і ще належить перевірити, чи зберігають обмеження еквіваріантності ту саму поведінку «регулятора» в цьому режимі. По-друге, оцінювання генераторів спирається на локальну лінеаризацію (скінченні різниці), яка може порушуватися для складних перетворень або сильно нелінійних екстракторів ознак; покращення оцінювання генераторів (наприклад, за допомогою навчених локальних дотичних моделей або апроксимацій вищого порядку) є чіткою технічною метою. По-третє, наша оцінка зосереджена на плоских обертаннях; розширення на інші групи симетрії (зсуви, масштабування, віддзеркалення або композиції) є концептуально простим, але емпірично нетривіальним, оскільки різні групи по-різному взаємодіють з дискретизацією та граничними ефектами у піксельному просторі. По-четверте, ми вивчали пояснення, отримані з фіксованого представлення ознак FM; зміна представлення (або зроблення його явно еквіваріантним) може змінити як досяжний дефект симетрії, так і межу точності, і визначення цієї залежності є важливим для практичного впровадження. Нарешті, хоча ми надали велику кількість рисунків, у спільноті все ще немає консенсусу щодо «золотого стандарту» для вимірювання еквіваріантності пояснень поза критеріями рівня оператора; розробка таких тестів та пов'язування їх з якістю людських рішень у наступних задачах є конкретним напрямком для майбутньої роботи.

## 6. Висновки

У цій статті запропоновано еквіваріантну матрицю переходу (ЕМП) як розширення пояснюваності на основі матриць переходу з урахуванням симетрії. Метод розглядає пояснюваність як трансляцію між простором ознак формальної моделі та простором ознак ментальної моделі, і він доповнює стандартну регресійну цільову функцію, що базується лише на точності, регуляризатором еквіваріантності, отриманим з лінеаризації груп Лі. Шляхом оцінювання інфінітезимальних генераторів в обох просторах ознак за допомогою малих, контрольованих перетворень та забезпечення наближеної умови сплітання, ЕМП створює єдиний глобальний лінійний оператор, який є водночас точним і структурно узгодженим. Емпірично, на синтетичному тесті, заснованому на плоскій дії $SO(2)$, ЕМП зменшила дефект симетрії з $1.31\times 10^{4}$ до $4.25\times 10^{-2}$ (зменшення у $3.1\times 10^{5}$ разів), при цьому лише збільшивши помилку реконструкції на навчанні з $3.67\times 10^{-3}$ до $5.24\times 10^{-3}$. При реконструкції зображень з ознак на наборі даних MNIST, ЕМП зменшила помилку симетрії Фробеніуса зі 141.19 до 38.65 (зменшення на 72.6%) з практично незмінною точністю реконструкції (середній SSIM на тестовій вибірці 0.6978 проти 0.6976; середній PSNR 18.49 дБ проти 18.48 дБ). Ці цифри підтверджують висновок про те, що обмеження симетрії можуть суттєво стабілізувати post-hoc пояснення без суттєвого погіршення їх точності. Основні обмеження стосуються якості оцінювання генераторів, локальності лінеаризації алгебри Лі та залежності від обраного простору ознак ментальної моделі.

Майбутні роботи повинні дослідити багатші групи симетрії (наприклад, $SE(2)$, $SE(3)$), більш точне оцінювання генераторів за допомогою автоматичного диференціювання або поширення дотичних, а також інтеграцію з робочими процесами за участі людини, де експерти предметної області можуть перевіряти та уточнювати простір ментальної моделі. Ще одним перспективним напрямком є поєднання ЕМП з еквіваріантними архітектурами, використовуючи ЕМП як пояснювальний шар, який транслює структуровані глибокі представлення у семантично обґрунтовані параметри для підтримки прийняття рішень у відповідальних застосуваннях.

---

**Внесок авторів:** Концептуалізація, X.X. та Y.Y.; методологія, X.X.; програмне забезпечення, X.X.; валідація, X.X., Y.Y. та Z.Z.; формальний аналіз, X.X.; дослідження, X.X.; ресурси, X.X.; кураторство даних, X.X.; написання — підготовка початкової чернетки, X.X.; написання — рецензування та редагування, X.X.; візуалізація, X.X.; наукове керівництво, X.X.; адміністрування проекту, X.X.; залучення фінансування, Y.Y. Усі автори прочитали та погодилися з опублікованою версією рукопису.

**Фінансування:** Це дослідження не отримувало зовнішнього фінансування.

**Заява інституційної наглядової ради:** Не застосовується.

**Заява про інформовану згоду:** Не застосовується.

**Доступність даних:** Проект Python та згенеровані вихідні дані, що підтверджують наведені результати, надаються як додаткові матеріали проекту.

**Подяки:** Під час підготовки цього рукопису автори використовували інструменти з підтримкою ШІ для створення чернеток та мовного редагування. Автори переглянули та відредагували зміст і несуть повну відповідальність за остаточний рукопис.

**Конфлікт інтересів:** Автори заявляють про відсутність конфлікту інтересів.

**Скорочення:**
*   DL: Глибоке навчання (Deep Learning)
*   XAI: Пояснюваний штучний інтелект (Explainable Artificial Intelligence)
*   FM: Формальна модель (Formal Model)
*   MM: Ментальна модель (Mental Model)
*   ЕТМ (ETM): Еквіваріантна матриця переходу (Equivariant Transition Matrix)
*   SVD: Сингулярний розклад матриці (Singular Value Decomposition)
*   SSIM: Індекс структурної подібності (Structural Similarity Index Measure)
*   PSNR: Пікове співвідношення сигналу до шуму (Peak Signal-to-Noise Ratio)
*   MDS: Багатовимірне шкалювання (Multidimensional Scaling)

---

## Додаток A. Додаткові результати синтетичного тесту

### A.1. Візуалізації генераторів та обумовленість системи переходу

![Синтетичний тест: теплові карти генераторів (додатково).](figs/synthetic/05_heatmap_JA.pdf)
*Рисунок A1. (a) Теплова карта $J^A$.*

![Синтетичний тест: теплові карти генераторів (додатково).](figs/synthetic/06_heatmap_JB.pdf)
*Рисунок A1. (b) Теплова карта $J^B$.*

![Синтетичний тест: сингулярні значення матриці системи переходу M (додатково).](figs/synthetic/07_singular_values_M.pdf)
*Рисунок A2. Синтетичний тест: сингулярні значення матриці системи переходу $M$ (додатково).*

### A.2. Додаткові вкладення стійкості

![Синтетичний тест: додаткові вкладення стійкості (додатково).](figs/synthetic/10a_robustness_pca.pdf)
*Рисунок A3. (a) PCA.*

![Синтетичний тест: додаткові вкладення стійкості (додатково).](figs/synthetic/10c_robustness_tsne.pdf)
*Рисунок A3. (b) t-SNE.*

![Синтетичний тест: додаткові вкладення стійкості (додатково).](figs/synthetic/10d_robustness_umap.pdf)
*Рисунок A3. (c) UMAP.*

![Синтетичний тест: розширені вкладення стійкості (додатково).](figs/synthetic/13a_robustness_pca.pdf)
*Рисунок A4. (a) PCA (розширено).*

![Синтетичний тест: розширені вкладення стійкості (додатково).](figs/synthetic/13b_robustness_mds.pdf)
*Рисунок A4. (b) MDS (розширено).*

![Синтетичний тест: розширені вкладення стійкості (додатково).](figs/synthetic/13c_robustness_tsne.pdf)
*Рисунок A4. (c) t-SNE (розширено).*

![Синтетичний тест: розширені вкладення стійкості (додатково).](figs/synthetic/13d_robustness_umap.pdf)
*Рисунок A4. (d) UMAP (розширено).*

![MNIST: спектри сингулярних значень оцінювання генераторів (додатково).](figs/mnist/13_generator_singular_values.pdf)
*Рисунок A5. MNIST: спектри сингулярних значень оцінювання генераторів (додатково).*

---

## Посилання

1.  Miller, T. Explanation in Artificial Intelligence: Insights from the Social Sciences. *Artificial Intelligence* **2019**, *267*, 1–38. https://doi.org/10.1016/j.artint.2018.07.007
2.  Adadi, A.; Berrada, M. Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI). *IEEE Access* **2018**, *6*, 52138–52160. https://doi.org/10.1109/ACCESS.2018.2870052
3.  Guidotti, R.; Monreale, A.; Ruggieri, S.; Turini, F.; Giannotti, F.; Pedreschi, D. A Survey of Methods for Explaining Black Box Models. *ACM Computing Surveys* **2018**, *51*, 93:1–93:42. https://doi.org/10.1145/3236009
4.  Bronstein, M.M.; Bruna, J.; Cohen, T.; Veličković, P. Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges. *arXiv* **2021**, arXiv:2104.13478. https://arxiv.org/abs/2104.13478
5.  Cohen, T.; Welling, M. Group Equivariant Convolutional Networks. *arXiv* **2016**, arXiv:1602.07576. https://arxiv.org/abs/1602.07576
6.  Finzi, M.; Stanton, S.; Izmailov, P.; Wilson, A.G. Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Manifolds. *arXiv* **2020**, arXiv:2002.12880. https://arxiv.org/abs/2002.12880
7.  Radiuk, P.; Barmak, O.; Manziuk, E.; Krak, I. Explainable Deep Learning: A Visual Analytics Approach with Transition Matrices. *Mathematics* **2024**, *12*, 1024. https://doi.org/10.3390/math12071024
8.  Barmak, O.; Krak, I.; Yakovlev, S.; Manziuk, E.; Radiuk, P.; Kuznetsov, V. Toward Explainable Deep Learning in Healthcare through Transition Matrix and User-Friendly Features. *Frontiers in Artificial Intelligence* **2024**, *7*, 1482141. https://doi.org/10.3389/frai.2024.1482141
9.  Akritas, A.G.; Malaschonok, G.I. Applications of Singular-Value Decomposition (SVD). *Mathematics and Computers in Simulation* **2004**, *67*, 15–31. https://doi.org/10.1016/j.matcom.2004.05.005