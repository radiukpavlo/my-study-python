\subsection{Eigenvalues and Eigenvectors for Special Types of Matrices}

%There are three special kinds of matrices which we can use to simplify the process of finding eigenvalues and eigenvectors. 
%Throughout this section, we will discuss similar matrices, elementary matrices, as well as triangular matrices. 
%
%We begin with a definition.
%
%\begin{definition}{Similar Matrices}{similarmatrices}
%Let $A$ and $B$ be $n \times n$ matrices. Suppose there exists an invertible matrix $P$ such that 
%\begin{equation*}
%A = P^{-1}BP
%\end{equation*}
%Then $A$ and $B$ are called \textbf{similar matrices}\index{similar matrix}.
%\end{definition}
%
%It turns out that we can use the concept of similar matrices to help us find the eigenvalues
%of matrices. Consider the following lemma.
%
%\begin{lemma}{Similar Matrices and Eigenvalues}{similarmatrices}
%Let $A$ and $B$ be similar matrices, so that $A=P^{-1}BP$ where $A,B$ are $n\times n$ matrices and $P$ is invertible. Then $A,B$ have the
%same eigenvalues.
%\end{lemma}
%
%\begin{proof}
%We need to show two things. First, we need to show that if
%$A=P^{-1}BP$, then $A$ and $B$ have the same eigenvalues.  Secondly,
%we show that if $A$ and $B$ have the same eigenvalues, then
%$A=P^{-1}BP$.
%
%Here is the proof of the first statement. 
%Suppose $A = P^{-1}BP$ and $\lambda$ is an eigenvalue of $A$, that is $AX=\lambda X$ for some $X\neq 0.$ Then
%\begin{equation*}
%P^{-1}BPX=\lambda X
%\end{equation*}
%and so
%\begin{equation*}
%BPX=\lambda PX
%\end{equation*}
%
%Since $P$ is one to one and $X \neq 0$, it follows that $PX \neq
%0$. Here, $PX$ plays the role of the eigenvector in this equation.
%Thus $\lambda$ is also an eigenvalue of $B$. One can similarly verify
%that any eigenvalue of $B$ is also an eigenvalue of $A$, and thus both
%matrices have the same eigenvalues as desired.
%
%Proving the second statement is similar and is left as an exercise. 
%\end{proof}
%
%Note that this proof also demonstrates that the eigenvectors of $A$ and $B$ will (generally) be {\em different\em}.
%We see in the proof that $AX = \lambda X$, while $B \left(PX\right)=\lambda \left(PX\right)$. Therefore,
%for an eigenvalue $\lambda$, $A$ will have the eigenvector $X$ while $B$ will have the eigenvector $PX$. 
%
%
%The second special type of matrices we discuss in this section is elementary matrices.  
%Recall from Definition \ref{def:elementarymatricesandrowops} that an elementary matrix $E$ is obtained by applying
%one row operation to the identity matrix. 
%
%It is possible to use elementary matrices to simplify a matrix before searching for its
%eigenvalues and eigenvectors. This is illustrated in the following
%example.
%
%\begin{example}{Simplify Using Elementary Matrices}{simplifyusingelementarymatrices}
%Find the eigenvalues for the matrix
%\begin{equation*}
%A = \leftB
%\begin{array}{rrr}
% 33 & 105 & 105 \\
% 10 &  28 & 30 \\
%-20 & -60 & -62
%\end{array}
%\rightB
%\end{equation*}
%\end{example}
%
%\begin{solution} This matrix has big numbers and therefore we would like
%to simplify as much as possible before computing the eigenvalues.
%
%We will do so using row operations. First, add $2$ times the second row
%to the third row. To do so, left multiply $A$ by $E \left(2,2\right)$. 
%Then right multiply $A$ by the inverse of $E \left(2,2\right)$ as illustrated.
%\begin{equation*}
%\leftB
%\begin{array}{rrr}
%1 & 0 & 0 \\
%0 & 1 & 0 \\
%0 & 2 & 1
%\end{array}
%\rightB \leftB
%\begin{array}{rrr}
%33 & 105 & 105 \\
%10 & 28 & 30 \\
%-20 & -60 & -62
%\end{array}
%\rightB \leftB
%\begin{array}{rrr}
%1 & 0 & 0 \\
%0 & 1 & 0 \\
%0 & -2 & 1
%\end{array}
%\rightB =\leftB
%\begin{array}{rrr}
%33 & -105 & 105 \\
%10 & -32 & 30 \\
%0 & 0 & -2
%\end{array}
%\rightB
%\end{equation*}
%By Lemma \ref{lem:similarmatrices}, the resulting matrix has the same eigenvalues as $A$ where here, the matrix $E \left(2,2\right)$ plays the role of $P$.
%
%We do this step again, as follows. In this step, we use the elementary matrix obtained by adding $-3$
%times the second row to the first row. 
%\begin{equation}
%\leftB
%\begin{array}{rrr}
%1 & -3 & 0 \\
%0 &  1 & 0 \\
%0 &  0 & 1
%\end{array}
%\rightB \leftB
%\begin{array}{rrr}
%33 & -105 & 105 \\
%10 & -32  & 30 \\
%0  &   0  & -2
%\end{array} 
%\rightB \leftB
%\begin{array}{rrr}
%1 & 3 & 0 \\
%0 & 1 & 0 \\
%0 & 0 & 1
%\end{array}
%\rightB =\leftB
%\begin{array}{rrr}
%3  & 0  & 15 \\
%10 & -2 & 30 \\
%0  & 0  & -2
%\end{array}
%\rightB  \label{elemeigenvalue}
%\end{equation}
%Again by Lemma \ref{lem:similarmatrices}, this resulting matrix has the same eigenvalues as $A$. 
%At this point, we can easily find the eigenvalues.
%Let 
%\begin{equation*}
%B = \leftB
%\begin{array}{rrr}
%3  & 0  & 15 \\
%10 & -2 & 30 \\
%0  & 0  & -2
%\end{array}
%\rightB 
%\end{equation*}
%Then, we find the eigenvalues of $B$ (and therefore of $A$) by solving the equation 
%$\det \left( \eigenVar I - B  \right) = 0$.
%You should verify that this equation becomes
%\begin{equation*}
%\left(\eigenVar  +2 \right) \left( \eigenVar  +2 \right) \left( \eigenVar  - 3 \right)
%=0
%\end{equation*}
%Solving this equation results in eigenvalues of $\lambda_1 = -2, \lambda_2 = -2$, and $\lambda_3 = 3$.
%Therefore, these are also the eigenvalues of $A$. 
%
%\end{solution}
%
%Through using elementary matrices, we were able to create a matrix for
%which finding the eigenvalues was easier than for $A$. At this point,
%you could go back to the original matrix $A$ and solve $\left(
%\lambda I - A \right) X = 0$ to obtain the eigenvectors of $A$.
%
%Notice that when you multiply on the right by an elementary matrix,
%you are doing the column operation defined by the elementary
%matrix. In \ref{elemeigenvalue} multiplication by the elementary matrix on
%the right merely involves taking three times the first column and
%adding to the second. Thus, without referring to the elementary
%matrices, the transition to the new matrix in \ref{elemeigenvalue} can be
%illustrated by
%\begin{equation*}
%\leftB
%\begin{array}{rrr}
%33 & -105 & 105 \\
%10 & -32 & 30 \\
%0 & 0 & -2
%\end{array}
%\rightB \rightarrow \leftB
%\begin{array}{rrr}
%3 & -9 & 15 \\
%10 & -32 & 30 \\
%0 & 0 & -2
%\end{array}
%\rightB \rightarrow \leftB
%\begin{array}{rrr}
%3 & 0 & 15 \\
%10 & -2 & 30 \\
%0 & 0 & -2
%\end{array}
%\rightB
%\end{equation*}

A special type of matrix we will consider in this section is
the triangular matrix.  Recall Definition \ref{def:triangularmatrices}
which states that an upper (lower) triangular matrix contains all
zeros below (above) the main diagonal. Remember that finding the
determinant of a triangular matrix is a simple procedure of taking the product of the entries on the main diagonal.. It turns out
that there is also a simple way to find the eigenvalues of a
triangular matrix.

In the next example we will demonstrate that the eigenvalues of a 
triangular matrix are the entries on the main diagonal. 

\begin{example}{Eigenvalues for a Triangular Matrix}{eigenvaluestriangularmatrix}
Let $A=\leftB
\begin{array}{rrr}
1 & 2 & 4 \\
0 & 4 & 7 \\
0 & 0 & 6
\end{array}
\rightB .$ Find the eigenvalues of $A$.
\end{example}

\begin{solution}
We need to solve the equation $\det \left( \eigenVar I - A \right) = 0$ as follows
\begin{eqnarray*}
\det \left( \eigenVar I - A \right) =
\det \leftB
\begin{array}{ccc}
\eigenVar -1 & -2 & -4 \\
0 & \eigenVar-4 & -7 \\
0 & 0 & \eigenVar-6
\end{array}
\rightB =\left( \eigenVar-1 \right) \left( \eigenVar-4 \right) \left( \eigenVar-6 \right) =0
\end{eqnarray*}

Solving the equation $\left( \eigenVar-1 \right) \left( \eigenVar-4
\right) \left( \eigenVar-6 \right) = 0$ for $\eigenVar$ results in the eigenvalues 
$\lambda_1 = 1, \lambda_2 = 4$ and $\lambda_3 = 6$.  Thus the
eigenvalues are the entries on the main diagonal of the original
matrix.
\end{solution}

The same result is true for lower triangular matrices. For any triangular matrix,
the eigenvalues are equal to the entries on the main diagonal. To find the 
eigenvectors of a triangular matrix, we use the usual procedure. 

%In the next section, we explore an important process involving the eigenvalues and eigenvectors of a matrix. 


\section{Positive Semi-Definite Matrices}


%
%
%\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
%\PassOptionsToPackage{hyphens}{url}
%%
%\documentclass[]{article}
%\usepackage{lmodern}
%\usepackage{amssymb,amsmath}
%\usepackage{ifxetex,ifluatex}
%\usepackage{fixltx2e} % provides \textsubscript
%\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
%  \usepackage[T1]{fontenc}
%  \usepackage[utf8]{inputenc}
%  \usepackage{textcomp} % provides euro and other symbols
%\else % if luatex or xelatex
%  \usepackage{unicode-math}
%  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
%\fi
%% use upquote if available, for straight quotes in verbatim environments
%\IfFileExists{upquote.sty}{\usepackage{upquote}}
%% use microtype if available
%\IfFileExists{microtype.sty}{%
%\usepackage[]{microtype}
%\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
%}
%\IfFileExists{parskip.sty}{%
%\usepackage{parskip}
%}{% else
%\setlength{\parindent}{0pt}
%\setlength{\parskip}{6pt plus 2pt minus 1pt}
%}
%\usepackage{hyperref}
%\hypersetup{
%            pdfborder={0 0 0},
%            breaklinks=true}
%\urlstyle{same}  % don't use monospace font for urls
%\setlength{\emergencystretch}{3em}  % prevent overfull lines
%\providecommand{%
%  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
%\setcounter{secnumdepth}{0}
%% Redefines (sub)paragraphs to behave more like sections
%\ifx\paragraph\undefined\else
%\let\oldparagraph\paragraph
%\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox}
%\fi
%\ifx\subparagraph\undefined\else
%\let\oldsubparagraph\subparagraph
%\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox}
%\fi
%
%% set default figure placement to htbp
%\makeatletter
%\def\fps@figure{htbp}
%\makeatother
%
%
%\date
%
%\begin{document}

\href{https://en.wikipedia.org/wiki/Definite_symmetric_matrix}{Wikipedia - Definite Symmetric Matrix}

In \href{linear_algebra}{linear algebra}, a
\href{symmetric_matrix}{symmetric} \(n \times n\)
\href{real_number}{real} \href{matrix_(mathematics)}{matrix} \(M\) is
said to be \textbf{positive-definite} if the scalar \(z^\top  Mz\)
is strictly positive for every non-zero column
\href{vector_(mathematics)}{vector} \(z\) of \(n\) real numbers. Here
\(z^\top \) denotes the \url{transpose} of \(z\).\footnote When
interpreting \(Mz\) as the output of an operator, \(M\), that is acting
on an input, \(z\), the property of positive definiteness implies that
the output always has a positive \href{inner_product}{inner product}
with the input, as often observed in physical processes.

More generally, a complex \(n\times n\)
\href{Hermitian_matrix}{Hermitian matrix} \(M\) is said to be
\textbf{positive-definite} if the scalar \(z^* Mz\) is strictly positive
for every non-zero column vector \(z\) of \(n\) complex numbers. Here
\(z^*\) denotes the \href{conjugate_transpose}{conjugate transpose} of
\(z\). Note that \(z^* Mz\) is automatically real since \(M\) is
Hermitian.

\textbf{Positive semi-definite} matrices are defined similarly, except
that the above scalars \(z^\top  Mz\) or \(z^* Mz\) must be positive
\emph{or zero} (i.e. non-negative). \textbf{Negative-definite} and
\textbf{negative semi-definite} matrices are defined analogously. A
matrix that is not positive semi-definite and not negative semi-definite
is called \textbf{indefinite}.

The matrix \(M\) is positive-definite if and only if the
\href{bilinear_form}{bilinear form}
\(\langle z, w\rangle = z^\top  Mw\) is
\href{definite_bilinear_form}{positive-definite} (and similarly for a
positive-definite \href{sesquilinear_form}{sesquilinear form} in the
complex case). This is a coordinate realization of an
\href{inner_product}{inner product} on a \href{vector_space}{vector
space}.\footnote

Some authors use more general definitions of definiteness, including
some non-symmetric real matrices, or non-Hermitian complex ones.

\hypertarget{definitions}{%
\subsection{Definitions}\label{definitions}}

In the following definitions, \(\x^\top \) is the transpose of
\(\x\), \(\x^*\) is the \href{conjugate_transpose}{conjugate transpose} of
\(\x\) and \(\mathbf{0}\) denotes the \emph{n}-dimensional zero-vector.

%\hypertarget{definitions-for-real-matrices}{%
%\subsubsection{Definitions for real
%matrices}\label{definitions-for-real-matrices}}
\begin{definition}{Definiteness for Real Matrices}{}
\label{definitions-for-real-matrices}
\index{PSD}
\index{Positive semidefinite}
An \(n \times n\) symmetric real matrix \(M\) is said to be
\begin{itemize}
\item  \textbf{positive-definite} if \(\x^\top  M\x > 0\) for all non-zero
\(\x\) in \(\mathbb{R}^n\), 
%
\item 
\textbf{positive semidefinite} or \textbf{non-negative-definite} if
\(\x^\top  M\x \geq 0\) for all \(\x\) in \(\mathbb{R}^n\), 
%
\item 
\textbf{negative-definite} if \(\x^\top  M\x < 0\) for all non-zero
\(\x\) in \(\mathbb{R}^n\), 
%
\item 
\textbf{negative-semidefinite} or \textbf{non-positive-definite} if
\(\x^\top  M\x \leq 0\) for all \(\x\) in \(\mathbb{R}^n\),
%
\item An \(n \times n\) symmetric real matrix which is neither positive
semidefinite nor negative semidefinite is called \textbf{indefinite}.
\end{itemize}
\end{definition}
%\hypertarget{definitions-for-complex-matrices}{%
%\subsubsection{Definitions for complex
%matrices}\label{definitions-for-complex-matrices}}
%
%The following definitions all involve the term \(x^* Mx\). Notice that
%this is always a real number for any Hermitian square matrix \(M\).
%
%An \(n \times n\) Hermitian complex matrix \(M\) is said to be
%\textbf{positive-definite} if \(x^* Mx > 0\) for all non-zero \(x\) in
%\(\mathbb{C}^n\). Formally,
%
%An \(n \times n\) Hermitian complex matrix \(M\) is said to be
%\textbf{positive semi-definite} or \textbf{non-negative-definite} if
%\(x^* Mx \geq 0\) for all \(x\) in \(\mathbb{C}^n\). Formally,
%
%An \(n \times n\) Hermitian complex matrix \(M\) is said to be
%\textbf{negative-definite} if \(x^* Mx < 0\) for all non-zero \(x\) in
%\(\mathbb{C}^n\). Formally,
%
%An \(n \times n\) Hermitian complex matrix \(M\) is said to be
%\textbf{negative semi-definite} or \textbf{non-positive-definite} if
%\(x^* Mx \leq 0\) for all \(x\) in \(\mathbb{C}^n\). Formally,
%
%An \(n \times n\) Hermitian complex matrix which is neither positive
%semidefinite nor negative semidefinite is called \textbf{indefinite}.
%
%\hypertarget{consistency-between-real-and-complex-definitions}{%
%\subsubsection{Consistency between real and complex
%definitions}\label{consistency-between-real-and-complex-definitions}}
%
%Since every real matrix is also a complex matrix, the definitions of
%"definiteness" for the two classes must agree.
%
%For complex matrices, the most common definition says that "\(M\) is
%positive-definite if and only if \(z^* Mz\) is real and positive for all
%non-zero \emph{complex} column vectors \(z\)". This condition implies
%that \(M\) is Hermitian (i.e. its transpose is equal to its conjugate).
%To see this, consider the matrices
%\(A = \tfrac{1}{2} \left(M + M^*\right)\) and
%\(B = \tfrac{1}{2i} \left(M - M^*\right)\), so that \(M = A + iB\) and
%\(z^* Mz = z^* Az + iz^* Bz\). The matrices \(A\) and \(B\) are
%Hermitian, therefore \(z^* Az\) and \(z^* Bz\) are individually real. If
%\(z^* Mz\) is real, then \(z^* Bz\) must be zero for all \(z\). Then
%\(B\) is the zero matrix and \(M = A\), proving that \(M\) is Hermitian.
%
%By this definition, a positive-definite \emph{real} matrix \(M\) is
%Hermitian, hence symmetric; and \(z^\top  Mz\) is positive for all
%non-zero \emph{real} column vectors \(z\). However the last condition
%alone is not sufficient for \(M\) to be positive-definite. For example,
%if
%
%\(M = \begin{bmatrix} 1 & 1 \\ -1 & 1 \end{bmatrix},\)
%
%then for any real vector \(z\) with entries \(a\) and \(b\) we have
%\(z^\top  Mz = (a + b)a + (-a + b)b = a^2 + b^2\), which is always
%positive if \(z\) is not zero. However, if \(z\) is the complex vector
%with entries \(1\) and \(i\), one gets
%
%\[z^* M z = [1, -i] M [1, i]^\top  = [1 + i, 1 - i] [1, i]^\top  = 2+2i\]
%
%which is not real. Therefore, \(M\) is not positive-definite.
%
%On the other hand, for a \emph{symmetric} real matrix \(M\), the
%condition "\(z^\top  Mz > 0\) for all nonzero real vectors \(z\)"
%\emph{does} imply that \(M\) is positive-definite in the complex sense.

%\hypertarget{notation}{%
%\subsubsection{Notation}\label{notation}}
%
%If a Hermitian matrix \(M\) is positive semi-definite, one sometimes
%writes \(M \succeq 0\) and if \(M\) is positive-definite one writes
%\(M \succ 0\). To denote that \(M\) is negative semi-definite one writes
%\(M \preceq 0\) and to denote that \(M\) is negative-definite one writes
%\(M \prec 0\).
%
%The notion comes from \href{functional_analysis}{functional analysis}
%where positive semidefinite matrices define
%\href{positive_operator}{positive operators}.
%
%A common alternative notation is \(M \geq 0\), \(M > 0\), \(M \leq 0\)
%and \(M < 0\) for positive semi-definite and positive-definite, negative
%semi-definite and negative-definite matrices, respectively. This may be
%confusing, as sometimes \href{nonnegative_matrix}{nonnegative matrices}
%(respectively, nonpositive matrices) are also denoted in this way.

%\hypertarget{examples}{%
%\subsection{Examples}\label{examples}}
\begin{example}{Identity Matrix}{}
\label{pd-example}
The \href{identity_matrix}{identity matrix}
\(I = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}\) is
positive-definite (and as such also positive semi-definite). It is a
real symmetric matrix, and, for any non-zero column vector \emph{z} with
real entries \emph{a} and \emph{b}, one has


\[z^\top  z = \begin{bmatrix} a & b \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix} = a^2 + b^2.\]


Seen as a complex matrix, for any non-zero column vector $\z$ with
complex entries $a$ and $b$ one has


\[\z^*\z = \begin{bmatrix} \overline{a} & \overline{b} \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} a \\ b\end{bmatrix} = \overline{a}a + \overline{b}b = |a|^2 + |b|^2\].


Either way, the result is positive since \(z\) is not the zero vector
(that is, at least one of \(a\) and \(b\) is not zero). 
\end{example}

\begin{example}{Positive definite}{}
 The real symmetric matrix
\begin{description}
\item
\(M = \begin{bmatrix} 2 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 2 \end{bmatrix}\)
\end{description}

is positive-definite since for any non-zero column vector \emph{z} with
entries \emph{a}, \emph{b} and \emph{c}, we have

\begin{align}
z^\top  M z=\left(z^\top M\right)z\\
&=\begin{bmatrix}(2a-b)&(-a+2b-c)&(-b+2c)\end{bmatrix}\\
\begin{bmatrix}a\\b\\c\end{bmatrix}\\
&=(2a-b)a+(-a+2b-c)b+(-b+2c)c\\
&=2a^2-ba-ab+2b^2-cb-bc+2c^2\\
&=2a^2-2ab+2b^2-2bc+2c^2\\
&=a^2+a^2-2ab+b^2+b^2-2bc+c^2+c^2\\
&=a^2+(a-b)^2+(b-c)^2+c^2
\end{align}

This result is a sum of squares, and therefore non-negative; and is zero
only if \(a = b = c = 0\), that is, when $\z$ is the zero vector.
\end{example}

\begin{example}{$A^\top A$}{}
For any real \href{Invertible_matrix}{invertible} matrix
\(A\), the product \(A^\top  A\) is a positive definite matrix. A
simple proof is that for any non-zero vector \(\z\), the condition
\(\z^\top  A^\top  A\z = (A\z)^\top  (A\z) = \|A\z\|^2 > 0,\)
since the invertibility of matrix \(A\) means that \(A\z \neq 0.\)


The example \(M\) above shows that a matrix in which some
elements are negative may still be positive definite. Conversely, a
matrix whose entries are all positive is not necessarily positive
definite, as for example
\[N = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix},\]
for which
\(\begin{bmatrix} -1 & 1 \end{bmatrix}N\begin{bmatrix} -1 & 1 \end{bmatrix}^\top  = -2 < 0.\)
\end{example}

\hypertarget{eigenvalues}{%
\subsection{Eigenvalues}\label{eigenvalues}}

\begin{theorem}{Eigenvalue Characterizations}{}
Let \(M\) be an \(n \times n\) \href{Hermitian_matrix}{Hermitian
matrix}.
\begin{itemize}
\item
  \(M\) is positive definite if and only if all of its eigenvalues are
  positive.
\item
  \(M\) is positive semi-definite if and only if all of its eigenvalues
  are non-negative.
\item
  \(M\) is negative definite if and only if all of its eigenvalues are
  negative
\item
  \(M\) is negative semi-definite if and only if all of its eigenvalues
  are non-positive.
\item
  \(M\) is indefinite if and only if it has both positive and negative
  eigenvalues.
\end{itemize}
\end{theorem}

Let \(P^{-1} DP\) be an
\href{eigendecomposition_of_a_matrix}{eigendecomposition} of \(M\),
where \(P\) is a \href{unitary_matrix}{unitary complex matrix} whose
rows comprise an \href{orthonormal_basis}{orthonormal basis} of
\href{eigenvector}{eigenvectors} of \(M\), and \(D\) is a \emph{real}
\href{diagonal_matrix}{diagonal matrix} whose \href{main_diagonal}{main
diagonal} contains the corresponding \href{eigenvalue}{eigenvalues}. The
matrix \(M\) may be regarded as a diagonal matrix \(D\) that has been
re-expressed in coordinates of the basis \(P\). In particular, the
one-to-one change of variable \(y = Pz\) shows that \(z^* Mz\) is real
and positive for any complex vector \(z\) if and only if \(y^* Dy\) is
real and positive for any \(y\); in other words, if \(D\) is positive
definite. For a diagonal matrix, this is true only if each element of
the main diagonal---that is, every eigenvalue of \(M\)---is positive.
Since the \href{spectral_theorem}{spectral theorem} guarantees all
eigenvalues of a Hermitian matrix to be real, the positivity of
eigenvalues can be checked using
\href{Descartes'_rule_of_signs}{Descartes' rule of alternating signs}
when the \href{characteristic_polynomial}{characteristic polynomial} of
a real, symmetric matrix \(M\) is available.

%\hypertarget{characterizations}{%
%\subsection{Characterizations}\label{characterizations}}
%
%Let \(M\) be an \(n \times n\) \href{Hermitian_matrix}{Hermitian
%matrix}. The following properties are equivalent to \(M\) being positive
%definite:
%
%\begin{itemize}
%\item[The associated sesquilinear form is an inner product:] The
%\href{sesquilinear_form}{sesquilinear form} defined by \(M\) is the
%function \(\langle \cdot, \cdot\rangle\) from
%\(\mathbb{C}^n \times \mathbb{C}^n\) to \(\mathbb{C}^n\) such that
%\(\langle x, y \rangle := y^*M x\) for all \(x\) and \(y\) in
%\(\mathbb{C}^n\), where \(y^*\) is the conjugate transpose of \(y\). For
%any complex matrix \(M\), this form is linear in \(x\) and semilinear in
%\(y\). Therefore, the form is an \href{inner_product}{inner product} on
%\(\mathbb{C}^n\) if and only if \(\langle z, z \rangle\) is real and
%positive for all nonzero \(z\); that is if and only if \(M\) is positive
%definite. (In fact, every inner product on \(\mathbb{C}^n\) arises in
%this fashion from a Hermitian positive definite matrix.)\\
%\item 
%It is the Gram matrix of a set of linearly independent vectors: Let
%\(x_1, \ldots, x_n\) be a list of \(n\)
%\href{linearly_independent}{linearly independent} vectors of some
%\href{complex_vector_space}{complex vector space} with an inner product
%\(\langle \cdot, \cdot \rangle\). It can be verified that the
%\href{Gram_matrix}{Gram matrix} \(M\) of those vectors, defined by
%\(M_{ij} = \langle x_i, x_j \rangle\), is always positive definite.
%Conversely, if \(M\) is positive definite, it has an eigendecomposition
%\(P^{-1} DP\) where \(P\) is unitary, \(D\) diagonal, and all diagonal
%elements \(D_{ii} = \lambda_i\) of \(D\) are real and positive. Let
%\(E\) be the real diagonal matrix with entries
%\(E_{ii} = \sqrt{\lambda_i}\) so \(E^2 = D\); then
%\(P^{-1}DP = P^*DP = P^*E EP = (EP)^* EP\). Now we let
%\(x_1, \ldots, x_n\) be the columns of \(EP\). These vectors are
%linearly independent, and by the above \(M\) is their Gram matrix, under
%the standard inner product of \(\mathbb{C}^n\), namely
%\(\langle x_i, x_j \rangle = x_i^\ast x_j\).\\
%Its leading principal minors are all positive: The \emph{k}th
%\href{minor_(linear_algebra)}{leading principal minor} of a matrix \(M\)
%is the \url{determinant} of its upper-left \(k \times k\) sub-matrix. It
%turns out that a matrix is positive definite if and only if all these
%determinants are positive. This condition is known as
%\href{Sylvester's_criterion}{Sylvester's criterion}, and provides an
%efficient test of positive definiteness of a symmetric real matrix.
%Namely, the matrix is reduced to an \href{upper_triangular_matrix}{upper
%triangular matrix} by using \href{elementary_row_operations}{elementary
%row operations}, as in the first part of the
%\href{Gaussian_elimination}{Gaussian elimination} method, taking care to
%preserve the sign of its determinant during
%\href{pivot_element}{pivoting} process. Since the \emph{k}th leading
%principal minor of a triangular matrix is the product of its diagonal
%elements up to row \(k\), Sylvester's criterion is equivalent to
%checking whether its diagonal elements are all positive. This condition
%can be checked each time a new row \(k\) of the triangular matrix is
%obtained.]
%\end{itemize}

\hypertarget{quadratic-forms}{%
\subsection{Quadratic forms}\label{quadratic-forms}}

The (purely) \href{quadratic_form}{quadratic form} associated with a
real \(n \times n\) matrix \(M\) is the function
\(Q : \mathbb{R}^n \to \mathbb{R}\) such that \(Q(x) = x^\top  Mx\)
for all \(x\). \(M\) can be assumed symmetric by replacing it with
\(\tfrac{1}{2} \left(M + M^\top \right)\).

A symmetric matrix \(M\) is positive definite if and only if its
quadratic form is a \href{strictly_convex_function}{strictly convex
function}.

More generally, any \href{quadratic_function}{quadratic function} from
\(\mathbb{R}^n\) to \(\mathbb{R}\) can be written as
\(x^\top  Mx + x^\top  b + c\) where \(M\) is a symmetric
\(n \times n\) matrix, \(b\) is a real \(n\)-vector, and \(c\) a real
constant. This quadratic function is strictly convex, and hence has a
unique finite global minimum, if and only if \(M\) is positive definite
. For this reason, positive definite matrices play an important role in
\href{optimization_(mathematics)}{optimization} problems.

%\hypertarget{simultaneous-diagonalization}{%
%\subsection{Simultaneous
%diagonalization}\label{simultaneous-diagonalization}}
%
%A symmetric matrix and another symmetric and positive definite matrix
%can be
%\href{diagonalizable_matrix\#Simultaneous_diagonalization}{simultaneously
%diagonalized}, although not necessarily via a
%\href{Matrix_similarity}{similarity transformation}. This result does
%not extend to the case of three or more matrices. In this section we
%write for the real case. Extension to the complex case is immediate.
%
%Let \(M\) be a symmetric and \(N\) a symmetric and positive definite
%matrix. Write the generalized eigenvalue equation as
%\((M - \lambda N)x = 0\) where we impose that \(x\) be normalized, i.e.
%\(x^\top  Nx = 1\). Now we use
%\href{Cholesky_decomposition}{Cholesky decomposition} to write the
%inverse of \(N\) as \(Q^\top  Q\). Multiplying by \(Q\) and letting
%\(x = Q^\top  y\), we get \(Q(M - \lambda N)Q^\top  y = 0\),
%which can be rewritten as \(\left(QMQ^\top \right)y = \lambda y\)
%where \(y^\top  y = 1\). Manipulation now yields \(MX = NX\Lambda\)
%where \(X\) is a matrix having as columns the generalized eigenvectors
%and \(\Lambda\) is a diagonal matrix of the generalized eigenvalues. Now
%premultiplication with \(X^\top \) gives the final result:
%\(X^\top  MX = \Lambda\) and \(X^\top  NX = I\), but note that
%this is no longer an orthogonal diagonalization with respect to the
%inner product where \(y^\top  y = 1\). In fact, we diagonalized
%\(M\) with respect to the inner product induced by \(N\).
%
%Note that this result does not contradict what is said on simultaneous
%diagonalization in the article
%\href{diagonalizable_matrix\#Simultaneous_diagonalization}{Diagonalizable
%matrix}, which refers to simultaneous diagonalization by a similarity
%transformation. Our result here is more akin to a simultaneous
%diagonalization of two quadratic forms, and is useful for optimization
%of one form under conditions on the other.\footnote{, p. 218 ff.}
%
\hypertarget{properties}{%
\subsection{Properties}\label{properties}}

%\hypertarget{induced-partial-ordering}{%
%\subsubsection{Induced partial
%ordering}\label{induced-partial-ordering}}
%
%For arbitrary square matrices \(M\), \(N\) we write \(M \ge N\) if
%\(M - N \ge 0\) i.e., \(M - N\) is positive semi-definite. This defines
%a \href{partially_ordered_set}{partial ordering} on the set of all
%square matrices. One can similarly define a strict partial ordering
%\(M > N\). The ordering is called the \href{Loewner_order}{Loewner
%order}.

\hypertarget{inverse-of-positive-definite-matrix}{%
\subsubsection{Inverse of positive definite
matrix}\label{inverse-of-positive-definite-matrix}}

Every positive definite matrix is \href{invertible_matrix}{invertible}
and its inverse is also positive definite.\footnote{, p. 397} If
\(M \geq N > 0\) then \(N^{-1} \geq M^{-1} > 0\).\footnote{, Corollary
  7.7.4(a)} Moreover, by the \href{min-max_theorem}{min-max theorem},
the \emph{k}th largest eigenvalue of \(M\) is greater than the
\emph{k}th largest eigenvalue of \(N\).

\hypertarget{scaling}{%
\subsubsection{Scaling}\label{scaling}}

If \(M\) is positive definite and \(r > 0\) is a real number, then
\(rM\) is positive definite.\footnote{, Observation 7.1.3}

\hypertarget{addition}{%
\subsubsection{Addition}\label{addition}}

If \(M\) and \(N\) are positive definite, then the sum \(M + N\) is also
positive definite.\footnote

\hypertarget{multiplication}{%
\subsubsection{Multiplication}\label{multiplication}}

\begin{itemize}
\item
  If \(M\) and \(N\) are positive definite, then the products \(MNM\)
  and \(NMN\) are also positive definite. If \(MN = NM\), then \(MN\) is
  also positive definite.
\end{itemize}

\begin{itemize}
\item
  If \(M\) is positive semidefinite, then \(Q^\top  MQ\) is
  positive semidefinite. If \(M\) is positive definite and \(Q\) has
  full column rank, then \(Q^\top  MQ\) is positive definite.
\end{itemize}

%"\textbf{Observation 7.1.8} Let \(A \in M_n\) be Hermitian and let
%\(C \in M_{n,m}\):
%
%\begin{itemize}
%
%\item
%  Suppose that is positive semidefinite. Then \(C^* AC\) is positive
%  semidefinite,
%  \(\operatorname{nullspace}C^* AC) = \operatorname{nullspace}(AC)\),
%  and \(\operatorname{rank}(C^* AC) = \operatorname{rank}(AC)^*\)
%\item
%  Suppose that is positive definite. Then
%  \(\operatorname{rank}(C^* AC) = \operatorname{rank}(C)\), and
%  \(C^* AC\) is positive definite \href{if_and_only_if}{if and only if}
%  rank() \{\{=\}\} "
%\end{itemize}

\hypertarget{cholesky-decomposition}{%
\subsubsection{Cholesky decomposition}\label{cholesky-decomposition}}

For any matrix \(A\), the matrix \(A^* A\) is positive semidefinite, and
\(\operatorname{rank}(A) = \operatorname{rank}(A^* A)\). Conversely, any
Hermitian positive semi-definite matrix \(M\) can be written as
\(M = LL^*\), where \(L\) is lower triangular; this is the
\href{Cholesky_decomposition}{Cholesky decomposition}. If \(M\) is not
positive definite, then some of the diagonal elements of \(L\) may be
zero.

A hermitian matrix \(M\) is positive definite if and only if it has a
unique Cholesky decomposition, i.e. the matrix \(M\) is positive
definite if and only if there exists a unique lower triangular matrix
\(L\), with real and strictly positive diagonal elements, such that
\(M = LL^*\).

\hypertarget{square-root}{%
\subsubsection{Square root}\label{square-root}}

A matrix \(M\) is positive semi-definite if and only if there is a
positive semi-definite matrix \(B\) with \(B^2 = M\). This matrix \(B\)
is unique,\footnote{, Theorem 7.2.6 with \(k = 2\)} is called the
\href{square_root_of_a_matrix}{square root} of \(M\), and is denoted
with \(B = M^\frac{1}{2}\) (the square root \(B\) is not to be confused
with the matrix \(L\) in the \href{Cholesky_factorization}{Cholesky
factorization} \(M = LL^*\), which is also sometimes called the square
root of \(M\)).

If \(M > N > 0\) then \(M^\frac{1}{2} > N^\frac{1}{2} > 0\).

\hypertarget{submatrices}{%
\subsubsection{Submatrices}\label{submatrices}}

Every principal submatrix of a positive definite matrix is positive
definite.

%\hypertarget{trace}{%
%\subsubsection{Trace}\label{trace}}
%
%The diagonal entries \(m_{ii}\) of a positive definite matrix are real
%and non-negative. As a consequence the
%\href{trace_(linear_algebra)}{trace}, \(\operatorname{tr}(M) \ge 0\).
%Furthermore,\footnote{, p. 398} since every principal sub-matrix (in
%particular, 2-by-2) is positive definite,
%
%\begin{description}
%\item
%\(\left|m_{ij}\right| \leq \sqrt{m_{ii}m_{jj}} \leq \frac{m_{ii} + m_{jj}}{2} \quad \forall i, j\)
%\end{description}
%
%and thus
%
%\begin{description}
%\item
%\(\max_{i,j} \left|m_{ij}\right| \leq \max_i\left|m_{ii}\right|\)
%\end{description}
%
%An \(n \times n\) Hermitian matrix \(M\) is positive definite if it
%satisfies the following trace inequalities:\footnote
%
%\begin{description}
%\item
%\(\operatorname{tr}(M) > 0  \quad \mathrm{and} \quad \frac{(\operatorname{tr}(M))^2}{\operatorname{tr}(M^2)} > n-1.\)
%\end{description}

%\hypertarget{hadamard-product}{%
%\subsubsection{Hadamard product}\label{hadamard-product}}
%
%If \(M, N \geq 0\), although \(MN\) is not necessary positive
%semidefinite, the \href{Hadamard_product_(matrices)}{Hadamard product}
%\(M \circ N \geq 0\) (this result is often called the
%\href{Schur_product_theorem}{Schur product theorem}).\footnote{, Theorem
%  7.5.3}
%
%Regarding the Hadamard product of two positive semidefinite matrices
%\(M = (m_{ij}) \geq 0\), \(N \geq 0\), there are two notable
%inequalities:
%
%\begin{itemize}
%\item
%  Oppenheim's inequality:
%  \(\det(M \circ N) \geq \det (N) \prod\nolimits_i m_{ii}.\)\footnote{,
%    Theorem 7.8.6}
%\item
%  \(\det(M \circ N) \geq \det(M) \det(N)\).
%\end{itemize}

%, Corollary 3.6, p. 227
%
%\hypertarget{kronecker-product}{%
%\subsubsection{Kronecker product}\label{kronecker-product}}
%
%If \(M, N \geq 0\), although \(MN\) is not necessary positive
%semidefinite, the \href{Kronecker_product}{Kronecker product}
%\(M \otimes N \geq 0\).
%
%\hypertarget{frobenius-product}{%
%\subsubsection{Frobenius product}\label{frobenius-product}}
%
%If \(M, N \geq 0\), although \(MN\) is not necessary positive
%semidefinite, the
%\href{Matrix_multiplication\#Frobenius_product}{Frobenius product}
%\(M : N \geq 0\) (Lancaster--Tismenetsky, \emph{The Theory of Matrices},
%p.218).

\hypertarget{convexity}{%
\subsection{Convexity}\label{convexity}}

The set of positive semidefinite symmetric matrices is
\href{convex_set}{convex}. That is, if \(M\) and \(N\) are positive
semidefinite, then for any \(\alpha\) between 0 and 1,
\(\alpha M + (1 - \alpha) N\) is also positive semidefinite. For any
vector \(x\):

\[x^\top  \left(\alpha M + (1 - \alpha)N\right)x = \alpha x^\top  Mx + (1 - \alpha) x^\top  Nx \geq 0.\]

This property guarantees that
\href{semidefinite_programming}{semidefinite programming} problems
converge to a globally optimal solution.

%\hypertarget{relation-with-cosine}{%
%\subsubsection{Relation with cosine}\label{relation-with-cosine}}
%
%The positive-definiteness of a matrix \(A\) expresses that the angle
%\(\theta\) between any vector \(x\) and its image \(Ax\) is always
%\(-\pi / 2 < \theta < +\pi / 2\):
%
%\begin{description}
%\item
%\(\cos\theta = \frac{x^{T}Ax}{\left\Vert x \right\Vert \left\Vert Ax \right\Vert}=\frac{\langle x,Ax \rangle}{\left\Vert x \right\Vert \left\Vert Ax \right\Vert} ,  \theta=\theta(x,Ax)=\widehat{x,Ax}= \text{the angle between } x \text{ and } Ax\)
%\end{description}

\hypertarget{further-properties}{%
\subsubsection{Further properties}\label{further-properties}}

A Hermitian matrix is positive semidefinite if and only if all of its
principal minors are nonnegative. It is however not enough to consider
the leading principal minors only, as is checked on the diagonal matrix
with entries 0 and 1.

\hypertarget{block-matrices}{%
\subsubsection{Block matrices}\label{block-matrices}}

A positive \(2n \times 2n\) matrix may also be defined by
\href{block_matrix}{blocks}:

\begin{description}
\item
\(M =  \begin{bmatrix} A & B \\ C & D \end{bmatrix}\)
\end{description}

where each block is \(n \times n\). By applying the positivity
condition, it immediately follows that \(A\) and \(D\) are hermitian,
and \(C = B^*\).

We have that \(z^* Mz \ge 0\) for all complex \(z\), and in particular
for \(z = [v, 0]^\top \). Then

\[\begin{bmatrix} v^* & 0 \end{bmatrix} \begin{bmatrix} A & B \\ B^* & D \end{bmatrix} \begin{bmatrix} v \\ 0 \end{bmatrix} = v^* Av \ge 0.\]


A similar argument can be applied to \(D\), and thus we conclude that
both \(A\) and \(D\) must be positive definite matrices, as well.

Converse results can be proved with stronger conditions on the blocks,
for instance using the
\href{Schur_complement\#Schur_complement_condition_for_positive_definiteness}{Schur
complement}.

\hypertarget{local-extrema}{%
\subsubsection{Local extrema}\label{local-extrema}}

A general \href{quadratic_form}{quadratic form} \(f(\mathbf{x})\) on
\(n\) real variables \(x_1, \ldots, x_n\) can always be written as
\(\mathbf{x}^\top  M \mathbf{x}\) where \(\mathbf{x}\) is the
column vector with those variables, and \(M\) is a symmetric real
matrix. Therefore, the matrix being positive definite means that \(f\)
has a unique minimum (zero) when \(\mathbf{x}\) is zero, and is strictly
positive for any other \(\mathbf{x}\).

More generally, a twice-differentiable real function \(f\) on \(n\) real
variables has local minimum at arguments \(x_1, \ldots, x_n\) if its
\url{gradient} is zero and its \href{Hessian_matrix}{Hessian} (the
matrix of all second derivatives) is positive semi-definite at that
point. Similar statements can be made for negative definite and
semi-definite matrices.

\hypertarget{covariance}{%
\subsubsection{Covariance}\label{covariance}}

In \url{statistics}, the \href{covariance_matrix}{covariance matrix} of
a \href{multivariate_probability_distribution}{multivariate probability
distribution} is always positive semi-definite; and it is positive
definite unless one variable is an exact linear function of the others.
Conversely, every positive semi-definite matrix is the covariance matrix
of some multivariate distribution.


%\hypertarget{references}{%
%\subsection{References}\label{references}}


\hypertarget{external-links}{%
\subsection{External links}\label{external-links}}

\begin{itemize}
\item
  \href{http://mathworld.wolfram.com/PositiveDefiniteMatrix.html}{Wolfram
  MathWorld: Positive Definite Matrix}
\end{itemize}

%\href{de:Definitheit\#Definitheit_von_Matrizen}{de:Definitheit\#Definitheit
%von Matrizen}
%
%\url{Category:Matrices}
%
%\end{document}
