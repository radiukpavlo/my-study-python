\section{Subspaces and Basis}

\begin{outcome}
\begin{enumerate}
\item[A.] Utilize the subspace test to determine if a set is a subspace of a given vector space.

\item[B.] Extend a linearly independent set and shrink a spanning set to a basis of a given vector space. 
\end{enumerate}
\end{outcome}

In this section we will examine the concept of subspaces introduced earlier in terms of $\mathbb{R}^n$. Here, we will discuss these concepts in terms of abstract vector spaces. 

Consider the definition of a subspace.

\begin{definition}{Subspace}{subspace}
Let $V$ be a vector space. A subset $W\subseteq V$ is said to be a\index{subspace} \textbf{subspace} of $V$ if $a\vect{x}+b\vect{y}
\in W$ whenever $a,b\in \mathbb{R}$ and $\vect{x},\vect{y}\in W.$
\end{definition}

The span of a set of vectors as described in Definition \ref{def:span} is an example of a subspace. The following fundamental result says that subspaces are subsets of a
vector space which are themselves vector spaces.

\begin{theorem}{Subspaces are Vector Spaces}{subspacesarevectorspaces}
Let $W$ be a nonempty collection of vectors in a vector space $V$. Then $W$
is a subspace if and only if $W$ satisfies the vector space axioms, using the same
operations as those defined on $V$.
\end{theorem}

\begin{proof}
Suppose first that $W$ is a subspace. It is obvious that
all the algebraic laws hold on $W$ because it is a subset of $V$ and they
hold on $V$. Thus $\vect{u}+\vect{v}=\vect{v}+\vect{u}$ along with the other axioms. Does $W$
contain $\vect{0}?$ Yes because it contains $0\vect{u}=\vect{0}$. See
Theorem \ref{thm:axiomuniqueness}.

 Are the operations of $V$ defined on $W?$ That is,
when you add vectors of $W$ do you get a vector in $W?$ When you multiply a
vector in $W$ by a scalar, do you get a vector in $W?$ Yes. This is
contained in the definition. Does every vector in $W$ have an additive
inverse? Yes by Theorem \ref{thm:axiomuniqueness} because $-\vect{v}=\left(
-1\right) \vect{v}$ which is given to be in $W$ provided $\vect{v}\in W$.

Next suppose $W$ is a vector space. Then by definition, it is closed with
respect to linear combinations. Hence it is a subspace. 
\end{proof}

Consider the following useful Corollary.

\begin{corollary}{Span is a Subspace}{spansubspace}
Let $V$ be a vector space with $W \subseteq V$. If $W = \func{span} \left\{ \vect{v}_1, \cdots,  \vect{v}_n \right\}$ then $W$ is a subspace of $V$.
\end{corollary}

When determining spanning sets the following theorem proves useful.

\begin{theorem}{Spanning Set}{spanningset}
Let $W \subseteq V$ for a vector space $V$ and suppose $W = \func{span} \left\{ \vect{v}_1, \vect{v}_2, \cdots, \vect{v}_n \right\}$. 

Let $U \subseteq V$ be a subspace such that $\vect{v}_1, \vect{v}_2, \cdots, \vect{v}_n \in U$. Then it follows that $W \subseteq U$. 
\end{theorem}

In other words, this theorem claims that any subspace that contains a set of vectors must also contain the span of these vectors. 

The following example will show that two spans, described differently, can in fact be equal. 

\begin{example}{Equal Span}{equalspan}
Let $p(x), q(x)$ be polynomials and suppose $U = \func{span}\left\{ 2p(x) - q(x), p(x) + 3q(x)\right\} $ and $W =  \func{span}\left\{ p(x), q(x) \right\}$. Show that $U = W$. 
\end{example}

\begin{solution}
We will use Theorem \ref{thm:spanningset} to show that $U \subseteq W$ and $W \subseteq U$. It will then follow that $U=W$. 
\begin{enumerate}
\item $U \subseteq W$

Notice that $2p(x) - q(x)$ and $p(x) + 3q(x)$  are both in $W = \func{span} \left\{ p(x), q(x) \right\}$. Then by Theorem \ref{thm:spanningset} $W$ must contain the span of these polynomials and so $U \subseteq W$. 

\item $W \subseteq U$

Notice that 
\begin{eqnarray*}
p(x) &=& \frac{3}{7} \left( 2p(x) - q(x) \right)  + \frac{2}{7} \left( p(x) + 3q(x)\right) \\
q(x) &=& -\frac{1}{7} \left( 2p(x) - q(x) \right)  + \frac{2}{7} \left( p(x) + 3q(x)\right)
\end{eqnarray*}
Hence $p(x), q(x)$ are in $\func{span} \left\{ 2p(x) - q(x), p(x) + 3q(x) \right\}$. By Theorem \ref{thm:spanningset} $U$ must contain the span of these polynomials and so $W \subseteq U$. 
\end{enumerate}
\end{solution}

To prove that a set is a vector space, one must verify each of the axioms given in Definition \ref{def:vectorspaceaxiomsaddition} and \ref{def:vectorspaceaxiomsscalarmult}. This is a cumbersome task, and therefore a shorter procedure is used to verify a subspace. 

\begin{procedure}{Subspace Test}{subspacetest}
Suppose $W$ is a subset of a vector space $V$. To determine if $W$ is a subspace of $V$, it is sufficient to determine if the following three conditions hold, using the operations of $V$: 
\begin{enumerate}
\item
The additive identity $\vect{0}$ of $V$ is contained in $W$.
\item
For any vectors $\vect{w}_1, \vect{w}_2$ in $W$, $\vect{w}_1 + \vect{w}_2$ is also in $W$.
\item
For any vector $\vect{w}_1$ in $W$ and scalar $a$,  the product $a\vect{w}_1$ is also in $W$. 
\end{enumerate}
\end{procedure}

Therefore it suffices to prove these three steps to show that a set is a subspace. 

Consider the following example.

\begin{example}{Improper Subspaces}{impropersubspaces}
Let $V$ be an arbitrary vector space. Then $V$ is a subspace of itself. Similarly, the set $\left\{ \vect{0} \right\}$ containing only the zero vector is also a subspace. 
\end{example}

\begin{solution}
Using the subspace test in Procedure \ref{proc:subspacetest} we can show that $V$ and $\left\{ \vect{0} \right\}$ are subspaces of $V$. 

Since $V$ satisfies the vector space axioms it also satisfies the three steps of the subspace test. Therefore $V$ is a subspace.

Let's consider the set $\left\{ \vect{0} \right\}$. 
\begin{enumerate}
\item
The vector $\vect{0}$ is clearly contained in  $\left\{ \vect{0} \right\}$, so the first condition is satisfied.

\item
Let $\vect{w}_1, \vect{w}_2$ be in  $\left\{ \vect{0} \right\}$. Then $\vect{w}_1 = \vect{0}$ and $\vect{w}_2 = \vect{0}$ and so 
\[
\vect{w}_1 + \vect{w}_2 = \vect{0} + \vect{0} = \vect{0}
\]
It follows that the sum is contained in $\left\{ \vect{0} \right\}$ and the second condition is satisfied. 

\item
Let $\vect{w}_1$ be in  $\left\{ \vect{0} \right\}$ and let $a$ be an arbitrary scalar. Then
\[
a\vect{w}_1  = a\vect{0} = \vect{0}
\]
Hence the product is contained in  $\left\{ \vect{0} \right\}$ and the third condition is satisfied. 
\end{enumerate}

It follows that  $\left\{ \vect{0} \right\}$ is a subspace of $V$. 
\end{solution}

The two subspaces described above are called \textbf{improper subspaces}.\index{improper subspace} Any subspace of a vector space $V$ which is not equal to $V$ or  $\left\{ \vect{0} \right\}$ is called a \textbf{proper subspace}.\index{proper subspace} 

Consider another example.

\begin{example}{Subspace of Polynomials}{polysubspace}
Let $\mathbb{P}_2$ be the vector space of polynomials of degree two or less. Let $W \subseteq \mathbb{P}_2$ be all polynomials of degree two or less which have $1$ as a root. Show that $W$ is a subspace of $\mathbb{P}_2$. 
\end{example}

\begin{solution}
First, express $W$ as follows:
\[
W = \left\{ p(x) = ax^2 +bx +c, a,b,c, \in \mathbb{R} | p(1)  = 0 \right\}
\]

We need to show that $W$ satisfies the three conditions of Procedure \ref{proc:subspacetest}. 
\begin{enumerate}
\item
The zero polynomial of $\mathbb{P}_2$ is given by $0(x) = 0x^2 + 0x + 0 = 0$. Clearly $0(1) = 0$ so $0(x)$ is contained in $W$. 

\item
Let $p(x), q(x)$ be polynomials in $W$.  It follows that $p(1) = 0 $ and $q(1) = 0$. Now consider $p(x) + q(x)$. Let $r(x)$ represent this sum.
\begin{eqnarray*}
r(1) &=& p(1) + q(1) \\
&=& 0 + 0 \\
&=& 0
\end{eqnarray*}

Therefore the sum is also in $W$ and the second condition is satisfied. 

\item
Let $p(x)$ be a polynomial in $W$ and let $a$ be a scalar. It follows that $p(1) = 0$. Consider the product $ap(x)$. 
\begin{eqnarray*}
ap(1) &=& a(0) \\
&=& 0
\end{eqnarray*}

Therefore the product is in $W$ and the third condition is satisfied.
\end{enumerate}

It follows that $W$ is a subspace of $\mathbb{P}_2$. 
\end{solution} 

Recall the definition of basis, considered now in the context of vector spaces.

\begin{definition}{Basis}{basisvectorspace}
Let $V$ be a vector space. Then $\{\vect{v}_{1},\cdots ,\vect{v}_{n}\}$ is called a basis
\index{basis} for $V$ if the following conditions hold.
\begin{enumerate}
\item
$\func{span}\left\{ \vect{v}_{1},\cdots ,\vect{v}_{n}\right\} = V$
\item
$\{\vect{v}_{1},\cdots ,\vect{v}_{n}\}$ is linearly independent
\end{enumerate}
\end{definition}

Consider the following example.

\begin{example}{Polynomials of Degree Two}{polydegreetwo}
Let $\mathbb{P}_2$ be the set polynomials of degree no more than 2. We can write
$\mathbb{P}_2=\func{span}\left\{ x^{2}, x, 1\right\} .$ Is $\left\{ x^{2}, x, 1\right\} $ a
basis for $\mathbb{P}_2$?
\end{example}

\begin{solution}
It can be verified that $\mathbb{P}_2$ is a vector space defined under the usual addition and scalar multiplication of polynomials. 

Now, since $\mathbb{P}_2=\func{span}\left\{ x^{2},x, 1\right\}$, the set  $\left\{ x^{2}, x, 1\right\} $ is a basis if it is linearly independent. Suppose then that 
\begin{equation*}
ax^{2}+bx+c=0x^2 + 0x + 0 
\end{equation*}
where $a,b,c$ are real numbers. It is clear that this can only occur if $a=b=c=0$. Hence the set is linearly independent and forms a basis of $\mathbb{P}_2$.
\end{solution}

The next theorem is an essential result in linear algebra and is called the exchange theorem. 
\index{exchange theorem}

\begin{theorem}{Exchange Theorem}{exchangetheorem}
Let $\left\{ \vect{x}_{1},\cdots ,\vect{x}_{r}\right\} $
be a linearly independent set of vectors such that each $\vect{x}_{i}$ is
contained in span$\left\{ \vect{y}_{1},\cdots ,\vect{y}_{s}\right\} .$ Then $
r\leq s.$
\end{theorem}

\begin{proof} The proof will proceed as follows. First, we set up the necessary steps for the proof. Next, we will assume that $r > s$ and show that this leads to a contradiction, thus requiring that $r \leq s$. 

Define span$\left\{ \vect{y}_{1},\cdots ,\vect{y}_{s}\right\} = V.$ Since each $\vect{x}_i$ is in  span$\left\{ \vect{y}_{1},\cdots ,\vect{y}_{s}\right\}$, it follows there exist scalars $c_{1},\cdots ,c_{s}$
such that 
\begin{equation}
\vect{x}_{1}=\sum_{i=1}^{s}c_{i}\vect{y}_{i}  \label{lincomb}
\end{equation}
Note that not all of these scalars $c_i$ can equal zero. Suppose that all the $c_i=0$. Then it
would follow that $\vect{x}_{1}=\vect{0}$ and so $\left\{ \vect{x}
_{1},\cdots ,\vect{x}_{r}\right\} $ would not be linearly independent.
Indeed, if $\vect{x}_{1}=\vect{0}$, $1\vect{x}_{1}+\sum_{i=2}^{r}0
\vect{x}_{i}=\vect{x}_{1}=\vect{0}$ and so there would exist a
nontrivial linear combination of the vectors $\left\{ \vect{x}_{1},\cdots ,
\vect{x}_{r}\right\} $ which equals zero. Therefore at least one $c_i$ is nonzero. 

Say $c_{k}\neq 0.$ Then solve \ref{lincomb} for $\vect{y}_{k}$ and obtain 
\begin{equation*}
\vect{y}_{k}\in \func{span}\left\{ \vect{x}_{1},\overset{\text{s-1
vectors here}}{\overbrace{\vect{y}_{1},\cdots ,\vect{y}_{k-1},\vect{y}
_{k+1},\cdots ,\vect{y}_{s}}}\right\} .
\end{equation*}
Define $\left\{ \vect{z}_{1},\cdots ,\vect{z}_{s-1}\right\} $ to be
\begin{equation*}
\left\{ \vect{z}_{1},\cdots ,\vect{z}_{s-1}\right\} = \left\{ 
\vect{y}_{1},\cdots ,\vect{y}_{k-1},\vect{y}_{k+1},\cdots ,\vect{y}
_{s}\right\}
\end{equation*}
Now we can write 
\begin{equation*}
\vect{y}_{k}\in \func{span}\left\{ \vect{x}_{1}, \vect{z}_{1},\cdots, \vect{z}_{s-1}\right\} 
\end{equation*}
Therefore, $\func{span}\left\{ \vect{x}_{1},\vect{z}_{1},\cdots ,\vect{z
}_{s-1}\right\}=V$. To see this, suppose $\vect{v}\in V$. Then there exist constants $
c_{1},\cdots ,c_{s}$ such that 
\begin{equation*}
\vect{v}=\sum_{i=1}^{s-1}c_{i}\vect{z}_{i}+c_{s}\vect{y}_{k}.
\end{equation*}
Replace this $\vect{y}_{k}$ with a linear combination of the
vectors $\left\{ \vect{x}_{1},\vect{z}_{1},\cdots ,\vect{z}_{s-1}\right\}$
to obtain $\vect{v}\in \func{span}\left\{ \vect{x}_{1},\vect{z}
_{1},\cdots ,\vect{z}_{s-1}\right\} .$ The vector $\vect{y}_{k},$ in the
list $\left\{ \vect{y}_{1},\cdots ,\vect{y}_{s}\right\} ,$ has now been
replaced with the vector $\vect{x}_{1}$ and the resulting modified list of
vectors has the same span as the original list of vectors, $\left\{ \vect{y
}_{1},\cdots ,\vect{y}_{s}\right\} .$

We are now ready to move on to the proof. Suppose that $r>s$ and that 
\[
\func{span}\left\{ \vect{x}_{1},\cdots ,
\vect{x}_{l},\vect{z}_{1},\cdots ,\vect{z}_{p}\right\} =V
\]
 where the process established above has continued. In other words, the vectors $\vect{z}_{1},\cdots ,\vect{z}_{p}$ are each taken from the
set $\left\{ \vect{y}_{1},\cdots ,\vect{y}_{s}\right\} $ and $l+p=s.$
This was done for $l=1$ above. Then since $r>s,$ it follows that $
l\leq s<r$ and so $l+1\leq r.$ Therefore, $\vect{x}_{l+1}$ is a vector not
in the list, $\left\{ \vect{x}_{1},\cdots ,\vect{x}_{l}\right\} $ and
since 
\[
\func{span}\left\{ \vect{x}_{1},\cdots ,\vect{x}_{l},\vect{z}
_{1},\cdots ,\vect{z}_{p}\right\} =V
\]
 there exist scalars, $c_{i}$ and $
d_{j}$ such that 
\begin{equation}
\vect{x}_{l+1}=\sum_{i=1}^{l}c_{i}\vect{x}_{i}+\sum_{j=1}^{p}d_{j}
\vect{z}_{j}.  \label{lincomb2}
\end{equation}
Not all the $d_{j}$ can equal zero because if this were so, it would follow
that $\left\{ \vect{x}_{1},\cdots ,\vect{x}_{r}\right\} $ would be a
linearly dependent set because one of the vectors would equal a linear
combination of the others. Therefore, \ref{lincomb2} can be solved for one of the 
$\vect{z}_{i},$ say $\vect{z}_{k},$ in terms of $\vect{x}_{l+1}$ and
the other $\vect{z}_{i}$ and just as in the above argument, replace that $
\vect{z}_{i}$ with $\vect{x}_{l+1}$ to obtain 
\begin{equation*}
\func{span}\left\{ \vect{x}_{1},\cdots \vect{x}_{l},\vect{x}_{l+1},
\overset{\text{p-1 vectors here}}{\overbrace{\vect{z}_{1},\cdots \vect{z}
_{k-1},\vect{z}_{k+1},\cdots ,\vect{z}_{p}}}\right\} =V
\end{equation*}
Continue this way, eventually obtaining 
\begin{equation*}
\func{span}\left\{ \vect{x}_{1},\cdots ,\vect{x}_{s}\right\} =V.
\end{equation*}
But then $\vect{x}_{r}\in $ $\func{span}\left\{ \vect{x}_{1},\cdots ,
\vect{x}_{s}\right\} $ contrary to the assumption that $\left\{ \vect{x}
_{1},\cdots ,\vect{x}_{r}\right\} $ is linearly independent. Therefore, $
r\leq s$ as claimed.
\end{proof}

The following corollary follows from the exchange theorem.

\begin{corollary}{Two Bases of the Same Length}{baseslength}
Let $B_1$, $B_2$ be two bases of a vector space $V$. Suppose $B_1$ contains $m$ vectors and $B_2$ contains $n$ vectors. Then $m = n$. 
\end{corollary}

\begin{proof} By Theorem \ref{thm:exchangetheorem}, $m\leq n$ and $n\leq m$. Therefore $m=n$. 
\end{proof}

This corollary is very important so we provide another proof independent of the exchange theorem above.
\index{basis!any two same size}

\begin{proof}Suppose $n > m.$ Then since the vectors $\left\{ \vect{u}
_{1},\cdots ,\vect{u}_{m}\right\} $ span $V,$ there exist scalars $c_{ij}$
such that 
\begin{equation*}
\sum_{i=1}^{m}c_{ij}\vect{u}_{i}=\vect{v}_{j}.
\end{equation*}
Therefore, 
\begin{equation*}
\sum_{j=1}^{n}d_{j}\vect{v}_{j}=\vect{0}
\text{ if and only if }\sum_{j=1}^{n}\sum_{i=1}^{m}c_{ij}d_{j}\vect{u}_{i}=
\vect{0}
\end{equation*}
if and only if 
\begin{equation*}
\sum_{i=1}^{m}\left( \sum_{j=1}^{n}c_{ij}d_{j}\right) \vect{u}_{i}=\vect{
0}
\end{equation*}
Now since $\{\vect{u}_{1},\cdots ,\vect{u}_{n}\}$ is independent, this
happens if and only if 
\begin{equation*}
\sum_{j=1}^{n}c_{ij}d_{j}=0,\;i=1,2,\cdots ,m.
\end{equation*}
However, this is a system of $m$ equations in $n$ variables, $d_{1},\cdots
,d_{n}$ and $m<n.$ Therefore, there exists a solution to this system of
equations in which not all the $d_{j}$ are equal to zero. Recall why this is
so. The augmented matrix for the system is of the form 
$\leftB 
\begin{array}{c|c}
C & \vect{0}
\end{array}
\rightB $ where $C$ is a matrix which has more columns than rows. Therefore,
there are free variables and hence nonzero solutions to the system of
equations. However, this contradicts the linear independence of $\left\{ 
\vect{u}_{1},\cdots ,\vect{u}_{m}\right\} $. Similarly it cannot happen
that $m > n$.
\end{proof}

Given the result of the previous corollary, the following definition follows.

\begin{definition}{Dimension}{dimensionvectorspace}
 A vector space $V$ is of dimension $n$ if it has a basis consisting of $n$ vectors. \index{dimension of vector space}
\index{vector space!dimension} 
\end{definition}

Notice that the dimension is well defined by Corollary \ref{cor:baseslength}. It is assumed here
that $n<\infty $ and therefore such a vector space is said to be \textbf{finite
dimensional}.\index{finite dimensional}

\begin{example}{Dimension of a Vector Space}{dimension}
Let $\mathbb{P}_2$ be the set of all polynomials of degree at most $2$. Find the dimension of $\mathbb{P}_2$. 
\end{example}

\begin{solution}
If we can find a basis of $\mathbb{P}_2$ then the number of vectors in the basis will give the dimension. Recall from Example \ref{exa:polydegreetwo} that a basis of $\mathbb{P}_2$ is given by 
\[
S  = \left\{ x^2, x, 1 \right\}
\]
There are three polynomials in $S$ and hence the dimension of $\mathbb{P}_2$ is three. 
\end{solution}

It is important to note that a basis for a vector space is not unique. A vector space can have many bases. Consider the following example.

\begin{example}{A Different Basis for Polynomials of Degree Two}{polydegtwodifferentbasis}
Let $\mathbb{P}_2$ be the polynomials of degree no more than 2. Is $\left\{
x^{2}+x+1,2x+1,3x^{2}+1\right\} $ a basis for $\mathbb{P}_2$?
\end{example}

\begin{solution}
Suppose these vectors are linearly independent but do not form a spanning set for $\mathbb{P}_2$. Then by Lemma \ref{lem:addinglinearlyindependent}, we could find a fourth polynomial in $\mathbb{P}_2$ to create a new linearly independent 
set containing four polynomials. However this would imply that we could find a basis of $\mathbb{P}_2$ of more than three polynomials. This contradicts the result of Example \ref{exa:dimension} in which we determined the dimension of $\mathbb{P}_2$ is three.  Therefore if these vectors are linearly independent they must also form a spanning set and thus a basis for $\mathbb{P}_2$. 

Suppose then that 
\begin{eqnarray*}
a\left( x^{2}+x+1\right) +b\left( 2x+1\right) +c\left( 3x^{2}+1\right) &=& 0\\
\left( a+3c\right) x^{2}+\left( a+2b\right) x+\left( a+b+c\right) &=& 0 
\end{eqnarray*}
We know that $\left\{ x^2, x, 1 \right\}$ is linearly independent, and so it follows that  
\begin{eqnarray*}
a+3c &=& 0 \\
a+2b &=& 0 \\
a+b+c &=& 0
\end{eqnarray*}
and there is only one solution to this system of equations, $a=b=c=0$.
Therefore, these are linearly independent and form a basis for $\mathbb{P}_2$.
\end{solution}

Consider the following theorem. 

\begin{theorem}{Every Subspace has a Basis}{everysubspacebasis}
Let $W$ be a nonzero subspace of a finite dimensional vector
space $V$. Suppose $V$ has dimension $n$.
\index{subspace!has a basis} Then $W$ has a basis with no more than $n$
vectors.
\end{theorem}

\begin{proof}
Let $\vect{v}_{1}\in V$ where $\vect{v}_{1}\neq 0.$ If $
\func{span}\left\{ \vect{v}_{1}\right\} =V,$ then it follows that $\left\{ \vect{v}
_{1}\right\} $ is a basis for $V$. Otherwise, there exists $\vect{v}
_{2}\in V$ which is not in $\func{span}\left\{ \vect{v}_{1}\right\} .$ By
Lemma \ref{lem:addinglinearlyindependent} $\left\{ \vect{v}_{1},\vect{v}_{2}\right\} $ is a
linearly independent set of vectors. Then $\left\{ \vect{v}_{1},\vect{v}
_{2}\right\} $ is a basis for $V$ and we are done. If $\func{span}\left\{ \vect{v}_{1},
\vect{v}_{2}\right\} \neq V,$ then there exists $\vect{v}_{3}\notin \func{
span}\left\{ \vect{v}_{1},\vect{v}_{2}\right\} $ and $\left\{ \vect{v}
_{1},\vect{v}_{2},\vect{v}_{3}\right\} $ is a larger linearly
independent set of vectors. Continuing this way, the process must stop
before $n+1$ steps because if not, it would be possible to obtain $n+1$
linearly independent vectors contrary to the exchange theorem, Theorem \ref{thm:exchangetheorem}. 
\end{proof}

If in fact $W$ has $n$ vectors, then it follows that $W=V$. 

\begin{theorem}{Subspace of Same Dimension}{subspacevectorspace}
Let $V$ be a vector space of dimension $n$ and let $W$ be a
subspace. Then $W=V$ if and only if the dimension of $W$ is also $n$.
\end{theorem}

\begin{proof}First suppose $W=V.$ Then obviously the dimension of $W=n.$

Now suppose that the dimension of $W$ is $n$. Let a basis for $W$ be $
\left\{ \vect{w}_{1},\cdots ,\vect{w}_{n}\right\} $. If $W$ is not equal to $V$
, then let $\vect{v}$ be a vector of $V$ which is not contained in $W.$ Thus $
\vect{v}$ is not in $\func{span}\left\{ \vect{w}_{1},\cdots ,\vect{w}
_{n}\right\} $ and by Lemma \ref{lem:basesisomorphism}, $\left\{ \vect{w}_{1},\cdots ,\vect{w}_{n},\vect{v}\right\} $ is linearly independent which contradicts
Theorem \ref{thm:exchangetheorem} because it would be an independent set of $n+1$
vectors even though each of these vectors is in a spanning set of $n$
vectors, a basis of $V$. 
\end{proof}

Consider the following example.

\begin{example}{Basis of a Subspace}{basissubspace}
Let $U=\left\{ A\in\mathbb{M}_{22} ~\left|~
A\leftB\begin{array}{rr}
1 & 0 \\ 1 & -1 \end{array}\rightB\right. 
= \leftB\begin{array}{rr}
1 & 1 \\ 0 & -1 \end{array}\rightB A \right\}$. 
Then $U$ is a subspace of $\mathbb{M}_{22}$ 
Find a basis of $U$, and hence $\dim(U)$.
\end{example}

\begin{solution}
Let $A=\leftB\begin{array}{rr} a & b \\ c & d \end{array}\rightB
\in\mathbb{M}_{22}$.
Then
\[ A\leftB\begin{array}{rr} 1 & 0 \\ 1 & -1 \end{array}\rightB
= \leftB\begin{array}{rr} a & b \\ c & d \end{array}\rightB
\leftB\begin{array}{rr} 1 & 0 \\ 1 & -1 \end{array}\rightB
=\leftB\begin{array}{rr} a+b & -b \\ c+d & -d \end{array}\rightB\]
and
\[ \leftB\begin{array}{rr} 1 & 1 \\ 0 & -1 \end{array}\rightB A
= \leftB\begin{array}{rr} 1 & 1 \\ 0 & -1 \end{array}\rightB
\leftB\begin{array}{rr} a & b \\ c & d \end{array}\rightB
=\leftB\begin{array}{cc} a+c & b+d \\ -c & -d \end{array}\rightB.\]
If $A\in U$, then
$\leftB\begin{array}{cc} a+b & -b \\ c+d & -d \end{array}\rightB=
\leftB\begin{array}{cc} a+c & b+d \\ -c & -d \end{array}\rightB$.

Equating entries leads to a system of four equations in the four
variables $a,b,c$ and $d$.
\[ \begin{array}{ccc}
a+b & = & a + c \\
-b & = & b + d \\
c + d & = & -c \\
-d & = & -d \end{array} \hspace*{.2in}\mbox{ or }\hspace*{.2in}
\begin{array}{rcc}
b - c & = & 0 \\
-2b - d & = & 0 \\
2c + d & = & 0 
\end{array}.  \] 

The solution to this system is
$a=s$, $b=-\frac{1}{2}t$, $c=-\frac{1}{2}t$,  $d=t$ for any $s,t\in\mathbb{R}$, 
and thus 
\[ A=\leftB\begin{array}{cc} s & \frac{t}{2} \\
-\frac{t}{2} & t \end{array}\rightB
= s\leftB\begin{array}{cc} 1 & 0 \\ 0 & 0 \end{array}\rightB
+ t\leftB\begin{array}{rr} 0  & -\frac{1}{2} \\ 
-\frac{1}{2} & 1 \end{array}\rightB .\]
Let 
\[ B=\left\{ 
\leftB\begin{array}{cc} 1 & 0 \\ 0 & 0 \end{array}\rightB,
\leftB\begin{array}{rr} 0  & -\frac{1}{2} \\
-\frac{1}{2} & 1 \end{array}\rightB\right\}.\]
Then $\func{span}(B)=U$, and it is routine to verify that $B$ is
an independent subset of $\mathbb{M}_{22}$.  
Therefore $B$ is a basis of $U$, and $\dim(U)=2$.
\end{solution}

The following theorem claims that a spanning set of a vector space $V$ can be shrunk down to a basis of $V$. Similarly, a linearly independent set within $V$ can be enlarged to create a basis of $V$.

\begin{theorem}{Basis of $V$}{basisfromspanninglinind}
 If $V=\func{span}\left\{ \vect{u}_{1},\cdots ,\vect{u}
_{n}\right\} $ is a vector space, then some subset of $\{\vect{u}_{1},\cdots ,\vect{u}_{n}\}$
is a basis for $V.$ Also, if $\{\vect{u}_{1},\cdots ,\vect{u}
_{k}\}\subseteq V$ is linearly independent and the vector space is finite
dimensional,
\index{linear independence!enlarging to form a basis} then the set $\{
\vect{u}_{1},\cdots ,\vect{u}_{k}\},$ can be enlarged to obtain a basis
of $V.$
\end{theorem}

\begin{proof}Let 
\begin{equation*}
S=\{E\subseteq \{\vect{u}_{1},\cdots ,\vect{u}_{n}\}\text{ such that }%
\func{span}\left\{ E\right\} =V\}.
\end{equation*}
For $E\in S,$ let $\left\vert E\right\vert $ denote the number of elements
of $E.$ Let 
\begin{equation*}
m= \min \{\left\vert E\right\vert \text{ such that }E\in S\}.
\end{equation*}
Thus there exist vectors 
\begin{equation*}
\{\vect{v}_{1},\cdots ,\vect{v}_{m}\}\subseteq \{\vect{u}_{1},\cdots ,%
\vect{u}_{n}\}
\end{equation*}
such that 
\begin{equation*}
\func{span}\left\{ \vect{v}_{1},\cdots ,\vect{v}_{m}\right\} =V
\end{equation*}
and $m$ is as small as possible for this to happen. If this set is linearly
independent, it follows it is a basis for $V$ and the theorem is proved. On
the other hand, if the set is not linearly independent, then there exist
scalars, $c_{1},\cdots ,c_{m}$ such that 
\begin{equation*}
\vect{0}=\sum_{i=1}^{m}c_{i}\vect{v}_{i}
\end{equation*}
and not all the $c_{i}$ are equal to zero. Suppose $c_{k}\neq 0.$ Then solve for the
vector $\vect{v}_{k}$ in terms of the other vectors.
Consequently, 
\begin{equation*}
V=\func{span}\left\{ \vect{v}_{1},\cdots ,\vect{v}_{k-1},\vect{v}
_{k+1},\cdots ,\vect{v}_{m}\right\}
\end{equation*}
contradicting the definition of $m$. This proves the first part of the
theorem.

To obtain the second part, begin with $\{\vect{u}_{1},\cdots ,\vect{u}
_{k}\}$ and suppose a basis for $V$ is 
\begin{equation*}
\left\{ \vect{v}_{1},\cdots ,\vect{v}_{n}\right\} 
\end{equation*}
If 
\begin{equation*}
\func{span}\left\{ \vect{u}_{1},\cdots ,\vect{u}_{k}\right\} =V,
\end{equation*}
then $k=n$. If not, there exists a vector 
\begin{equation*}
\vect{u}_{k+1}\notin \func{span}\left\{ \vect{u}_{1},\cdots ,\vect{u}
_{k}\right\}
\end{equation*}
Then from Lemma \ref{lem:addinglinearlyindependent}, $\{\vect{u}_{1},\cdots ,\vect{u}_{k},
\vect{u}_{k+1}\}$ is also linearly independent. Continue adding vectors in
this way until $n$ linearly independent vectors have been obtained. Then 
\begin{equation*}
\func{span}\left\{ \vect{u}_{1},\cdots ,\vect{u}_{n}\right\} =V
\end{equation*}
because if it did not do so, there would exist $\vect{u}_{n+1}$ as just
described and $\left\{ \vect{u}_{1},\cdots ,\vect{u}_{n+1}\right\} $
would be a linearly independent set of vectors having $n+1$ elements. This contradicts the fact that $\left\{ \vect{v}_{1},\cdots ,\vect{v}_{n}\right\} $ is a basis.
 In turn this would contradict Theorem \ref{thm:exchangetheorem}. Therefore, this list is a
basis. 
\end{proof}

Recall Example \ref{exa:addinglinind} in which we added a matrix to a linearly independent set to create a larger linearly independent set. By Theorem \ref{thm:basisfromspanninglinind} we can extend a linearly independent set to a basis.  

\begin{example}{Adding to a Linearly Independent Set}{addinglinearindbasis}
Let $S \subseteq M_{22}$ be a linearly independent set given by 
\[
S  = \left\{ \leftB \begin{array}{rr}
1 & 0 \\
0 & 0 
\end{array} \rightB, \leftB \begin{array}{rr}
0 & 1 \\
0 & 0 
\end{array} \rightB \right\}
\]
Enlarge $S$ to a basis of $M_{22}$. 
\end{example}

\begin{solution}
Recall from the solution of Example \ref{exa:addinglinind} that the set  $R \subseteq M_{22}$ given by 
\[
R = \left\{ \leftB \begin{array}{rr}
1 & 0 \\
0 & 0 
\end{array} \rightB, \leftB \begin{array}{rr}
0 & 1 \\
0 & 0 
\end{array} \rightB, \leftB \begin{array}{rr}
0 & 0 \\
1 & 0 
\end{array} \rightB \right\}
\]
is also linearly independent.
However this set is still not a basis for $M_{22}$ as it is not a spanning set. In particular, $\leftB \begin{array}{rr}
0 & 0 \\
0 & 1 
\end{array} \rightB$ is not in $\func{span} R$. Therefore, this matrix can be added to the set by Lemma \ref{lem:addinglinearlyindependent} to obtain a new linearly independent set given by 
\[
T = \left\{ \leftB \begin{array}{rr}
1 & 0 \\
0 & 0 
\end{array} \rightB, \leftB \begin{array}{rr}
0 & 1 \\
0 & 0 
\end{array} \rightB, \leftB \begin{array}{rr}
0 & 0 \\
1 & 0 
\end{array} \rightB, \leftB \begin{array}{rr}
0 & 0 \\
0 & 1 
\end{array} \rightB \right\}
\]

This set is linearly independent and now spans $M_{22}$. Hence $T$ is a basis. 
\end{solution}

Next we consider the case where you have a
spanning set and you want a subset which is a basis. The above discussion involved adding vectors to a set. The next theorem involves removing vectors. 

\begin{theorem}{Basis from a Spanning Set}{}
Let $V$ be a vector space and let $W$ be a subspace. Also
suppose that $W=\func{span}\left\{ \vect{w}_{1},\cdots ,\vect{w}
_{m}\right\} $. Then there exists a subset of $\left\{ \vect{w}_{1},\cdots ,
\vect{w}_{m}\right\} $ which is a basis for $W$.
\end{theorem}

\begin{proof}
Let $S$ denote the set of positive integers such that for $
k\in S,$ there exists a subset of $\left\{ \vect{w}_{1},\cdots ,\vect{w}
_{m}\right\} $ consisting of exactly $k$ vectors which is a spanning set for 
$W$. Thus $m\in S$. Pick the smallest positive integer in $S$. Call it $k$.
Then there exists $\left\{ \vect{u}_{1},\cdots ,\vect{u}_{k}\right\} \subseteq
\left\{ \vect{w}_{1},\cdots ,\vect{w}_{m}\right\} $ such that $\limfunc{span}
\left\{ \vect{u}_{1},\cdots ,\vect{u}_{k}\right\} =W.$ If 
\begin{equation*}
\sum_{i=1}^{k}c_{i}\vect{w}_{i}=\vect{0}
\end{equation*}
and not all of the $c_{i}=0,$ then you could pick $c_{j}\neq 0$, divide by
it and solve for $\vect{u}_{j}$ in terms of the others. 
\begin{equation*}
\vect{w}_{j}=\sum_{i\neq j}\left( -\frac{c_{i}}{c_{j}}\right) \vect{w}_{i}
\end{equation*}
Then you could delete $\vect{w}_{j}$ from the list and have the same span.
In any linear combination involving $\vect{w}_{j}$, the linear
combination would equal one in which $\vect{w}_{j}$ is replaced with the
above sum, showing that it could have been obtained as a linear combination
of $\vect{w}_{i}$ for $i\neq j$. Thus $k-1\in S$ contrary to the choice of $k$
. Hence each $c_{i}=0$ and so $\left\{ \vect{u}_{1},\cdots ,\vect{u}
_{k}\right\} $ is a basis for $W$ consisting of vectors of $\left\{ \vect{w}
_{1},\cdots ,\vect{w}_{m}\right\} $. 
\end{proof}

Consider the following example of this concept. 

\begin{example}{Basis from a Spanning Set}{}
Let $V$ be the vector space of polynomials of degree no more than 3,
denoted earlier as $\mathbb{P}_{3}$. Consider the following vectors in $V$.
\begin{eqnarray*}
&&2x^{2}+x+1,x^{3}+4x^{2}+2x+2,2x^{3}+2x^{2}+2x+1, \\
&&x^{3}+4x^{2}-3x+2,x^{3}+3x^{2}+2x+1
\end{eqnarray*}
Then, as mentioned above, $V$ has dimension 4 and so clearly these vectors
are not linearly independent. A basis for $V$ is $\left\{
1,x,x^{2},x^{3}\right\} $. Determine a linearly independent subset of these
which has the same span. Determine whether this subset is a basis for $V$.
\end{example}

\begin{solution}
Consider an isomorphism which maps $\mathbb{R}%
^{4}$ to $V$ in the obvious way. Thus 
\begin{equation*}
\leftB 
\begin{array}{c}
1 \\ 
1 \\ 
2 \\ 
0
\end{array}
\rightB
\end{equation*}
corresponds to $2x^{2}+x+1$ through the use of this isomorphism. Then
corresponding to the above vectors in $V$ we would have the following
vectors in $\mathbb{R}^{4}.$ 
\begin{equation*}
\leftB 
\begin{array}{c}
1 \\ 
1 \\ 
2 \\ 
0
\end{array}
\rightB ,\leftB 
\begin{array}{c}
2 \\ 
2 \\ 
4 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{c}
1 \\ 
2 \\ 
2 \\ 
2
\end{array}
\rightB ,\leftB 
\begin{array}{r}
2 \\ 
-3 \\ 
4 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{c}
1 \\ 
2 \\ 
3 \\ 
1
\end{array}
\rightB
\end{equation*}
Now if we obtain a subset of these which has the same span but which is
linearly independent, then the corresponding vectors from $V$ will also be
linearly independent. If there are four
in the list, then the resulting vectors from $V$ must be a basis for $V$.
The \rref\; for the matrix which has the above vectors as
columns is 
\begin{equation*}
\leftB 
\begin{array}{rrrrr}
1 & 0 & 0 & -15 & 0 \\ 
0 & 1 & 0 & 11 & 0 \\ 
0 & 0 & 1 & -5 & 0 \\ 
0 & 0 & 0 & 0 & 1
\end{array}
\rightB
\end{equation*}
Therefore, a basis for $V$ consists of the vectors
\begin{eqnarray*}
&&2x^{2}+x+1,x^{3}+4x^{2}+2x+2,2x^{3}+2x^{2}+2x+1, \\
&&x^{3}+3x^{2}+2x+1.
\end{eqnarray*}
Note how this is a subset of the original set of vectors. If there had been
only three pivot columns in this matrix, then we would not have had a basis
for $V$ but we would at least have obtained a linearly independent subset of
the original set of vectors in this way. 

Note also that, since all linear relations are preserved by an isomorphism,
\begin{eqnarray*}
&&-15\left( 2x^{2}+x+1\right) +11\left( x^{3}+4x^{2}+2x+2\right) +\left(
-5\right) \left( 2x^{3}+2x^{2}+2x+1\right) \\
&=&x^{3}+4x^{2}-3x+2
\end{eqnarray*}

\end{solution}

Consider the following example.

\begin{example}{Shrinking a Spanning Set}{shrinkspanning}
Consider the set $S \subseteq \mathbb{P}_2$ given by 
\[
S = \left\{ 1, x, x^2, x^2 + 1 \right\}
\]
Show that $S$ spans $\mathbb{P}_2$, then remove vectors from $S$ until it creates a basis. 
\end{example}

\begin{solution}
First we need to show that $S$ spans $\mathbb{P}_2$. Let $ax^2 + bx + c$ be an arbitrary polynomial in $\mathbb{P}_2$. Write 
\[
ax^2 + bx + c = r(1) + s(x) + t(x^2) + u (x^2 + 1)
\]
Then,
\begin{eqnarray*}
ax^2 +bx + c &=& r(1) + s(x) + t(x^2) + u (x^2 + 1) \\
&=& (t+u) x^2 + s(x) + (r+u) 
\end{eqnarray*}

It follows that 
\begin{eqnarray*}
a &=& t + u \\
b &=& s \\
c &=& r + u 
\end{eqnarray*}

Clearly a solution exists for all $a,b,c$ and so $S$ is a spanning set for $\mathbb{P}_2$. By Theorem \ref{thm:basisfromspanninglinind}, some subset of $S$ is a basis for $\mathbb{P}_2$. 

Recall that a basis must be both a spanning set and a linearly independent set.
Therefore we must remove a vector from $S$ keeping this in mind. Suppose we remove $x$ from $S$. The resulting set would be $\left\{ 1, x^2, x^2 + 1 \right\}$. This set is clearly linearly dependent (and also does not span $\mathbb{P}_2$) and so is not a basis. 

Suppose we remove $x^2 + 1$ from $S$. The resulting set is $\left\{ 1, x, x^2 \right\}$ which is both linearly independent and spans $\mathbb{P}_2$. Hence this is a basis for $\mathbb{P}_2$. Note that removing any one of $1, x^2$, or $x^2 + 1$ will result in a basis.
\end{solution}

Now the following is a fundamental result about subspaces.

\begin{theorem}{Basis of a Vector Space}{basisvectorspace}
Let $V$ be a finite dimensional vector space and let $W$ be
a non-zero subspace. Then $W$ has a basis. That is, there exists a linearly
independent set of vectors $\left\{ \vect{w}_{1},\cdots ,\vect{w}_{r}\right\} $
such that 
\begin{equation*}
\limfunc{span}\left\{ \vect{w}_{1},\cdots ,\vect{w}_{r}\right\} =W
\end{equation*}
Also if $\left\{ \vect{w}_{1},\cdots ,\vect{w}_{s}\right\} $ is a linearly
independent set of vectors, then $W$ has a basis of the form $\left\{ \vect{w}
_{1},\cdots ,\vect{w}_{s},\cdots ,\vect{w}_{r}\right\} $ for $r\geq s$.
\end{theorem}

\begin{proof}
Let the dimension of $V$ be $n$. Pick $\vect{w}_{1}\in W$
where $\vect{w}_{1}\neq \vect{0}.$ If $\vect{w}_{1},\cdots ,\vect{w}_{s}$ have
been chosen such that $\left\{ \vect{w}_{1},\cdots ,\vect{w}_{s}\right\} $ is
linearly independent, if $\limfunc{span}\left\{ \vect{w}_{1},\cdots ,\vect{w}
_{r}\right\} =W,$ stop. You have the desired basis. Otherwise, there exists $
\vect{w}_{s+1}\notin \limfunc{span}\left\{ \vect{w}_{1},\cdots ,\vect{w}
_{s}\right\} $ and $\left\{ \vect{w}_{1},\cdots ,
\vect{w}_{s},\vect{w}_{s+1}\right\} $ is linearly independent. Continue this
way until the process stops. It must stop since otherwise, you could obtain a
linearly independent set of vectors having more than $n$ vectors which is
impossible.

The last claim is proved by following the above procedure starting with $
\left\{ \vect{w}_{1},\cdots ,\vect{w}_{s}\right\} $ as above. 
\end{proof}

This also proves the following corollary. Let $V$ play the role of $
W$ in the above theorem and begin with a basis for $W$, enlarging it to form
a basis for $V$ as discussed above.

\begin{corollary}{Basis Extension}{}
Let $W$ be any non-zero subspace of a vector space $V$.
Then every basis of $W$ can be extended to a basis for $V$.
\end{corollary}

Consider the following example.

\begin{example}{Basis Extension}{}
Let $V=\mathbb{R}^{4}$ and let 
\begin{equation*}
W=\func{span}\left\{ \leftB 
\begin{array}{c}
1 \\ 
0 \\ 
1 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{c}
0 \\ 
1 \\ 
0 \\ 
1
\end{array}
\rightB \right\}
\end{equation*}
Extend this basis of $W$ to a basis of $V$.
\end{example}

\begin{solution}
An easy way to do this is to take the \rref\; of the matrix 
\begin{equation}
\leftB 
\begin{array}{cccccc}
1 & 0 & 1 & 0 & 0 & 0 \\ 
0 & 1 & 0 & 1 & 0 & 0 \\ 
1 & 0 & 0 & 0 & 1 & 0 \\ 
1 & 1 & 0 & 0 & 0 & 1
\end{array}
\rightB  \label{vectorspaceeq1}
\end{equation}
Note how the given vectors were placed as the first two and then the matrix
was extended in such a way that it is clear that the span of the columns of
this matrix yield all of $\mathbb{R}^{4}$. Now determine the pivot columns.
The \rref\; is 
\begin{equation}
\leftB 
\begin{array}{rrrrrr}
1 & 0 & 0 & 0 & 1 & 0 \\ 
0 & 1 & 0 & 0 & -1 & 1 \\ 
0 & 0 & 1 & 0 & -1 & 0 \\ 
0 & 0 & 0 & 1 & 1 & -1
\end{array}
\rightB  \label{vectorspaceeq2}
\end{equation}
These are 
\begin{equation*}
\leftB 
\begin{array}{c}
1 \\ 
0 \\ 
1 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{c}
0 \\ 
1 \\ 
0 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{c}
1 \\ 
0 \\ 
0 \\ 
0
\end{array}
\rightB ,\leftB 
\begin{array}{c}
0 \\ 
1 \\ 
0 \\ 
0
\end{array}
\rightB
\end{equation*}
and now this is an extension of the given basis for $W$ to a basis for $
\mathbb{R}^{4}$.

Why does this work? The columns of \ref{vectorspaceeq1} obviously span $\mathbb{R}
^{4}$  the span of the first four is the same as the span of all
six.
\end{solution}
