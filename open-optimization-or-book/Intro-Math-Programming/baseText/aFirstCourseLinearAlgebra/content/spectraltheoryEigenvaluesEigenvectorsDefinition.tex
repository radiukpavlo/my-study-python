\subsection{Definition of Eigenvectors and Eigenvalues}

In this section, we will work with the entire set of complex numbers, 
denoted by $\mathbb{C}$. Recall that the real numbers, $\mathbb{R}$ are 
contained in the complex numbers, so the discussions in this section 
apply to both real and complex numbers. 

To illustrate the idea behind what will be discussed, consider the following
example.

\begin{example}{Eigenvectors and Eigenvalues}{eigenvectorsandeigenvalues}
Let
\begin{equation*}
A = \leftB
\begin{array}{rrr}
0 & 5 & -10 \\
0 & 22 & 16 \\
0 & -9 & -2
\end{array}
\rightB 
\end{equation*}
Compute the product $AX$ for 
\begin{equation*}
X = \leftB
\begin{array}{r}
5 \\
-4 \\
 3
\end{array}
\rightB, X = \leftB
\begin{array}{r}
1 \\
0 \\
0
\end{array}
\rightB
\end{equation*}
What do you notice about $AX$ in each of these products? 
\end{example}

\begin{solution}
First, compute $AX$ for 
\begin{equation*}
X =\leftB
\begin{array}{r}
5 \\
-4 \\
3
\end{array}
\rightB
\end{equation*}

This product is given by 
\begin{equation*}
AX = \leftB
\begin{array}{rrr}
0 & 5 & -10 \\
0 & 22 & 16 \\
0 & -9 & -2
\end{array}
\rightB \leftB
\begin{array}{r}
-5 \\
-4 \\
3
\end{array}
\rightB = \leftB
\begin{array}{r}
-50 \\
-40 \\
30
\end{array}
\rightB =10\leftB
\begin{array}{r}
-5 \\
-4 \\
3
\end{array}
\rightB 
\end{equation*}

In this case, the product $AX$ resulted in a vector which is equal to $10$ times the vector $X$. 
In other words, $AX=10X$.

Let's see what happens in the next product.
Compute $AX$ for the vector 
\begin{equation*}
X =  \leftB
\begin{array}{r}
1 \\
0 \\
0
\end{array}
\rightB 
\end{equation*}

This product is given by 
\begin{equation*}
AX = \leftB
\begin{array}{rrr}
0 & 5 & -10 \\
0 & 22 & 16 \\
0 & -9 & -2
\end{array}
\rightB \leftB
\begin{array}{r}
1 \\
0 \\
0
\end{array}
\rightB = \leftB
\begin{array}{r}
0 \\
0 \\
0
\end{array}
\rightB =0\leftB
\begin{array}{r}
1 \\
0 \\
0
\end{array}
\rightB 
\end{equation*}

In this case, the product $AX$ resulted in a vector equal to $0$ times the vector $X$, $AX=0X$. 

Perhaps this matrix is such that $AX$ results in $kX$, for every vector $X$. However, consider
\begin{equation*}
\leftB
\begin{array}{rrr}
0 & 5 & -10 \\
0 & 22 & 16 \\
0 & -9 & -2
\end{array}
\rightB \leftB
\begin{array}{r}
1 \\
1 \\
1
\end{array}
\rightB = \leftB
\begin{array}{r}
-5 \\
38 \\
-11
\end{array}
\rightB 
\end{equation*}
In this case, $AX$ did not result in a vector of the form $kX$ for some scalar $k$. 
\end{solution}

There is something special about the first two products calculated in
Example \ref{exa:eigenvectorsandeigenvalues}.  Notice that for each,
$AX=kX$ where $k$ is some scalar.  When this equation holds for some
$X$ and $k$, we call the scalar $k$ an \textbf{eigenvalue} of $A$. We
often use the special symbol $\lambda$ instead of $k$ when referring
to eigenvalues. In Example \ref{exa:eigenvectorsandeigenvalues}, the
values $10$ and $0$ are eigenvalues for the matrix $A$ and we can
label these as $\lambda_1 = 10$ and $\lambda_2 = 0$.

When $AX = \lambda X$ for some $X \neq 0$, we call such an $X$ an
\textbf{eigenvector} of the matrix $A$. The eigenvectors of $A$ are
associated to an eigenvalue.  Hence, if $\lambda_1$ is an eigenvalue
of $A$ and $AX = \lambda_1 X$, we can label this eigenvector as
$X_1$. Note again that in order to be an eigenvector, $X$ must be
nonzero.

There is also a geometric significance to eigenvectors.  When you have a \textbf{nonzero} vector which, when multiplied by a matrix results in another vector
which is parallel to the first or equal to \textbf{0}, this vector is called
an eigenvector of the matrix. This is the meaning when the vectors are in 
$\mathbb{R}^{n}.$ 

The formal definition of eigenvalues and eigenvectors is as follows.

\begin{definition}{Eigenvalues and Eigenvectors}{eigenvaluesandeigenvectors}
Let $A$ be an $n\times n$ matrix and let $X \in \mathbb{C}^{n}$ be a
\textbf{nonzero vector} for which

\begin{equation}
AX=\lambda X  \label{eigen1}
\end{equation}
for some scalar $\lambda .$ Then $\lambda $ is called an
\index{eigenvalue}
\index{eigenvector}\textbf{eigenvalue} of the matrix $A$ and $X$ is called an \textbf{eigenvector} of $A$ associated with $\lambda$, or a $\lambda$-eigenvector of $A$. 
 
The set of all eigenvalues of an $n\times n$ matrix $A$ is denoted by 
$\sigma \left( A\right) $ and is referred to as the \textbf{spectrum}
\index{spectrum} of $A.$
\end{definition}

The eigenvectors of a matrix $A$ are those vectors $X$ for which
multiplication by $A$ results in a vector in the same direction or opposite
direction to $X$. Since the zero vector $0$ has no
direction this would make no sense for the zero vector. As noted above, 
$0$ is never allowed to be an eigenvector. 

Let's look at eigenvectors in more detail. Suppose $X$ satisfies \ref{eigen1}. Then
\begin{equation*}
\begin{array}{c}
AX - \lambda X = 0 \\
\mbox{or} \\
\left( A-\lambda I\right) X = 0
\end{array}
\end{equation*}
for some $X \neq 0.$ Equivalently you could write $\left( \lambda
I-A\right)X = 0$, which is more commonly used.  Hence, when we are looking for eigenvectors, we are
looking for nontrivial solutions to this homogeneous system of equations!

Recall that the solutions to a homogeneous system of equations consist
of basic solutions, and the linear combinations of those basic
solutions. In this context, we call the basic solutions of the
equation $\left( \lambda I - A\right) X = 0$ \textbf{basic
eigenvectors}. It follows that any (nonzero) linear combination of basic
eigenvectors is again an eigenvector.

Suppose the matrix $\left(\lambda I - A\right)$ is invertible, so that
$\left(\lambda I - A\right)^{-1}$ exists.
Then the following equation would be true.
\begin{eqnarray*}
X &=& IX \\
&=& \left( \left( \lambda I - A\right) ^{-1}\left(\lambda I - A \right)
\right) X \\
&=&\left( \lambda I - A\right) ^{-1}\left( \left( \lambda
I - A\right) X\right) \\
&=& \left( \lambda I - A\right) ^{-1}0 \\
&=& 0
\end{eqnarray*}
This claims that $X=0$. However, we have required that $ X \neq 0$. Therefore  $\left(\lambda I - A\right)$
cannot have an inverse! 

Recall that if a matrix
is not invertible, then its determinant is equal to $0$.  Therefore we
can conclude that
\begin{equation}
\det \left( \lambda I - A\right) =0  \label{eigen2}
\end{equation}
Note that this is equivalent to $\det \left(A- \lambda I \right) =0$. 

The expression $\det \left( \eigenVar I-A\right) $ is a polynomial (in
the variable $x$) called the
\textbf{characteristic polynomial of $A$}, and 
$\det \left( \eigenVar I-A\right) =0$ is called the \textbf{characteristic
equation}. \index{characteristic equation} For this reason we may also refer to
the eigenvalues of $A$ as \textbf{characteristic values}, but the
former is often used for historical reasons.

The following theorem claims that the roots of the characteristic
polynomial are the eigenvalues of $A$.  Thus when \ref{eigen2}
holds, $A$ has a nonzero eigenvector.

\begin{theorem}{The Existence of an Eigenvector}{existenceofeigenvector}
Let $A$ be an $n\times n$ matrix and suppose $\det \left( \lambda I -
A\right) =0$ for some $ \lambda \in \mathbb{C}$.  \\
Then $\lambda$ is an eigenvalue of $A$ and thus there exists a nonzero
vector $X \in \mathbb{C}^{n}$ such that $AX=\lambda X$. 
\end{theorem}

\ifdefined\showproofs
\begin{proof}
For $A$ an $n\times n$ matrix, the method of Laplace
Expansion demonstrates that $\det \left( \lambda I - A \right) $ is a
polynomial of degree $n.$ As such, the equation
\ref{eigen2} has a solution $\lambda \in \mathbb{C}$ by the Fundamental
Theorem of Algebra. The fact that $\lambda$ is an eigenvalue is left as an exercise. 
\end{proof}
\fi