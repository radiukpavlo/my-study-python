### 3.4. Experimental Framework and Evaluation Setup

Для оцінювання ефективності запропонованого підходу та його порівняння з класичною методикою побудови матриць переходу [4], було проведено низку обчислювальних експериментів на синтетичних даних та еталонному наборі даних MNIST [].

#### 3.4.1. Опис набору даних

***Протокол синтетичних даних.*** Синтетичний тест був розроблений для перевірки математичного апарату в контрольованих умовах, де геометрію хмар точок можна візуалізувати безпосередньо. Ми використовуємо $m = 15$ зразків з $k = 5$ формальними ознаками та $l = 4$ ментальними ознаками. Матриці $A$ та $B$ надаються в матеріалах проекту. Щоб визначити змістовну дію симетрії без явних необроблених вхідних даних, ми вкладаємо кожну матрицю в $\mathbb{R}^2$ за допомогою MDS і інтерпретуємо плоске вкладення як «простір об’єктів», на якому $SO(2)$ діє обертанням. Лінійний декодер з 2D-вкладення назад у початковий простір ознак дає обернені версії $A_{rot}$ та $B_{rot}$, з яких генератори $J^A$ та $J^B$ оцінюються методом скінченних різниць.

Ми оцінюємо (i) точність за середньоквадратичною похибкою $\text{MSE}_{\text{fid}} = (ml)^{-1} \| B - A T^\top \|_F^2$ та (ii) дефект симетрії за $\text{SymErr} = \| T J^A - J^B T \|_F^2$. Стійкість оцінюється шляхом застосування діапазону кутів повороту та вимірювання помилки передбачених ознак ментальної моделі відносно оберненої цілі ментальної моделі.

***Протокол даних MNIST.*** MNIST складається з 70 000 напівтонових зображень рукописних цифр розміром $28 \times 28$. Використовується стандартне розбиття на 60 000 тренувальних та 10 000 тестових зображень. Формальна модель – це згорткова нейронна мережа, навчена для класифікації цифр; глибокі ознаки $a(x)$ витягуються з передостаннього повнозв’язного шару, даючи $k = 490$ латентних вимірів. Ментальна модель – це представлення у піксельному просторі $b(x)$, отримане шляхом розгортання зображення у 784-вимірний вектор ($l = 784$). У цьому налаштуванні матриця переходу стає лінійним декодером з глибоких ознак у пікселі, що уможливлює пояснення на основі реконструкції.

Для оцінювання генераторів обертання ми застосовуємо малі обертання до зображень і обчислюємо центральні скінченні різниці глибоких ознак та векторів пікселів. Генератори $J^A$ та $J^B$ отримуються розв’язанням лінійних регресій. ЕМП обчислюється з $\lambda = 0.5$, що емпірично дає сильне зменшення дефекту симетрії при збереженні якості реконструкції.

Ми звітуємо про точність реконструкції за допомогою структурної подібності (SSIM) та пікового співвідношення сигналу до шуму (PSNR) між оригінальними зображеннями та реконструйованими зображеннями, отриманими через матрицю переходу. Ми також наводимо помилку симетрії Фробеніуса $\| T J^A - J^B T \|_F$ та стійкість до обертань, оцінюючи SSIM та PSNR на обернених тестових зображеннях.

#### 3.4.2. Методика експерименту

Експериментальне дослідження складається з трьох ключових етапів:

1.  *Базовий рівень (Baseline):* Розраховується статична матриця переходу $T_{old}$ згідно з методом, наведеним у [4], де оптимізується лише функціонал точності відтворення (Fidelity).
2.  *Групова дія та генератори:* Для реалізації еквіваріантного підходу вводиться дія групи поворотів $SO(2)$. Навчальна вибірка доповнюється копіями зображень, зміненими на малий кут $\epsilon = 0.01$ рад. На основі реакції шарів CNN та піксельного представлення обчислюються матриці інфінітезимальних генераторів $J^A$ та $J^B$.
3.  *Обчислення розширеної матриці:* За допомогою **Алгоритму 1**, будується матриця $T_{new}$, що інтегрує умови апроксимації та симетрії. Розв'язок системи проводиться через SVD-розклад з параметром регуляризації $\lambda$.

#### 3.4.3. Чисельний приклад та порівняльний аналіз на синтетичних даних

Для демонстрації переваг запропонованого методу проведемо обчислювальний експеримент на синтетичному наборі даних, де зв'язок між формальною (FM) та ментальною (MM) моделями заданий явно.

***Постановка експерименту.*** Розглянемо набір з $m = 15$ зразків, розділених на три класи (аналогічно до наведеного в [4], Appendix 1.1). Нехай:

*   Матриця $A \in \mathbb{R}^{15 \times 5}$ представляє глибокі ознаки (рис.1 (a)).
*   Матриця $B \in \mathbb{R}^{15 \times 4}$ представляє інтерпретовані параметри (рис.1 (b)).
*   Група симетрії – $SO(2)$ (повороти), де генератори $J^A$ та $J^B$ відповідають обертанню у відповідних підпросторах ознак.

*(a) матриця $A \in \mathbb{R}^{15 \times 5}$, (b) матриця $B \in \mathbb{R}^{15 \times 4}$*
**Рис. 1. Візуалізація за допомогою MDS: (a) Formal Model, (b) Mental (Human) Model**

Далі отримаємо генератори $J$. Математична логіка полягає у наступному.
Генератор $J$ описує швидкість зміни ознак. Якщо ми трохи трансформуємо вхід (наприклад, повернемо вхідне зображення на кут $\epsilon$), то нові ознаки $A_{\epsilon}$ будуть пов'язані зі старими $A$ через генератор: $A_{\epsilon} \approx A(I + \epsilon J^\top)$. Звідси отримуємо рівняння для $J$: $A J^\top \approx (A_{\epsilon} - A)/\epsilon$. Позначимо $\Delta A = (A_{\epsilon} - A)/\epsilon$ (це чисельна похідна ознак за параметром трансформації). Тоді задача зводиться до: $A J^\top = \Delta A$.

Моделюємо $\Delta A$, припустивши, що "поворот" у просторі ознак – це лінійна комбінація існуючих ознак.

Виходячи з того, що у цьому прикладі ми маємо тільки синтетично створені матриці $A$ та $B$ і не маємо вхідних даних та функцій (моделей глибокого навчання та моделей машинного навчання), запропоновано підхід, який фактично дозволить «заземлити» абстрактні синтетичні дані, надавши їм геометричного змісту через візуалізацію. Оскільки ваші матриці $A$ та $B$ не мають зв'язку з вхідними образами, використаємо 2D-простір як «контрольний місток».

Для цього нам потрібно зворотне відображення (Inverse Mapping). Оскільки стандартні методи пониження розмірності (t-SNE або MDS) не мають прямої формули повернення, ми побудуємо її самостійно за допомогою регресії.

**Алгоритм 2. Отримання генератора $J$ через 2D-поворот:**

1.  **Редукція:** Переводимо $A \in \mathbb{R}^{15 \times 5}$ у $A_{2D} \in \mathbb{R}^{15 \times 2}$ за допомогою методу MDS (тому, що цей метод найкраще для цього підходить, бо зберігає глобальні відстані).
2.  **Побудова містка (Inverse Map):** Навчаємо лінійну регресію (декодер), яка відображає $(x, y) \rightarrow (a_1, \dots, a_5)$.
3.  **Поворот:** Повертаємо точки в 2D на малий кут $\epsilon$ та отримуємо $A_{2D,rot}$.
4.  **Повернення:** Пропускаємо $A_{2D,rot}$ через декодер, щоб отримати $A_{rot}$ у 5-вимірному просторі.
5.  **Генератор:** За існуючими $A$ та $A_{rot}$ обчислюємо $J_A$ через $\Delta A$.

У **App. 1.2.** наведено код на Pyton для отримання генератора $J$.
Аналогічно отримуємо $J_B$ для матриці $B$. Оскільки обидві матриці ($A$ і $B$) походять від одних і тих же об'єктів, їхні 2D-проекції будуть схожими, і ми знайдемо зв'язок між ними через **Алгоритм 2**.

У результаті ми отримаємо матриці $J^A (5 \times 5)$ та $J^B (4 \times 4)$ (**App. 1.2.**), які ідеально підходять для рівняння переплітання $T J^A - J^B T$. У підсумку, ми зможемо стверджувати, що матриця $T$ "поважає" поворот, який людина бачить на екрані візуалізації.

Для наведеного чисельного прикладу оберемо $r = 1$ (один генератор), що відповідає групі $SO(2)$ (повороти на площині). Такий вибір випливає із наступних міркувань: (1) Оскільки ми робимо редукцію у 2-вимірний простір (MDS), там існує лише один фундаментальний вид неперервного повороту – навколо центру координат. Цій трансформації відповідає рівно один генератор в алгебрі Лі. (2) Це зробить блочні матриці $M$ та $Y$ компактнішими і легшими для розуміння.

Також, для цього прикладу, оберемо дуже малий кут $\epsilon = 0.01$ радіан (або навіть 0.001). Вибір таких малих значень обгрунтовується тим, що генератор визначає дію групи в околі одиниці (тобто при нульовому повороті). Якщо взяти великий кут, лінійна апроксимація $A_{rot} \approx A(I + \epsilon J^\top)$ стане неточною, і ми отримаємо "брудний" генератор із великою похибкою. Також зазначимо, що не потрібно брати різні кути для різних рядків. Групова дія є глобальною операцією. Ми повертаємо всю хмару точок у 2D на один і той самий кут $\epsilon$. Це дає нам змогу знайти одну матрицю $J$, яка описує цей поворот для всього простору ознак одночасно.

***Сценарій 1: Старий підхід [4] (Статична матриця переходу)***
Використовуючи метод, описаний у [4], було обчислено матрицю $T_{old}$ (**App. 1.1**). За отриманою матрицею реконструйовано параметри ментальної моделі: $B_{old}^* = A T_{old}^\top$ (**App. 1.4**). Результат був оцінений за метриками:

*   похибка апроксимації ($MSE\_fid$): $\frac{1}{m \cdot l} \| B - B_{old}^* \|_F^2$
*   дефект симетрії ($Sym\_err$): $\| T_{old} J^A - J^B T_{old} \|_F^2$

Значення $MSE\_fid$ є мінімально можливим (оскільки $T$ оптимізувалася саме під цю задачу). Значення $Sym\_err$ є великим. Це математично доводить, що старий підхід ігнорує «геометрію» даних, розглядаючи ознаки просто як набір чисел, а не як еквіваріантні представлення.
У Сценарії 2 ми покажемо, що трохи пожертвувавши $MSE\_fid$, ми зможемо радикально зменшити $Sym\_err$, що зробить модель стійкою до трансформацій.

***Сценарій 2: Новий підхід (Еквіваріантна матриця переходу)***
Застосуємо розширений алгоритм (Розділ 3.3) з ваговим коефіцієнтом $\lambda = 0.5$. Обчислимо матрицю $T_{new}$ та відповідну їй матрицю прогнозів $B_{new}^* = A T_{new}^\top$ (**App. 1.4**).
Оцінимо отриманий результат за $MSE\_fid$ та $Sym\_err$: $MSE\_fid$, $Sym\_err$.
Як бачимо, похибка апроксимації є дещо вищою, ніж у Сценарії 1 (через введення додаткового обмеження), а $Sym\_err$ – показник дефекту симетрії зменшився, порівняно зі старим підходом. Це свідчить про те, що матриця $T_{new}$ тепер «знає» про існування групової дії та зберігає структуру перетворень при переході між просторами ознак.

***Сценарій 3: Тестування стійкості до трансформацій (Robustness Test)***
Найбільш критичним тестом для XAI-моделей є перевірка стабільності пояснень при варіаціях вхідних даних. Для цього сценарію ми згенеруємо нову «тестову» матрицю ознак $A_{rot}$, яка імітує поворот вихідних об’єктів. Оскільки в синтетичному прикладі прямий зв'язок із вхідними образами відсутній, ми реалізуємо цей крок через геометричний простір візуалізації (MDS), використовуючи 

**Алгоритм 2**:

1.  Для кожного зразка $j$ у просторі візуалізації $A_{2D}$ виконуємо поворот на випадковий кут $\alpha_i \in [-\pi/2 ; \pi/2]$ (що відповідає $\pm 90^\circ$).
2.  За допомогою навченого «зворотного містка» (декодера з Алгоритму 2) отримані 2D-точки відображаються назад у 5-вимірний простір FM-ознак, формуючи матрицю $A_{rot}$.
3.  Для отриманої матриці $A_{rot}$ обчислюються прогнози інтерпретованих ознак двома способами:

    *   Старий підхід: $B_{old\_rot}^* = A_{rot} T_{old}^\top$.
    *   Новий підхід: $B_{new\_rot}^* = A_{rot} T_{new}^\top$.

Для оцінки стійкості порівнюємо отримані вектори $b^*$ із «ідеальними» значеннями $b_{target}$, які мали б вийти, якби ментальна модель MM так само ідеально повернулася на кут $\alpha$.

#### 3.4.4. Оцінювання результатів та метрики

Для порівняння baseline-моделі та запропонованого еквіваріантного методу використовуються наступні показники:

*   *Якість реконструкції:* Оцінюється на основі метрик структурної схожості (**SSIM**) та пікового відношення сигналу до шуму (**PSNR**) між оригінальними зображеннями та цифрами, відновленими з простору FM через матрицю переходу.
*   *Дефект симетрії (Symmetry Error):* Пряме вимірювання порушення рівняння переплітання $\| T J^A - J^B T \|_F$.
*   *Стійкість до трансформацій (Robustness):* Тестування проводиться на викривленій (повернутій) тестовій вибірці. Порівнюється здатність матриць $T_{old}$ та $T_{new}$ зберігати семантичний зміст при геометричних змінах вхідних даних.

Очікується, що використання еквіваріантної матриці переходу забезпечить стабільніші результати при варіаціях вхідних даних, зберігаючи при цьому високу візуальну достовірність реконструкції, характерну для методу матриць переходу.
