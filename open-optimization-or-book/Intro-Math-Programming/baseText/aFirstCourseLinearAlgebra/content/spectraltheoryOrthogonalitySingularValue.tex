\subsection{The Singular Value Decomposition}

We begin this section with an important definition.

\begin{definition}{Singular Values}{singularvalues}
Let $A$ be an $m\times n$ matrix. The singular values of $A$ are the square roots of the positive
eigenvalues of $A^TA.$ \index{singular values}
\end{definition}

Singular Value Decomposition (SVD) can be thought of as 
a generalization of orthogonal diagonalization of a symmetric matrix
to an arbitrary $m\times n$ matrix. This decomposition is the focus of this section. 

The following is a useful result that will help when computing the SVD of matrices.

\begin{proposition}{}{samenonzeroeigenvalues}
Let $A$ be an $m \times n$ matrix. Then $A^TA$ and $AA^T$ have the same \bf{nonzero} eigenvalues.
\end{proposition}

\begin{proof}
Suppose $A$ is an $m\times n$ matrix, and suppose that  $\lambda$ is a nonzero eigenvalue of $A^TA$.
Then there exists a nonzero vector $X\in \mathbb{R}^n$ such that

\begin{equation}\label{nonzero}
(A^TA)X=\lambda X.
\end{equation}

Multiplying both sides of this equation by $A$ yields:
\begin{eqnarray*}
A(A^TA)X & = & A\lambda X\\
(AA^T)(AX) & = & \lambda (AX).
\end{eqnarray*}
Since $\lambda\neq 0$ and $X\neq 0_n$, $\lambda X\neq 0_n$,
and thus by equation~(\ref{nonzero}),
$(A^TA)X\neq 0_m$; thus $A^T(AX)\neq 0_m$, 
implying that $AX\neq 0_m$.

Therefore $AX$ is an eigenvector of $AA^T$ corresponding to eigenvalue
$\lambda$.  An analogous argument can be used to show that every
nonzero eigenvalue of $AA^T$ is an eigenvalue of $A^TA$, thus
completing the proof.
\end{proof}

Given an $m\times n$ matrix $A$, we will see how to express $A$ as a product
\[ A=U\Sigma V^T\]
where
\begin{itemize}
\item $U$ is an $m\times m$ orthogonal matrix whose columns are
eigenvectors of $AA^T$.
\item $V$ is an $n\times n$ orthogonal matrix whose columns are
eigenvectors of $A^TA$.
\item $\Sigma$ is an $m\times n$ matrix whose only nonzero values
lie on its main diagonal, and are the singular values of $A$.
\end{itemize}

How can we find such a decomposition? We are aiming to decompose $A$ in the following form:

\begin{equation*}
A=U\leftB 
\begin{array}{cc}
\sigma & 0 \\ 
0 & 0
\end{array}
\rightB V^T 
\end{equation*}

where $\sigma $ is of the form 
\[
\sigma =\leftB 
\begin{array}{ccc}
\sigma _{1} &  & 0 \\ 
& \ddots &  \\ 
0 &  & \sigma _{k}
\end{array}
\rightB
\]

Thus $A^T=V\leftB 
\begin{array}{cc}
\sigma & 0 \\ 
0 & 0
\end{array}
\rightB U^T$ and it follows that 
\begin{equation*}
A^TA=V\leftB 
\begin{array}{cc}
\sigma & 0 \\ 
0 & 0
\end{array}
\rightB U^TU\leftB 
\begin{array}{cc}
\sigma & 0 \\ 
0 & 0
\end{array}
\rightB V^T=V\leftB 
\begin{array}{cc}
\sigma ^{2} & 0 \\ 
0 & 0
\end{array}
\rightB V^T
\end{equation*}
and so $A^TAV=V\leftB 
\begin{array}{cc}
\sigma ^{2} & 0 \\ 
0 & 0
\end{array}
\rightB .$ Similarly, $AA^TU=U\leftB 
\begin{array}{cc}
\sigma ^{2} & 0 \\ 
0 & 0
\end{array}
\rightB .$ Therefore, you would find an orthonormal basis of eigenvectors
for $AA^T$ make them the columns of a matrix such that the
corresponding eigenvalues are decreasing. This gives $U.$ You could then do
the same for $A^TA$ to get $V$.

We formalize this discussion in the following theorem. 

\begin{theorem}{Singular Value Decomposition}{singvaldecomp}
Let $A$ be an $m\times n$ matrix. Then there exist
orthogonal matrices $U$ and $V$ of the appropriate size such that $A= U \Sigma V^T$ where $\Sigma$ is of the form
\[
\Sigma = 
\leftB 
\begin{array}{cc}
\sigma & 0 \\ 
0 & 0
\end{array}
\rightB
\]
and $\sigma $ is of the form 
\[
\sigma =\leftB 
\begin{array}{ccc}
\sigma _{1} &  & 0 \\ 
& \ddots &  \\ 
0 &  & \sigma _{k}
\end{array}
\rightB
\]
for the $\sigma _{i}$ the singular values of $A.$
\end{theorem}

\begin{proof}
There exists an orthonormal basis, $\left\{ \vect{v}_{i}\right\} _{i=1}^{n}$ such that $
A^TA\vect{v}_{i}=\sigma _{i}^{2}\vect{v}_{i}$ where $\sigma
_{i}^{2}>0$ for $i=1,\cdots ,k,\left( \sigma _{i}>0\right) $ and equals zero
if $i>k.$ Thus for $i>k,$ $A\vect{v}_{i}=\vect{0}$ because 
\begin{equation*}
 A\vect{v}_{i}\dotprod A\vect{v}_{i} = A^TA\vect{v}_{i} \dotprod \vect{v}_{i}  = \vect{0} \dotprod \vect{v}_{i} =0.
\end{equation*}
For $i=1,\cdots ,k,$ define $\vect{u}_{i}\in \mathbb{R}^{m}$ by 
\begin{equation*}
\vect{u}_{i}= \sigma _{i}^{-1}A\vect{v}_{i}.
\end{equation*}

Thus $A\vect{v}_{i}=\sigma _{i}\vect{u}_{i}.$ Now 
\begin{eqnarray*}
\vect{u}_{i} \dotprod \vect{u}_{j} &=&  \sigma _{i}^{-1}A
\vect{v}_{i} \dotprod \sigma _{j}^{-1}A\vect{v}_{j}  = \sigma_{i}^{-1}\vect{v}_{i} \dotprod \sigma _{j}^{-1}A^TA\vect{v}_{j} \\
&=& \sigma _{i}^{-1}\vect{v}_{i} \dotprod \sigma _{j}^{-1}\sigma _{j}^{2} \vect{v}_{j} =
\frac{\sigma _{j}}{\sigma _{i}}\left( \vect{v}_{i} \dotprod \vect{v}_{j}\right)
=\delta _{ij}.
\end{eqnarray*}
Thus $\left\{ \vect{u}_{i}\right\} _{i=1}^{k}$ is an orthonormal set of
vectors in $\mathbb{R}^{m}.$ Also, 
\begin{equation*}
AA^T\vect{u}_{i}=AA^T\sigma _{i}^{-1}A\vect{v}_{i}=\sigma
_{i}^{-1}AA^TA\vect{v}_{i}=\sigma _{i}^{-1}A\sigma _{i}^{2}\vect{v}
_{i}=\sigma _{i}^{2}\vect{u}_{i}.
\end{equation*}
Now extend $\left\{ \vect{u}_{i}\right\} _{i=1}^{k}$ to an orthonormal
basis for all of $\mathbb{R}^{m},\left\{ \vect{u}_{i}\right\} _{i=1}^{m}$
and let 
\begin{equation*}
U= \leftB 
\begin{array}{ccc}
\vect{u}_{1} & \cdots & \vect{u}_{m}
\end{array}
\rightB
\end{equation*}
while $V= \left( \vect{v}_{1}\cdots \vect{v}_{n}\right) .$ Thus $U$
is the matrix which has the $\vect{u}_{i}$ as columns and $V$ is defined
as the matrix which has the $\vect{v}_{i}$ as columns. Then 
\begin{equation*}
U^TAV=\leftB 
\begin{array}{c}
\vect{u}_{1}^T \\ 
\vdots \\ 
\vect{u}_{k}^T \\ 
\vdots \\ 
\vect{u}_{m}^T
\end{array}
\rightB A\leftB \vect{v}_{1}\cdots \vect{v}_{n}\rightB
\end{equation*}
\begin{equation*}
=\leftB 
\begin{array}{c}
\vect{u}_{1}^T \\ 
\vdots \\ 
\vect{u}_{k}^T \\ 
\vdots \\ 
\vect{u}_{m}^T
\end{array}
\rightB \leftB 
\begin{array}{cccccc}
\sigma _{1}\vect{u}_{1} & \cdots & \sigma _{k}\vect{u}_{k} & \vect{0}
& \cdots & \vect{0}
\end{array}
\rightB =\leftB 
\begin{array}{cc}
\sigma & 0 \\ 
0 & 0
\end{array}
\rightB
\end{equation*}
where $\sigma $ is given in the statement of the theorem. 
\end{proof}

The singular value decomposition has as an immediate corollary which is given in the following interesting result. 

\begin{corollary}{Rank and Singular Values}{ranksingularvalues}
Let $A$ be an $m\times n$ matrix. Then the rank of $A$ and $A^T$equals
the number of singular values.
\end{corollary}

%%\begin{proof}
%%Since $V$ and $U$ are unitary, it follows that 
%%\begin{eqnarray*}
%%\limfunc{rank}\left( A\right) &=&\limfunc{rank}\left( U^TAV\right) =
%%\limfunc{rank}\leftB
%%\begin{array}{cc}
%%\sigma & 0 \\ 
%%0 & 0
%%\end{array}
%%\rightB \\
%%&=&\text{number of singular values.}
%%\end{eqnarray*}
%%Also since $U,V$ are unitary, 
%%\begin{equation*}
%%\func{rank}\left( A^T\right) =\func{rank}\left( V^TA^{\ast
%%}U\right) =\func{rank}\left( \left( U^TAV\right) ^T\right)
%%\end{equation*}
%%\begin{equation*}
%%=\func{rank}\left( \leftB 
%%\begin{array}{cc}
%%\sigma & 0 \\ 
%%0 & 0
%%\end{array}
%%\rightB ^T\right) =\text{number of singular values.}
%%\end{equation*}
%%\end{proof}
%%
%%\medskip

Let's compute the Singular Value Decomposition of a simple matrix.

\begin{example}{Singular Value Decomposition}{SVD}
Let 
$A=\leftB\begin{array}{rrr} 1 & -1 & 3 \\ 3 & 1 & 1 \end{array}\rightB$.
Find the Singular Value Decomposition (SVD) of $A$.
\end{example}

\begin{solution}
To begin, we compute $AA^T$ and $A^TA$.
\[ AA^T = \leftB\begin{array}{rrr} 1 & -1 & 3 \\ 3 & 1 & 1 \end{array}\rightB
\leftB\begin{array}{rr} 1 & 3 \\ -1 & 1 \\ 3 & 1  \end{array}\rightB
= \leftB\begin{array}{rr} 11 & 5 \\ 5 & 11  \end{array}\rightB.\]

\[ A^TA = \leftB\begin{array}{rr} 1 & 3 \\ -1 & 1 \\ 3 & 1  \end{array}\rightB
\leftB\begin{array}{rrr} 1 & -1 & 3 \\ 3 & 1 & 1 \end{array}\rightB
= \leftB\begin{array}{rrr} 10 & 2 & 6 \\ 2 & 2 & -2\\
6 & -2 & 10 \end{array}\rightB.\]

Since $AA^T$ is $2\times 2$ while $A^T A$ is $3\times 3$, and $AA^T$
and $A^TA$ have the same {\em nonzero} eigenvalues (by Proposition
\ref{prop:samenonzeroeigenvalues}), we compute the characteristic polynomial  $c_{AA^T}(x)$ (because it's
easier to compute than $c_{A^TA}(x)$).

\begin{eqnarray*}
c_{AA^T}(x)& = &\det(xI-AA^T)= \left|\begin{array}{cc}
x-11 & -5 \\ -5 & x-11 \end{array}\right|\\
& = &(x-11)^2 - 25 \\
& = & x^2-22x+121-25\\
& = & x^2-22x+96\\
& = & (x-16)(x-6)
\end{eqnarray*}

Therefore, the eigenvalues of $AA^T$ are $\lambda_1=16$ and $\lambda_2=6$.

The eigenvalues of $A^TA$ are $\lambda_1=16$, $\lambda_2=6$, and
$\lambda_3=0$, and the singular values of $A$ are $\sigma_1=\sqrt{16}=4$ and
$\sigma_2=\sqrt{6}$.
By convention, we list the eigenvalues (and corresponding singular values)
in non increasing order (i.e., from largest to smallest).

{\bf To find the matrix $V$}:

To construct the matrix $V$ we need to find eigenvectors for $A^TA$.
Since the eigenvalues of $AA^T$ are distinct, the corresponding
eigenvectors are orthogonal, and we need only normalize them.

$\lambda_1=16$: solve $(16I-A^TA)Y= 0$.

\[ \leftB\begin{array}{rrr|r}
6 & -2 & -6 & 0 \\ -2 & 14 & 2 & 0 \\ -6 & 2 & 6 & 0
\end{array}\rightB
\rightarrow
\leftB\begin{array}{rrr|r}
1 & 0 & -1 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0
\end{array}\rightB,
\mbox{ so }
Y=\leftB\begin{array}{r} t \\ 0 \\ t \end{array}\rightB
=t\leftB\begin{array}{r} 1 \\ 0 \\ 1 \end{array}\rightB,
t\in \mathbb{R}. \]

$\lambda_2=6$: solve $(6I-A^TA)Y= 0$.

\[ \leftB\begin{array}{rrr|r}
-4 & -2 & -6 & 0 \\ -2 & 4 & 2 & 0 \\ -6 & 2 & -4 & 0
\end{array}\rightB
\rightarrow
\leftB\begin{array}{rrr|r}
1 & 0 & 1 & 0 \\ 0 & 1 & 1 & 0 \\ 0 & 0 & 0 & 0
\end{array}\rightB,
\mbox{ so }
Y=\leftB\begin{array}{r} -s \\ -s \\ s \end{array}\rightB
=s\leftB\begin{array}{r} -1 \\ -1 \\ 1 \end{array}\rightB,
s\in \mathbb{R}. \]

$\lambda_3=0$: solve $(-A^TA)Y= 0$.
\[ \leftB\begin{array}{rrr|r}
-10 & -2 & -6 & 0 \\ -2 & -2 & 2 & 0 \\ -6 & 2 & -10 & 0
\end{array}\rightB
\rightarrow
\leftB\begin{array}{rrr|r}
1 & 0 & 1 & 0 \\ 0 & 1 & -2 & 0 \\ 0 & 0 & 0 & 0
\end{array}\rightB,
\mbox{ so }
Y=\leftB\begin{array}{r} -r \\ 2r \\ r \end{array}\rightB
=r\leftB\begin{array}{r} -1 \\ 2 \\ 1 \end{array}\rightB,
r\in \mathbb{R}. \]


Let
\[ V_1=\frac{1}{\sqrt{2}}\leftB\begin{array}{r} 1\\ 0\\ 1 \end{array}\rightB,
V_2=\frac{1}{\sqrt{3}}\leftB\begin{array}{r} -1\\ -1\\ 1 \end{array}\rightB,
V_3=\frac{1}{\sqrt{6}}\leftB\begin{array}{r} -1\\ 2\\ 1 \end{array}\rightB.\]

Then
\[ V=\frac{1}{\sqrt{6}}\leftB\begin{array}{rrr}
\sqrt 3 & -\sqrt 2 & -1  \\
0 & -\sqrt 2 & 2 \\
\sqrt 3 & \sqrt 2 & 1 \end{array}\rightB.\]

Also,
\[ \Sigma = \leftB\begin{array}{rrr} 4 & 0 & 0 \\
0 & \sqrt 6 & 0 \end{array}\rightB,\]
and we use $A$, $V^T$, and $\Sigma$ to find $U$.

Since $V$ is orthogonal and $A=U\Sigma V^T$, it follows that $AV=U\Sigma$.
Let $V=\leftB\begin{array}{ccc} V_1 & V_2 & V_3 \end{array}\rightB$, and
let $U=\leftB\begin{array}{cc} U_1 & U_2 \end{array}\rightB$, where
$U_1$ and $U_2$ are the two columns of $U$.

Then we have
\begin{eqnarray*}
A\leftB\begin{array}{ccc} V_1 & V_2 & V_3 \end{array}\rightB
&=& \leftB\begin{array}{cc} U_1 & U_2 \end{array}\rightB\Sigma\\
\leftB\begin{array}{ccc} AV_1 & AV_2 & AV_3 \end{array}\rightB
&=& \leftB\begin{array}{ccc} \sigma_1U_1 + 0U_2 &
0U_1 + \sigma_2 U_2 & 0 U_1 + 0 U_2 \end{array}\rightB \\
&=& \leftB\begin{array}{ccc} \sigma_1U_1 & \sigma_2 U_2 &
0 \end{array}\rightB
\end{eqnarray*}
which implies that $AV_1=\sigma_1U_1 = 4U_1$ and
$AV_2=\sigma_2U_2 = \sqrt 6 U_2$.

Thus,
\[ U_1 = \frac{1}{4}AV_1 
= \frac{1}{4}
\leftB\begin{array}{rrr} 1 & -1 & 3 \\ 3 & 1 & 1 \end{array}\rightB
\frac{1}{\sqrt{2}}\leftB\begin{array}{r} 1\\ 0\\ 1 \end{array}\rightB
= \frac{1}{4\sqrt 2}\leftB\begin{array}{r} 4\\ 4 \end{array}\rightB
= \frac{1}{\sqrt 2}\leftB\begin{array}{r} 1\\ 1 \end{array}\rightB,\]
and
\[ U_2 = \frac{1}{\sqrt 6}AV_2 
= \frac{1}{\sqrt 6}
\leftB\begin{array}{rrr} 1 & -1 & 3 \\ 3 & 1 & 1 \end{array}\rightB
\frac{1}{\sqrt{3}}\leftB\begin{array}{r} -1\\ -1\\ 1 \end{array}\rightB
=\frac{1}{3\sqrt 2}\leftB\begin{array}{r} 3\\ -3 \end{array}\rightB
=\frac{1}{\sqrt 2}\leftB\begin{array}{r} 1\\ -1 \end{array}\rightB.
\]
Therefore,
\[ U=\frac{1}{\sqrt{2}}\leftB\begin{array}{rr} 1 & 1 \\
1 & -1 \end{array}\rightB,\]
and
\begin{eqnarray*}
A & = & \leftB\begin{array}{rrr} 1 & -1 & 3 \\ 3 & 1 & 1 \end{array}\rightB\\
& = & \left(\frac{1}{\sqrt{2}}\leftB\begin{array}{rr} 1 & 1 \\
1 & -1 \end{array}\rightB\right)
\leftB\begin{array}{rrr} 4 & 0 & 0 \\
0 & \sqrt 6 & 0 \end{array}\rightB
\left(\frac{1}{\sqrt{6}}\leftB\begin{array}{rrr}
\sqrt 3 & 0 & \sqrt 3  \\
-\sqrt 2 & -\sqrt 2 & \sqrt2 \\
-1 & 2 & 1 \end{array}\rightB\right).
\end{eqnarray*}
\end{solution}

Here is another example. 

\begin{example}{Finding the SVD}{SVD2}
Find an SVD for
$A=\leftB\begin{array}{r} -1 \\ 2\\ 2 \end{array}\rightB$.
\end{example}

\begin{solution}
Since $A$ is $3\times 1$, $A^T A$ is a $1\times 1$ matrix
whose eigenvalues are easier to find than the eigenvalues of
the $3\times 3$ matrix $AA^T$.

\[ A^TA=\leftB\begin{array}{ccc} -1 & 2 & 2 \end{array}\rightB
\leftB\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\rightB
=\leftB\begin{array}{r} 9 \end{array}\rightB.\]

Thus $A^TA$ has eigenvalue $\lambda_1=9$, and
the eigenvalues of $AA^T$ are $\lambda_1=9$, $\lambda_2=0$, and
$\lambda_3=0$. 
Furthermore, $A$ has only one singular value, $\sigma_1=3$.

{\bf To find the matrix $V$}:
To do so we find an eigenvector for $A^TA$ and
normalize it.
In this case, finding a unit eigenvector is trivial:
$V_1=\leftB\begin{array}{r} 1 \end{array}\rightB$, and
\[ V=\leftB\begin{array}{r} 1 \end{array}\rightB.\]

Also,
$\Sigma =\leftB\begin{array}{r} 3 \\ 0\\ 0 \end{array}\rightB$,
and we use $A$, $V^T$, and $\Sigma$ to find $U$.


Now $AV=U\Sigma$, with
$V=\leftB\begin{array}{r} V_1 \end{array}\rightB$,
and $U=\leftB\begin{array}{rrr} U_1 & U_2 & U_3 \end{array}\rightB$,
where $U_1$, $U_2$, and $U_3$ are the columns of $U$.
Thus
\begin{eqnarray*}
A\leftB\begin{array}{r} V_1 \end{array}\rightB
&=& \leftB\begin{array}{rrr} U_1 & U_2 & U_3 \end{array}\rightB\Sigma\\
\leftB\begin{array}{r} AV_1 \end{array}\rightB
&=& \leftB\begin{array}{r} \sigma_1 U_1+0U_2+0U_3 \end{array}\rightB\\
&=& \leftB\begin{array}{r} \sigma_1 U_1 \end{array}\rightB
\end{eqnarray*}
This gives us $AV_1=\sigma_1 U_1= 3U_1$, so

\[ U_1 = \frac{1}{3}AV_1 
= \frac{1}{3}
\leftB\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\rightB
\leftB\begin{array}{r} 1 \end{array}\rightB
= \frac{1}{3}
\leftB\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\rightB.\]

The vectors $U_2$ and $U_3$ are eigenvectors of $AA^T$ corresponding
to the eigenvalue $\lambda_2=\lambda_3=0$.
Instead of solving the system $(0I-AA^T)X= 0$ and then using the
Gram-Schmidt process on the resulting set of
two basic eigenvectors, the following approach may be used.


Find vectors $U_2$ and $U_3$ by first extending $\{ U_1\}$ to a basis of
$\mathbb{R}^3$, then using the Gram-Schmidt algorithm to orthogonalize the basis,
and finally normalizing the vectors.

Starting with $\{ 3U_1 \}$ instead of $\{ U_1 \}$ makes the
arithmetic a bit easier.
It is easy to verify that

\[ \left\{ \leftB\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\rightB,
\leftB\begin{array}{r} 1 \\ 0 \\ 0 \end{array}\rightB,
\leftB\begin{array}{r} 0 \\ 1 \\ 0 \end{array}\rightB\right\}\]
is a basis of $\mathbb{R}^3$.  Set

\[ E_1 = \leftB\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\rightB,
X_2 = \leftB\begin{array}{r} 1 \\ 0 \\ 0 \end{array}\rightB,
X_3 =\leftB\begin{array}{r} 0 \\ 1 \\ 0 \end{array}\rightB,\]

and apply the Gram-Schmidt algorithm to
$\{ E_1, X_2, X_3\}$.

This gives us

\[ E_2 = \leftB\begin{array}{r} 4 \\ 1 \\ 1 \end{array}\rightB
\mbox{ and }
E_3 = \leftB\begin{array}{r} 0 \\ 1 \\ -1 \end{array}\rightB.\]

Therefore,
\[ U_2 = \frac{1}{\sqrt{18}}
 \leftB\begin{array}{r} 4 \\ 1 \\ 1 \end{array}\rightB,
U_3 = \frac{1}{\sqrt 2}
\leftB\begin{array}{r} 0 \\ 1 \\ -1 \end{array}\rightB,\]
and

\[ U = \leftB\begin{array}{rrr} -\frac{1}{3} & \frac{4}{\sqrt{18}} & 0 \\
\frac{2}{3} & \frac{1}{\sqrt{18}} & \frac{1}{\sqrt 2} \\
\frac{2}{3} & \frac{1}{\sqrt{18}} & -\frac{1}{\sqrt 2} \end{array}\rightB.\]

Finally,

\[ A = 
\leftB\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\rightB
=
\leftB\begin{array}{rrr} -\frac{1}{3} & \frac{4}{\sqrt{18}} & 0 \\
\frac{2}{3} & \frac{1}{\sqrt{18}} & \frac{1}{\sqrt 2} \\
\frac{2}{3} & \frac{1}{\sqrt{18}} & -\frac{1}{\sqrt 2} \end{array}\rightB
\leftB\begin{array}{r} 3 \\ 0 \\ 0 \end{array}\rightB
\leftB\begin{array}{r} 1 \end{array}\rightB.\]

\end{solution}

Consider another example.

\begin{example}{Find the SVD}{SVD3}
Find a singular value decomposition for the matrix 
\begin{equation*}
A= \leftB  
\begin{array}{ccc}
\frac{2}{5}\sqrt{2}\sqrt{5} & \frac{4}{5}\sqrt{2}\sqrt{5} & 0 \\ 
\frac{2}{5}\sqrt{2}\sqrt{5} & \frac{4}{5}\sqrt{2}\sqrt{5} & 0
\end{array}
\rightB
\end{equation*}
\end{example}

First consider $A^TA$
\begin{equation*}
\leftB 
\begin{array}{ccc}
\frac{16}{5} & \frac{32}{5} & 0 \\ 
\frac{32}{5} & \frac{64}{5} & 0 \\ 
0 & 0 & 0
\end{array}
\rightB
\end{equation*}
What are some eigenvalues and eigenvectors? Some computing shows these are

\begin{equation*}
\left\{ \leftB 
\begin{array}{c}
0 \\ 
0 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{c}
-\frac{2}{5}\sqrt{5} \\ 
\frac{1}{5}\sqrt{5} \\ 
0
\end{array}
\rightB \right\} \leftrightarrow 0,\left\{ \leftB 
\begin{array}{c}
\frac{1}{5}\sqrt{5} \\ 
\frac{2}{5}\sqrt{5} \\ 
0
\end{array}
\rightB \right\} \leftrightarrow 16
\end{equation*}
Thus the matrix $V$ is given by 
\begin{equation*}
V=\leftB 
\begin{array}{ccc}
\frac{1}{5}\sqrt{5} & -\frac{2}{5}\sqrt{5} & 0 \\ 
\frac{2}{5}\sqrt{5} & \frac{1}{5}\sqrt{5} & 0 \\ 
0 & 0 & 1
\end{array}
\rightB
\end{equation*}
Next consider $AA^T$
\begin{equation*}
\leftB 
\begin{array}{cc}
8 & 8 \\ 
8 & 8
\end{array}
\rightB
\end{equation*}
Eigenvectors and eigenvalues are

\begin{equation*}
\left\{ \leftB 
\begin{array}{c}
-\frac{1}{2}\sqrt{2} \\ 
\frac{1}{2}\sqrt{2}
\end{array}
\rightB \right\} \leftrightarrow 0,\left\{ \leftB 
\begin{array}{c}
\frac{1}{2}\sqrt{2} \\ 
\frac{1}{2}\sqrt{2}
\end{array}
\rightB \right\} \leftrightarrow 16
\end{equation*}
Thus you can let $U$ be given by 
\begin{equation*}
U=\leftB 
\begin{array}{cc}
\frac{1}{2}\sqrt{2} & -\frac{1}{2}\sqrt{2} \\ 
\frac{1}{2}\sqrt{2} & \frac{1}{2}\sqrt{2}
\end{array}
\rightB
\end{equation*}
Lets check this. $U^TAV=$ 
\begin{equation*}
\leftB 
\begin{array}{cc}
\frac{1}{2}\sqrt{2} & \frac{1}{2}\sqrt{2} \\ 
-\frac{1}{2}\sqrt{2} & \frac{1}{2}\sqrt{2}
\end{array}
\rightB \leftB  
\begin{array}{ccc}
\frac{2}{5}\sqrt{2}\sqrt{5} & \frac{4}{5}\sqrt{2}\sqrt{5} & 0 \\ 
\frac{2}{5}\sqrt{2}\sqrt{5} & \frac{4}{5}\sqrt{2}\sqrt{5} & 0
\end{array}
\rightB \leftB 
\begin{array}{ccc}
\frac{1}{5}\sqrt{5} & -\frac{2}{5}\sqrt{5} & 0 \\ 
\frac{2}{5}\sqrt{5} & \frac{1}{5}\sqrt{5} & 0 \\ 
0 & 0 & 1
\end{array}
\rightB
\end{equation*}
\begin{equation*}
=\leftB 
\begin{array}{ccc}
4 & 0 & 0 \\ 
0 & 0 & 0
\end{array}
\rightB
\end{equation*}

This illustrates that if you have a good way to find the eigenvectors and
eigenvalues for a Hermitian matrix which has nonnegative eigenvalues, then
you also have a good way to find the singular value decomposition of an
arbitrary matrix.