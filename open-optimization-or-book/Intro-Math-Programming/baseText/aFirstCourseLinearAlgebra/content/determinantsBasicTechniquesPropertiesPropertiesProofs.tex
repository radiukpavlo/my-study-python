\subsection{Properties of Determinants II: Some Important Proofs } \label{sec:determinantproofs}

This section includes some important proofs on determinants and cofactors.

First we recall the definition of a determinant. If $A=\leftB a_{ij} \rightB$ is an $n\times n$ matrix, then $\det A$ is defined by computing the expansion along the first row:
\begin{equation}
\label{E1} 
\det A=\sum_{i=1}^n a_{1,i} \func{cof}(A)_{1,i}. 
\end{equation}
If $n=1$ then $\det A=a_{1,1}$. 

The following example is straightforward and strongly recommended as a means for
getting used to definitions. 

\begin{example}{}{EX1}
(1) Let $E_{ij}$ be the elementary matrix obtained by interchanging $i$th and $j$th rows of $I$. 
Then $\det E_{ij}=-1$. 

(2) Let $E_{ik}$ be the elementary matrix obtained by multiplying the $i$th row of $I$ by $k$. 
Then $\det E_{ik}=k$. 

(3) Let $E_{ijk}$ be the elementary matrix obtained by multiplying $i$th row of $I$ by $k$ and 
adding it to its $j$th row. Then $\det E_{ijk}=1$. 

(4) If $C$ and $B$ are such that $CB$ is defined and the $i$th row of $C$ consists of zeros, 
then the $i$th row of $CB$ consists of zeros. 

(5) If $E$ is an elementary matrix, then $\det E=\det E^T$. 
\end{example} 

Many of the proofs in section use the Principle of Mathematical Induction. This concept is discussed in Appendix A.2 and is reviewed here for convenience.
First we check that the assertion is true for $n=2$ (the case $n=1$ is either completely trivial
or meaningless). 

Next, we assume that the assertion is true for $n-1$ (where $n\geq 3$) and prove it for $n$. 
Once this is accomplished, by the Principle of Mathematical Induction we can conclude that the 
statement is true for all $n\times n$ matrices for every $n\geq 2$. 

If $A$ is an $n\times n$ matrix and $1\leq j \leq n$,
then the matrix obtained by removing $1$st column and $j$th row from $A$ 
is an $n-1\times n-1$ matrix (we shall denote this matrix by $A(j)$ below). Since these matrices 
are used in computation of cofactors $\func{cof}(A)_{1,i}$, for $1\leq i\neq n$, 
the inductive assumption applies to these matrices. 

Consider the following lemma.

\begin{lemma}{}{L1} 
If $A$ is an $n\times n$ matrix such that one of its rows consists of zeros, then 
$\det A=0$. 
\end{lemma} 

\begin{proof} 
We will prove this lemma using Mathematical Induction. 

If $n=2$ this is easy (check!). 

Let $n\geq 3$ be such that every matrix of size $n-1\times n-1$ with a row consisting of zeros
has determinant equal to zero.  
Let $i$ be such that the $i$th row of $A$ consists of zeros. 
Then we have $a_{ij}=0$ for $1\leq j\leq n$. 

Fix $j\in \{1,2, \dots ,n\}$ such that $j\neq i$. Then  matrix $A(j)$ used in computation of
 $\func{cof}(A)_{1,j}$ has a row consisting of zeros, and by our inductive 
 assumption $\func{cof}(A)_{1,j}=0$. 

On the other hand, if $j=i$ then $a_{1,j}=0$.  
Therefore $a_{1,j}\func{cof}(A)_{1,j}=0$ for all $j$ and by \eqref{E1} we have 
\[
\det A=\sum_{j=1}^n a_{1,j} \func{cof}(A)_{1,j}=0
\]
as each of the summands is equal to 0. 
\end{proof} 

\begin{lemma}{}{L2} 
Assume $A$, $B$ and $C$ are $n\times n$ matrices that for some 
 $1\leq i\leq n$ satisfy the following. 
\begin{enumerate}
\item $j$th rows of all three matrices are identical, for $j\neq i$. 

\item Each entry in the $j$th row of $A$ is the sum of the corresponding 
entries in $j$th rows of $B$ and $C$. 
\end{enumerate}

Then $\det A=\det B+\det C$. 
\end{lemma} 

\begin{proof} 
This is not difficult to check for $n=2$ (do check it!). 

Now assume that the statement of Lemma is true for $n-1\times n-1$ matrices and 
fix $A,B$ and $C$ as in the statement. 
The assumptions state that 
we have  $a_{l,j}=b_{l,j}=c_{l,j}$ for $j\neq i$ and for $1\leq l\leq n$
and $a_{l,i}=b_{l,i}+c_{l,i}$ for all $1\leq l\leq n$. 
Therefore $A(i)=B(i)=C(i)$, and $A(j)$ has the property that 
its $i$th row is the sum of $i$th rows of $B(j)$ and $C(j)$ for $j\neq i$ while the  other rows of all three matrices are identical.  
Therefore by our inductive assumption we have $\func{cof}(A)_{1j}=\func{cof}(B)_{1j}+\func{cof}(C)_{1j}$
for $j\neq i$. 

By \eqref{E1} we have (using all equalities established above) 
\begin{align*}
\det A&=\sum_{l=1}^n a_{1,l} \func{cof}(A)_{1,l}\\
 &=\sum_{l\neq i} a_{1,l}(\func{cof}(B)_{1,l}+\func{cof}(C)_{1,l})+ 
(b_{1,i}+c_{1,i})\func{cof}(A)_{1,i}\\
&=
\det B+\det C
\end{align*}
This proves that the assertion is true for all $n$ and completes the proof.  
\end{proof} 

\begin{theorem}{} {T1} 
Let $A$ and $B$ be $n\times n$ matrices. 
\begin{enumerate}
\item If $A$ is obtained by interchanging $i$th and $j$th rows  of $B$ (with $i\neq j$), then $\det A=-\det B$. 
\item If $A$ is obtained by multiplying $i$th row of $B$ by $k$ then $\det A=k\det B$. 
\item If two rows of $A$ are identical then $\det A=0$. 
\item If $A$ is obtained by multiplying $i$th row of $B$ by $k$ and adding it to 
$j$th row of $B$ ($i\neq j$) then $\det A=\det B$. 
\end{enumerate}
\end{theorem}

\begin{proof} 
We prove all statements by induction. The case $n=2$ is easily checked directly (and it is strongly suggested that you do check it). 

We assume $n\geq 3$ and  (1)--(4) are true for all matrices of size $n-1\times n-1$. 

(1) 
We prove the case when $j=i+1$, i.e., we are interchanging two consecutive rows. 

Let $l\in \{1, \dots, n\}\setminus \{i,j\}$. 
Then $A(l)$ is obtained from $B(l)$ by interchanging two of its rows (draw a picture) and 
by our assumption 
\begin{equation} 
\label{E2} 
\func{cof}(A)_{1,l}=-\func{cof}(B)_{1,l}. 
\end{equation} 

Now consider $a_{1,i} \func{cof}(A)_{1,l}$. We have that $a_{1,i}=b_{1,j}$ 
and also that $A(i)=B(j)$. Since $j=i+1$, we have 
\[
(-1)^{1+j}=(-1)^{1+i+1}=-(-1)^{1+i} 
\]
and therefore $a_{1i}\func{cof}(A)_{1i}=-b_{1j} \func{cof}(B)_{1j}$ and $a_{1j}\func{cof}(A)_{1j}=-b_{1i} \func{cof}(B)_{1i}$. 
Putting this together with \eqref{E2} into \eqref{E1} we see that if in the formula
for $\det A$ we change the sign of each of the summands we obtain the formula for $\det B$. 
\[
\det A=\sum_{l=1}^n a_{1l}\func{cof}(A)_{1l}
=-\sum_{l=1}^n b_{1l} B_{1l}
=\det B. 
\]

We have therefore proved the case of (1) when $j=i+1$. In order to prove the general case, 
one needs the following fact. If $i<j$, then in order to interchange $i$th and $j$th row one 
can proceed by interchanging two adjacent rows $2(j-i)+1$ times: 
First swap $i$th and $i+1$st, then $i+1$st and $i+2$nd, and so on. 
After one interchanges $j-1$st and $j$th row, we have $i$th row in position of $j$th
and $l$th row in position of $l-1$st for $i+1\leq l\leq j$. Then proceed backwards 
swapping adjacent 
rows until everything is in place. 

Since $2(j-i)+1$ is an odd number $(-1)^{2(j-i)+1}=-1$ and 
we have that $\det A=-\det B$. 

(2) This is like (1)\dots{} but much easier. 
Assume that (2) is true for all $n-1\times n-1$ matrices. 
We have that $a_{ji}=k b_{ji}$ for $1\leq j\leq n$. 
In particular $a_{1i}=kb_{1i}$, and for $l\neq i$ matrix 
$A(l)$ is obtained from $B(l)$ by multiplying one of its rows by $k$. 
Therefore $\func{cof}(A)_{1l}=k\func{cof}(B)_{1l}$ for $l\neq i$, 
and for all $l$ we have $a_{1l} \func{cof}(A)_{1l}=k b_{1l}\func{cof}(B)_{1l}$. 
By \eqref{E1}, we have $\det A=k\det B$. 

(3) This is a consequence of (1). If two rows of $A$ are identical, then 
$A$ is equal to the matrix obtained by interchanging those two rows and 
therefore by (1) $\det A=-\det A$. This implies $\det A=0$. 

(4) Assume (4) is true for all $n-1\times n-1$ matrices
and fix $A$ and $B$ such that
 $A$ is obtained by multiplying $i$th row of $B$ by $k$ and adding it to 
$j$th row of $B$ ($i\neq j$) then $\det A=\det B$. 
If $k=0$ then $A=B$ and there is nothing to prove, so we may assume $k\neq 0$. 

Let $C$ be the matrix obtained by replacing the $j$th row of $B$ by the $i$th row of $B$ 
multiplied by $k$. 
By Lemma \ref{lem:L2}, we have that 
\[
\det A=\det B+\det C
\]
and we `only' need to show that $\det C=0$. But $i$th and $j$th rows of $C$
are proportional. If $D$ is obtained by multiplying the $j$th row of $C$ by $\frac 1k$
then  by (2) we have $\det C=\frac 1k\det D$ (recall that $k\neq 0$!). 
But $i$th and $j$th rows of $D$ are identical, hence by (3) we have $\det D=0$
and therefore $\det C=0$. 
\end{proof} 

\begin{theorem}{}{T2}
Let $A$ and $B$ be two $n\times n$ matrices. Then \index{determinant!product}
\begin{equation*}
\det \left( AB\right) =\det \left( A\right) \det \left( B\right)
\end{equation*}
\end{theorem}

\begin{proof} If $A$ is an elementary matrix of either type, then multiplying
by $A$ on the left has the same effect as performing the corresponding elementary 
row operation. Therefore the equality $\det (AB) =\det A\det B$  in this case follows by Example \ref{exa:EX1} 
and Theorem \ref{thm:T1}. 

If $C$ is the \rref\;of $A$ then we can write $A=E_1\cdot E_2\cdot\dots\cdot E_m\cdot C$
for some elementary matrices $E_1,\dots, E_m$. 

Now we consider two cases. 

Assume first that  $C=I$. Then $A=E_1\cdot E_2\cdot \dots\cdot E_m$ 
and $AB= E_1\cdot E_2\cdot \dots\cdot E_m B$. 
    By applying the above equality $m$ times, and then $m-1$ times,   
we have that 
\begin{align*}
\det AB&=\det E_1\det E_2\cdot \det E_m\cdot \det B\\
&=\det (E_1\cdot E_2\cdot\dots\cdot E_m) \det B\\
&=\det A\det B. 
\end{align*} 

Now assume $C\neq I$. Since it is in \rref, its last row consists of zeros 
and by (4) of Example \ref{exa:EX1} the last row of $CB$ consists of zeros. 
By Lemma \ref{lem:L1} we have $\det C=\det (CB)=0$ and therefore 
\[
\det A=\det (E_1\cdot E_2\cdot  E_m)\cdot  \det (C)
=
\det (E_1\cdot E_2\cdot  E_m)\cdot 0=0
\]
and also 
\[
\det AB=\det (E_1\cdot E_2\cdot  E_m)\cdot  \det (C B)
=\det (E_1\cdot E_2\cdot\dots\cdot E_m) 0
=0
\]
hence $\det AB=0=\det A \det B$. 
\end{proof} 

The same `machine' used in the previous proof will be used again. 

\begin{theorem}{}{T.T} 
Let $A$ be a matrix where $A^T$ is the transpose of $A$. Then,
\begin{equation*}
\det\left(A^T\right) = \det \left( A \right)
\end{equation*}
\end{theorem} 

\begin{proof} 
Note first that the conclusion is true if $A$ is elementary by (5) of Example \ref{exa:EX1}. 

Let $C$ be the \rref\;of $A$. Then we can write 
$A= E_1\cdot E_2\cdot \dots\cdot E_m C$. 
Then $A^T=C^T\cdot E_m^T\cdot \dots \cdot E_2^T\cdot E_1$. 
By Theorem \ref{thm:T2} we have 
\[
\det (A^T)=\det (C^T)\cdot \det (E_m^T)\cdot \dots \cdot \det (E_2^T)\cdot \det(E_1).
\] 
By (5) of Example \ref{exa:EX1} we have that $\det E_j=\det E_j^T$ for all $j$. 
Also, $\det C$ is either 0 or 1 (depending on whether $C=I$ or not) and in either 
case $\det C=\det C^T$. Therefore $\det A=\det A^T$. 
\end{proof} 

The above discussions allow us to now prove Theorem \ref{thm:welldefineddeterminant}. It is restated below. 
 
\begin{theorem}{}{}
Expanding an $n\times n$ matrix along any row or column always gives the same result, which is the determinant. 
\end{theorem} 

\begin{proof} We first show that the determinant can be computed along any row. The case $n=1$ does not apply and thus let $n \geq 2$. 

%Assume the theorem is true for all $n-1\times n-1$ matrices.  

Let $A$be an $n\times n$ matrix and fix $j>1$. We need to prove that
\[
	\det A=\sum_{i=1}^n a_{j,i} \func{cof}(A)_{j,i}. 
\]
Let us prove the case when $j=2$. 

Let $B$ be the matrix obtained from $A$ by interchanging its $1$st and $2$nd rows. 
Then by  Theorem \ref{thm:T1} we have
\[
\det A=-\det B. 
\]
Now we have 
\[
\det B=\sum_{i=1}^n b_{1,i} \func{cof}(B)_{1,i}. 
\]
Since $B$ is obtained by interchanging the $1$st and $2$nd rows of $A$
we have that $b_{1,i}=a_{2,i}$ for all $i$
and one can see that $minor(B)_{1,i}=minor(A)_{2,i}$. 

Further, 
\[
\func{cof}(B)_{1,i}=(-1)^{1+i} minor B_{1,i}=- (-1)^{2+i} minor (A)_{2,i} = - \func{cof}(A)_{2,i}
\]
hence $\det B=-\sum_{i=1}^n a_{2,i} \func{cof}(A)_{2,i}$, and therefore 
$\det A=-\det B=
\sum_{i=1}^n a_{2,i} \func{cof}(A)_{2,i}$ as desired. 

The case when $j>2$ is very similar; we still have
$minor(B)_{1,i}=minor (A)_{j,i}$ but checking that $\det
B=-\sum_{i=1}^n a_{j,i} \func{cof}(A)_{j,i}$ is slightly more
involved.

Now the cofactor expansion along column $j$ of $A$ is equal to the
cofactor expansion along row $j$ of $A^T$, which is by the above
result just proved equal to the cofactor expansion along row 1 of
$A^T$, which is equal to the cofactor expansion along column $1$ of
$A$. Thus the cofactor cofactor along any column yields the same result. 

Finally, since $\det A=\det A^T$ by  Theorem \ref{thm:T.T}, we conclude that 
the cofactor expansion along row $1$ of $A$ is equal to 
the cofactor expansion along row $1$ of $A^T$, which  is equal to 
the cofactor expansion along column $1$ of $A$. Thus the proof is complete. 
\end{proof} 