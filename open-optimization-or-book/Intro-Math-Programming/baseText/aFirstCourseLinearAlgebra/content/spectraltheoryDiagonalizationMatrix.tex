\subsection{Diagonalizing a Matrix}

The most important theorem about diagonalizability is the following major result.

\begin{theorem}{Eigenvectors and Diagonalizable Matrices}{eigenvectorsanddiagonalizable}
An $n\times n$ matrix $A$ is diagonalizable if and only if there is an
invertible matrix $P$ given by 
\begin{equation*}
P=\leftB
\begin{array}{cccc}
X_{1} & X_{2} & \cdots & X_{n}
\end{array}
\rightB
\end{equation*}
where the $X_{k}$ are eigenvectors of $A$. 

Moreover if $A$ is diagonalizable, the corresponding eigenvalues of $A$ are the
diagonal entries of the diagonal matrix $D$.
\end{theorem}

\begin{proof}
Suppose $P$ is given as above as an invertible matrix whose columns are eigenvectors of $A$. Then $P^{-1}$
is of the form
\begin{equation*}
P^{-1}=\leftB
\begin{array}{c}
W_{1}^{T} \\
W_{2}^{T} \\
\vdots \\
W_{n}^{T}
\end{array}
\rightB
\end{equation*}
where $W_{k}^{T}X_{j}=\delta _{kj},$ which is the Kronecker's symbol defined by
\begin{equation*}
\delta _{ij}=\left\{
\begin{array}{c}
1
\text{ if }i=j \\
0\text{ if }i\neq j
\end{array}
\right.
\end{equation*}

Then
\begin{eqnarray*}
P^{-1}AP & = & 
\leftB
\begin{array}{c}
W_{1}^{T} \\
W_{2}^{T} \\
\vdots \\
W_{n}^{T}
\end{array}
\rightB \leftB
\begin{array}{cccc}
AX_{1} & AX_{2} & \cdots & AX_{n}
\end{array}
\rightB \\
& = & 
\leftB
\begin{array}{c}
W_{1}^{T} \\
W_{2}^{T} \\
\vdots \\
W_{n}^{T}
\end{array}
\rightB \leftB
\begin{array}{cccc}
\lambda _{1}X_{1} & \lambda _{2}X_{2} & \cdots & \lambda
_{n}X_{n}
\end{array}
\rightB \\
&=&
\leftB
\begin{array}{ccc}
\lambda _{1} &  & 0 \\
& \ddots &  \\
0 &  & \lambda _{n}
\end{array}
\rightB \\
\end{eqnarray*}

Conversely, suppose $A$ is diagonalizable so that $P^{-1}AP=D.$ Let 
\begin{equation*}
P=\leftB
\begin{array}{cccc}
X_{1} & X_{2} & \cdots & X_{n}
\end{array}
\rightB 
\end{equation*}
 where the columns are the $X_{k}$ and
\begin{equation*}
D=\leftB
\begin{array}{ccc}
\lambda _{1} &  & 0 \\
& \ddots &  \\
0 &  & \lambda _{n}
\end{array}
\rightB
\end{equation*}
Then
\begin{equation*}
AP=PD=\leftB
\begin{array}{cccc}
X_{1} & X_{2} & \cdots & X_{n}
\end{array}
\rightB \leftB
\begin{array}{ccc}
\lambda _{1} &  & 0 \\
& \ddots &  \\
0 &  & \lambda _{n}
\end{array}
\rightB
\end{equation*}
and so
\begin{equation*}
\leftB
\begin{array}{cccc}
AX_{1} & AX_{2} & \cdots & AX_{n}
\end{array}
\rightB =\leftB
\begin{array}{cccc}
\lambda _{1}X_{1} & \lambda _{2}X_{2} & \cdots & \lambda
_{n}X_{n}
\end{array}
\rightB
\end{equation*}
showing the $X_{k}$ are eigenvectors of $A$ and the $\lambda _{k}$
are eigenvectors.
%% Now the $X_{k}$ form a basis for $\mathbb{R}^{n}$
%%because the matrix $S$ having these vectors as columns is given to be
%%invertible. 
\end{proof}

Notice that because the matrix $P$ defined above is invertible it follows that the set of eigenvectors of $A$, $\left\{ X_1, X_2, \cdots, X_n \right\}$, form a basis of $\mathbb{R}^n$. 

We demonstrate the concept given in the above theorem in the next example. Note that not only
are the columns of the matrix $P$ formed by eigenvectors, but $P$ must
be invertible so must consist of a wide variety of eigenvectors. We
achieve this by using basic eigenvectors for the columns of $P$.

\begin{example}{Diagonalize a Matrix}{diagonalizematrix}
Let 
\begin{equation*}
A=\leftB
\begin{array}{rrr}
2 & 0 & 0 \\
1 & 4 & -1 \\
-2 & -4 & 4
\end{array}
\rightB 
\end{equation*}
 Find an invertible matrix $P$ and a diagonal matrix $D$ such that $P^{-1}AP=D$.
\end{example}

\begin{solution}
By Theorem \ref{thm:eigenvectorsanddiagonalizable} we use the eigenvectors of $A$ as the columns of $P$, and
the corresponding eigenvalues of $A$ as the diagonal entries of $D$. 

First, we will find the eigenvalues of $A$. To do so, we solve $\det \left( \eigenVar I -A \right) =0$ as follows.
\begin{equation*}
\det
\left(
\eigenVar
\leftB
\begin{array}{rrr}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}
\rightB
-
\leftB
\begin{array}{rrr}
2 & 0 & 0 \\
1 & 4 & -1 \\
-2 & -4 & 4
\end{array}
\rightB
\right)
=
0
\end{equation*}

This computation is left as an exercise, and you should verify that the eigenvalues are $\lambda_1 =2,
\lambda_2 = 2$, and $\lambda_3 = 6$.

Next, we need to find the eigenvectors. We first find the eigenvectors for $\lambda_1, \lambda_2 = 2$. 
Solving $\left(2I - A \right)X = 0$ to find the eigenvectors, we find that the
eigenvectors are
\begin{equation*}
t\leftB
\begin{array}{r}
-2 \\
1 \\
0
\end{array}
\rightB +s\leftB
\begin{array}{r}
1 \\
0 \\
1
\end{array}
\rightB
\end{equation*}

where $t,s$ are scalars. Hence there are two basic eigenvectors which are given by
\begin{equation*}
X_1
=
\leftB
\begin{array}{r}
-2 \\
1 \\
0
\end{array}
\rightB,
X_2
=
\leftB
\begin{array}{r}
1 \\
0 \\
1
\end{array}
\rightB
\end{equation*}

You can verify that the basic eigenvector for $\lambda_3 =6$ is $ X_3 = \leftB
\begin{array}{r}
0 \\
1 \\
-2
\end{array}
\rightB$

Then, we construct the matrix $P$ as follows. 
\begin{equation*}
P=
\leftB
\begin{array}{rrr}
X_1 & X_2 & X_3
\end{array}
\rightB
=
\leftB
\begin{array}{rrr}
-2 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & -2
\end{array}
\rightB
\end{equation*}
That is, the columns of $P$ are the basic  eigenvectors of $A$. Then, you can verify that
\begin{equation*}
P^{-1}=\leftB
\begin{array}{rrr}
-\vspace{0.05in}\frac{1}{4} & \vspace{0.05in}\frac{1}{2} & \vspace{0.05in}\frac{1}{4} \\
\vspace{0.05in}\frac{1}{2} & 1 & \vspace{0.05in}\frac{1}{2} \\
\vspace{0.05in}\frac{1}{4} & \vspace{0.05in}\frac{1}{2} & -\vspace{0.05in}\frac{1}{4}
\end{array}
\rightB 
\end{equation*}
Thus, 
\begin{eqnarray*}
P^{-1}AP &=&\leftB
\begin{array}{rrr}
-\vspace{0.05in}\frac{1}{4} & \vspace{0.05in}\frac{1}{2} & \vspace{0.05in}\frac{1}{4} \\
\vspace{0.05in}\frac{1}{2} & 1 & \vspace{0.05in}\frac{1}{2} \\
\vspace{0.05in}\frac{1}{4} & \vspace{0.05in}\frac{1}{2} & -\vspace{0.05in}\frac{1}{4}
\end{array}
\rightB \leftB
\begin{array}{rrr}
2 & 0 & 0 \\
1 & 4 & -1 \\
-2 & -4 & 4
\end{array}
\rightB \leftB
\begin{array}{rrr}
-2 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & -2
\end{array}
\rightB \\
&=&\leftB
\begin{array}{rrr}
2 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & 6
\end{array}
\rightB 
\end{eqnarray*}

You can see that the result here is a diagonal matrix where the entries on the main diagonal are the eigenvalues of $A$. We expected this based on 
Theorem \ref{thm:eigenvectorsanddiagonalizable}. Notice that eigenvalues on the main diagonal {\em must\em} be in the same order as the corresponding eigenvectors in $P$. 
\end{solution}

Consider the next important theorem.

\begin{theorem}{Linearly Independent Eigenvectors}{linindepeigenvectors}
Let $A$ be an $n\times n$ matrix, and suppose that $A$ 
has distinct eigenvalues $\lambda_1, \lambda_2, \ldots, \lambda_m$.
For each $i$, let $X_i$ be a $\lambda_i$-eigenvector of $A$.
Then $\{ X_1, X_2, \ldots, X_m\}$ is 
linearly independent.
\end{theorem}

The corollary that follows from this theorem gives a useful tool in determining if $A$ is diagonalizable. 

\begin{corollary}{Distinct Eigenvalues}{distincteigenvalues}
Let $A$ be an $n \times n$ matrix and suppose it has $n$ distinct eigenvalues. Then it follows that $A$ is diagonalizable.
\end{corollary}

It is possible that a matrix $A$ cannot be diagonalized. In other words, we cannot find an invertible matrix $P$ so that $P^{-1}AP=D$.

Consider the following example. 

\begin{example}{A Matrix which cannot be Diagonalized}{impossiblediagonalize}
Let
\begin{equation*}
A = 
\leftB
\begin{array}{rr}
1 & 1 \\
0 & 1
\end{array}
\rightB
\end{equation*}
If possible, find an invertible matrix $P$ and diagonal matrix $D$ so that $P^{-1}AP=D$.
\end{example}

\begin{solution}
Through the usual procedure, we find that the eigenvalues of $A$ are $\lambda_1 =1, \lambda_2=1.$ 
To find the eigenvectors, we solve the equation $\left(\lambda I - A \right) X = 0$.
The matrix $\left(\lambda I -A \right)$ is given by 
\begin{equation*}
\leftB
\begin{array}{cc}
\lambda - 1 & -1 \\
0 & \lambda - 1
\end{array}
\rightB
\end{equation*}

Substituting in $\lambda = 1$, we have the matrix
\begin{equation*}
\leftB
\begin{array}{cc}
1 - 1 & -1 \\
0 & 1 - 1
\end{array}
\rightB
=
\leftB
\begin{array}{rr}
0 & -1 \\
0 & 0
\end{array}
\rightB
\end{equation*}

Then, solving the equation $\left(\lambda I - A\right) X = 0$ 
involves carrying the following augmented matrix to its \rref. 
\begin{equation*}
\leftB
\begin{array}{rr|r}
0 & -1 & 0 \\
0 & 0 & 0
\end{array}
\rightB 
\rightarrow \cdots \rightarrow
\leftB
\begin{array}{rr|r}
0 & -1 & 0 \\
0 & 0 & 0
\end{array}
\rightB 
\end{equation*}

Then the eigenvectors are of the form
\begin{equation*}
t\leftB
\begin{array}{r}
1 \\
0
\end{array}
\rightB
\end{equation*}
and the basic eigenvector is 
\begin{equation*}
X_1
=
\leftB
\begin{array}{r}
1 \\
0
\end{array}
\rightB
\end{equation*}

In this case, the matrix $A$ has one eigenvalue of multiplicity two, 
but only one basic eigenvector. In order to diagonalize $A$, we need to construct
an invertible $2\times 2$ matrix $P$. However, because $A$ only has one basic eigenvector,
we cannot construct this $P$. Notice that if we were to use $X_1$ as both columns of $P$, $P$ would not be invertible. For this reason, we cannot repeat eigenvectors in $P$.

Hence this matrix cannot be diagonalized. 
\end{solution}

The idea that a matrix may not be diagonalizable suggests that conditions exist to determine when it is possible to diagonalize a matrix. We saw earlier in Corollary \ref{cor:distincteigenvalues} that an $n \times n$ matrix with $n$ distinct eigenvalues is diagonalizable. It turns out that there are other useful diagonalizability tests. 

First we need the following definition.

\begin{definition}{Eigenspace}{eigenspace}
Let $A$ be an $n\times n$ matrix and $\lambda\in\mathbb{R}$.
The eigenspace of $A$ corresponding to $\lambda$, written $E_{\lambda}(A)$
is the set of all eigenvectors corresponding to $\lambda$. 
\end{definition}

In other words, the eigenspace $E_{\lambda}(A)$ is all $X$ such that $AX = \lambda X$. Notice that this set can be written $E_{\lambda}(A) = \func{null}(\lambda I - A)$, showing that $E_{\lambda}(A)$ is a subspace of $\mathbb{R}^n$. 

Recall that the multiplicity of an eigenvalue $\lambda$ is the number of times that it occurs as a root of the characteristic polynomial.

Consider now the following lemma.

\begin{lemma}{Dimension of the Eigenspace}{dimensioneigenspace}
If $A$ is an $n\times n$ matrix,  then 
\[ \dim(E_{\lambda}(A))\leq m\]
where $\lambda$ is an
eigenvalue of $A$ of multiplicity $m$.
\end{lemma}

This result tells us that if $\lambda$ is an eigenvalue of $A$, then
the number of linearly independent $\lambda$-eigenvectors
is never more than the multiplicity of $\lambda$. We now use this fact to provide a useful diagonalizability condition.

\begin{theorem}{Diagonalizability Condition}{diagonalizability}
Let $A$ be an $n \times n$ matrix $A$. Then $A$ is diagonalizable if and only if for each eigenvalue $\lambda$ of $A$, $\dim(E_{\lambda}(A))$ is equal to the multiplicity of $\lambda$.
\end{theorem}

%%Consider an example utilizing this condition.
%%
%%\medskip
%%
%%\begin{example}{Diagonalizability Condition}{}
%%If possible, diagonalize the matrix
%%$A=\leftB\begin{array}{rrr}
%%\end{array}\rightB$.
%%Otherwise, explain why $A$ is not diagonalizable.
%%\end{example}
%%
%%\medskip
%%
%%\begin{solution}
%%
%%\end{solution}
%%
%%\medskip
