\section{Isomorphisms}

\begin{outcome}
\begin{enumerate}
\item[A.]  Determine if a linear transformation is an isomorphism.

\item[B.]  Determine if two subspaces of $\mathbb{R}^n$ are isomorphic. 
\end{enumerate}
\end{outcome}

Recall the definition of a linear transformation. Let $V$ and $W$ be two subspaces of $\mathbb{R}^{n}$ and $\mathbb{R}^{m}$
respectively. A mapping $T:V\rightarrow W$ is called a\textbf{\ linear
transformation} or \textbf{linear map} if it preserves the algebraic
operations of addition and
\index{linear map}
\index{linear transformation} scalar multiplication. Specifically, if $a,b$
are scalars and $
\vect{x},\vect{y}$ are vectors, 
\begin{equation*}
T\left( a\vect{x}+b\vect{y}\right) =aT(\vect{x})+bT(\vect{y})
\end{equation*}

Consider the following important definition.

\begin{definition}{Isomorphism}{isomorphism}
A linear map $T$ is called an \textbf{isomorphism} 
\index{isomorphism}if the following two conditions are satisfied.

\begin{itemize}
\item $T$ is one to one. That is, if $T(\vect{x})=T(\vect{y}),$ then $\vect{x}=\vect{y}.$

\item $T$ is onto. That is, if $\vect{w}\in W,$ there exists $
\vect{v}\in V$ such that $T(\vect{v})=\vect{w}$.
\end{itemize}

Two such subspaces which have an isomorphism as described above are said to
be \textbf{isomorphic.}
\index{isomorphic}
\end{definition}

Consider the following example of an isomorphism.

\begin{example}{Isomorphism}{isomorphism}
Let $T: \mathbb{R}^2 \mapsto \mathbb{R}^2$ be defined by 
\[
T \leftB \begin{array}{c}
x \\
y
\end{array} \rightB = \leftB \begin{array}{c}
x + y \\
x - y 
\end{array} \rightB
\]
Show that $T$ is an isomorphism.
\end{example}

\begin{solution}
To prove that $T$ is an isomorphism we must show
\begin{enumerate}
\item $T$ is a linear transformation;
\item $T$ is one to one;
\item $T$ is onto.
\end{enumerate}

We proceed as follows.

\begin{enumerate}
\item $T$ is a linear transformation:

Let $k, p$ be scalars. 
\begin{eqnarray*}
T \left( k \leftB \begin{array}{c}
x_1 \\
y_1
\end{array} \rightB + p \leftB \begin{array}{c}
x_2 \\
y_2
\end{array} \rightB \right) &=& 
T \left(  \leftB \begin{array}{c}
kx_1 \\
ky_1
\end{array} \rightB + \leftB \begin{array}{c}
px_2 \\
py_2
\end{array} \rightB \right) \\
&=&
T \left(  \leftB \begin{array}{c}
kx_1 + px_2 \\
ky_1 + py_2
\end{array} \rightB  \right) \\
&=& 
\leftB \begin{array}{c}
(kx_1 + px_2) + (ky_1 + py_2) \\
(kx_1 + px_2) - (ky_1 + py_2)
\end{array} \rightB \\
&=& 
\leftB \begin{array}{c}
(kx_1 + ky_1) + (px_2 + py_2) \\
(kx_1  - ky_1) + (px_2 - py_2)
\end{array} \rightB \\
&=&
\leftB \begin{array}{c}
kx_1 + ky_1  \\
kx_1  - ky_1
\end{array} \rightB + 
\leftB \begin{array}{c}
px_2 + py_2 \\
px_2 - py_2
\end{array} \rightB \\
&=& k \leftB \begin{array}{c}
x_1 + y_1  \\
x_1  - y_1
\end{array} \rightB + 
p \leftB \begin{array}{c}
x_2 + y_2 \\
x_2 - y_2
\end{array} \rightB \\
&=& 
k T \left(  \leftB \begin{array}{c}
x_1 \\
y_1
\end{array} \rightB \right) + p T \left( \leftB \begin{array}{c}
x_2 \\
y_2
\end{array} \rightB \right)
\end{eqnarray*}

Therefore $T$ is linear. 

\item $T$ is one to one:

We need to show that if $T (\vect{x}) = \vect{0}$ for a vector $\vect{x} \in \mathbb{R}^2$, then it follows that $\vect{x} = \vect{0}$.  Let $\vect{x} = \leftB \begin{array}{c}
x \\
y
\end{array} \rightB$. 

\[
T  \left( \leftB \begin{array}{c}
x \\
y
\end{array} \rightB \right) = \leftB \begin{array}{c}
x + y\\
x - y 
\end{array} \rightB = \leftB \begin{array}{c}
0 \\
0
\end{array} \rightB
\]
This provides a system of equations given by 
\begin{eqnarray*}
x + y &=& 0\\
x - y &=& 0
\end{eqnarray*}
You can verify that the solution to this system if $x = y =0$. Therefore 
\[
\vect{x} = \leftB \begin{array}{c}
x \\
y
\end{array} \rightB
 = \leftB \begin{array}{c}
0 \\
0
\end{array} \rightB
\]
and $T$ is one to one.

\item $T$ is onto:

Let $a,b$ be scalars. We want to check if there is always a solution to 
\[
T  \left( \leftB \begin{array}{c}
x \\
y
\end{array} \rightB \right) = \leftB \begin{array}{c}
x + y\\
x - y 
\end{array} \rightB = \leftB \begin{array}{c}
a \\
b
\end{array} \rightB
\]

This can be represented as the system of equations 
\begin{eqnarray*}
x + y &=& a\\
x - y &=& b
\end{eqnarray*}

Setting up the augmented matrix and row reducing gives
\[
\leftB \begin{array}{cc|c}
1 & 1 & a \\
1 & -1 & b
\end{array} \rightB \rightarrow \cdots \rightarrow
\leftB \begin{array}{cc|c}
1 & 0 & \vspace{0.05in}\frac{a+b}{2} \\
0 & 1 & \vspace{0.05in}\frac{a-b}{2}
\end{array} \rightB
\]
This has a solution for all $a,b$ and therefore $T$ is onto. 
\end{enumerate}

Therefore $T$ is an isomorphism.
\end{solution}

An important property of isomorphisms is that its inverse is also an isomorphism. 

\begin{proposition}{Inverse of an Isomorphism}{inverseisomorphism}
Let $T:V\rightarrow W$ be an isomorphism and $V,W$ be subspaces of $\mathbb{R}^n$. Then $T^{-1}:W\rightarrow V$ is
also an isomorphism.
\index{isomorphism!inverse}
\end{proposition}

\begin{proof} Let $T$ be an isomorphism.  Since $T$ is onto, a typical
vector in $W$ is of the form $T(\vect{v})$ where $\vect{v} \in V$. Consider then for $a,b$
scalars, 
\begin{equation*}
T^{-1}\left( aT(\vect{v}_{1})+bT(\vect{v}_{2})\right)
\end{equation*}
where $\vect{v}_{1}, \vect{v}_2 \in V$. Is this equal to 
\begin{equation*}
aT^{-1}\left( T (\vect{v}_{1})\right) +bT^{-1}\left( T(\vect{v}_{2})\right) =a\vect{v}_{1}+b\vect{v}_{2}?
\end{equation*}
Since $T$ is one to one, this will be so if 
\begin{equation*}
T\left( a\vect{v}_{1}+b\vect{v}_{2}\right) =T\left( T^{-1}\left( aT(\vect{v}_{1})+bT(\vect{v}_{2})\right)
\right) =aT(\vect{v}_{1})+bT(\vect{v}_{2}).
\end{equation*}
However, the above statement is just the condition that $T$ is a linear map.
Thus $T^{-1}$ is indeed a linear map. If $\vect{v} \in V$ is given, then $\vect{v}=T^{-1}\left( T(\vect{v})\right) $ and so $T^{-1}$ is onto. If $T^{-1} (\vect{v})=0,$ then 
\begin{equation*}
\vect{v}=T\left( T^{-1}(\vect{v})\right) =T(\vect{0})=\vect{0}
\end{equation*}
and so $T^{-1}$ is one to one.
\end{proof}

Another important result is that the composition of multiple isomorphisms is also an isomorphism.

\begin{proposition}{Composition of Isomorphisms}{compositionisomorphisms}
Let $T:V\rightarrow W$ and  $S:W\rightarrow Z$ be isomorphisms where $V,W,Z$ are subspaces of $\mathbb{R}^n$. Then $S\circ
T $ defined by $\left( S\circ T\right) \left( \vect{v} \right) = S\left(
T\left( \vect{v} \right) \right) $ is also an isomorphism.
\index{isomorphism!composition}
\end{proposition}

\begin{proof}
Suppose $T:V\rightarrow W$ and  $S:W\rightarrow Z$ are isomorphisms. Why is $S\circ T$ a linear map?
For $a,b$ scalars,
\begin{eqnarray*}
S\circ T\left( a\vect{v}_{1}+b(\vect{v}_{2})\right) 
&=& S\left( T\left(a\vect{v}_{1}+b\vect{v}_{2}\right) \right) =S\left( aT\vect{v}_{1}+bT\vect{v}_{2}\right) \\
&=&aS\left( T\vect{v}_{1}\right) +bS\left( T\vect{v}_{2}\right) = a\left( S\circ
T\right) \left( \vect{v}_{1}\right) +b\left( S\circ T\right) \left( \vect{v}_{2}\right)
\end{eqnarray*}
Hence $S\circ T$ is a linear map. If $\left( S\circ T\right) \left( \vect{v} \right)
=0,$ then $S\left( T\left(  \vect{v} \right) \right) =0$ and it follows that $T(\vect{v})=\vect{0}$ and hence by this lemma again, $\vect{v}=\vect{0}$. Thus $S\circ
T $ is one to one. It remains to verify that it is onto. Let $\vect{z} \in Z$. Then
since $S$ is onto, there exists $\vect{w} \in W$ such that $S(\vect{w})=\vect{z}.$ Also, since $T$
is onto, there exists $\vect{v}\in V$ such that $T(\vect{v})=\vect{w}.$ It follows that $S\left(
T\left( \vect{v}\right) \right) =\vect{z}$ and so $S\circ T$ is also onto.
\end{proof}

Consider two subspaces $V$ and $W$, and suppose there exists an isomorphism mapping one to the other. In this way the two subspaces are related, which we can write as $V \sim W$. Then the previous two propositions together claim that $\sim $ is an equivalence relation. That is: $\sim $
satisfies the following conditions:
\index{isomorphic!equivalence relation}

\begin{itemize}
\item $V\sim V$

\item If $V\sim W,$ it follows that $W\sim V$

\item If $V\sim W$ and $W\sim Z,$ then $V\sim Z$
\end{itemize}

We leave the verification of these conditions as an exercise.

Consider the following example. 

\begin{example}{Matrix Isomorphism}{matrixisomorphism}
Let $T:\mathbb{R}^{n}\rightarrow \mathbb{R}^{n}$ be defined by $T(\vect{x}) = A(\vect{x})$ where $A$ is an invertible $n\times n$ matrix. Then $T$ is
an isomorphism.
\end{example}

\begin{solution}
The reason for this is that, since $A$ is invertible, the only vector it
sends to $\vect{0}$ is the zero vector. Hence if $A(\vect{x})=A(\vect{y}),$ then $A\left( \vect{x}-\vect{y}\right) =\vect{0}$ and so $\vect{x}=\vect{y}$. It is onto
because if $\vect{y}\in \mathbb{R}^{n},A\left( A^{-1} (\vect{y})\right) =\left(
AA^{-1}\right) (\vect{y})$ $=\vect{y}$. 
\end{solution}

In fact, all isomorphisms from $\mathbb{R}^{n}$ to $\mathbb{R}^{n}$ can be expressed as $T(\vect{x}) = A(\vect{x})$ where $A$ is an invertible $n \times n$ matrix. One
simply considers the matrix whose $i^{th}$ column is $T\vect{e}_{i}$.

Recall that a basis of a subspace $V$ is a set of linearly independent vectors which span $V$. The following fundamental lemma describes the relation between bases and
isomorphisms.

\begin{lemma}{Mapping Bases}{mappingbases}
Let $T:V\rightarrow W$ be a linear transformation where $V,W$ are
subspaces of $\mathbb{R}^n$. If $T$ is one to one, then it has the property that if $\left\{ \vect{u}_{1},\cdots ,\vect{u}_{k}\right\} $ is linearly independent, so is $\left\{ T(\vect{u}_{1}),\cdots ,T(\vect{u}_{k})\right\} $.

More generally, $T$ is an isomorphism if and only if whenever $\left\{ 
\vect{v}_{1},\cdots ,\vect{v}_{n}\right\} $ is a basis for $V,$ it follows that $\left\{ T
(\vect{v}_{1}),\cdots ,T(\vect{v}_{n})\right\} $ is a basis for $W$. 
\index{isomorphism!bases}
\index{one to one!linear independence}
\end{lemma}

\begin{proof}First suppose that $T$ is a linear transformation and is one to one
and $\left\{ \vect{u}_{1},\cdots ,\vect{u}_{k}\right\} $ is linearly
independent. It is required to show that $\left\{ T(\vect{u}_{1}),\cdots ,T(\vect{
u}_{k})\right\} $ is also linearly independent. Suppose then that 
\begin{equation*}
\sum_{i=1}^{k}c_{i}T(\vect{u}_{i})=\vect{0}
\end{equation*}
Then, since $T$ is linear, 
\begin{equation*}
T\left( \sum_{i=1}^{n}c_{i}\vect{u}_{i}\right) =\vect{0}
\end{equation*}
Since $T$ is one to one, it follows that 
\begin{equation*}
\sum_{i=1}^{n}c_{i}\vect{u}_{i}=0
\end{equation*}
Now the fact that $\left\{ \vect{u}_{1},\cdots ,\vect{u}_{n}\right\} $ is
linearly independent implies that each $c_{i}=0$. Hence $\left\{ T(\vect{u}
_{1}),\cdots ,T(\vect{u}_{n})\right\} $ is linearly independent.

Now suppose that $T$ is an isomorphism and $\left\{ \vect{v}_{1},\cdots ,\vect{
v}_{n}\right\} $ is a basis for $V$. It was just shown that $\left\{ T(\vect{v}
_{1}),\cdots ,T(\vect{v}_{n})\right\} $ is linearly independent. It remains to
verify that span$\left\{ T(\vect{v}_{1}),\cdots ,T(\vect{v}_{n})\right\}=W$. If $\vect{w}\in W,$ then since $T$ is onto there
exists $\vect{v}\in V$ such that $T(\vect{v})=\vect{w}$. Since $\left\{ \vect{v}
_{1},\cdots ,\vect{v}_{n}\right\} $ is a basis, it follows that there exists
scalars $\left\{ c_{i}\right\} _{i=1}^{n}$ such that 
\begin{equation*}
\sum_{i=1}^{n}c_{i}\vect{v}_{i}=\vect{v}.
\end{equation*}
Hence, 
\begin{equation*}
\vect{w}=T(\vect{v})=T\left( \sum_{i=1}^{n}c_{i}\vect{v}_{i}\right)
=\sum_{i=1}^{n}c_{i}T(\vect{v}_{i})
\end{equation*}
It follows that span$\left\{ T(\vect{v}_{1}),\cdots , T(\vect{v}_{n})\right\} =W$ showing that this set of vectors is a
basis for $W$.

Next suppose that $T$ is a linear transformation which takes a basis to a basis. This means that if $\left\{ \vect{v}_{1},\cdots ,\vect{v}_{n}\right\} $ is a basis for $V,$ it
follows $\left\{ T(\vect{v}_{1}),\cdots ,T(\vect{v}_{n})\right\} $ is a basis for $
W.$ Then if $w\in W,$ there exist scalars $c_{i}$ such that $
w=\sum_{i=1}^{n}c_{i}T(\vect{v}_{i})=T\left( \sum_{i=1}^{n}c_{i}\vect{v}_{i}\right) $
showing that $T$ is onto. If $T\left( \sum_{i=1}^{n}c_{i}\vect{v}_{i}\right) =\vect{0}$
then $\sum_{i=1}^{n}c_{i}T(\vect{v}_{i})=\vect{0}$ and since the vectors $\left\{ T(\vect{v}
_{1}),\cdots ,T(\vect{v}_{n})\right\} $ are linearly independent, it follows
that each $c_{i}=0.$ Since $\sum_{i=1}^{n}c_{i}\vect{v}_{i}$ is a typical vector in 
$V$, this has shown that if $T(\vect{v})=\vect{0}$ then $\vect{v}=\vect{0}$ and so $T$ is also one to one.
Thus $T$ is an isomorphism. 
\end{proof}

The following theorem illustrates a very useful idea for defining an
isomorphism. Basically, if you know what it does to a basis, then you can
construct the isomorphism.

\begin{theorem}{Isomorphic Subspaces}{isomorphicsubspaces}
Suppose $V$ and $W$ are two subspaces of $\mathbb{R}^n$. Then the two
subspaces are isomorphic if and only if they have the same dimension. In the
case that the two subspaces have the same dimension, then for
\index{isomorphism!equivalence} a linear map $T:V\rightarrow W$, the
following are equivalent.

\begin{enumerate}
\item $T$ is one to one.

\item $T$ is onto.

\item $T$ is an isomorphism.
\end{enumerate}
\end{theorem}

\begin{proof} Suppose first that these two subspaces have the same
dimension. Let a basis for $V$ be $\left\{ 
\vect{v}_{1},\cdots ,\vect{v}_{n}\right\} $ and let a basis for $W$ be $
\left\{ \vect{w}_{1},\cdots ,\vect{w}_{n}\right\} $. Now define $T$ as
follows. 
\begin{equation*}
T(\vect{v}_{i})=\vect{w}_{i}
\end{equation*}
for $\sum_{i=1}^{n}c_{i}\vect{v}_{i}$ an arbitrary vector of $V,$
\begin{equation*}
T\left( \sum_{i=1}^{n}c_{i}\vect{v}_{i}\right) = \sum_{i=1}^{n}c_{i}T
\vect{v}_{i}=\sum_{i=1}^{n}c_{i}\vect{w}_{i}.
\end{equation*}
It is necessary to verify that this is well defined. Suppose then that 
\begin{equation*}
\sum_{i=1}^{n}c_{i}\vect{v}_{i}=\sum_{i=1}^{n}\hat{c}_{i}\vect{v}_{i}
\end{equation*}
Then 
\begin{equation*}
\sum_{i=1}^{n}\left( c_{i}-\hat{c}_{i}\right) \vect{v}_{i}=\vect{0}
\end{equation*}
and since $\left\{ \vect{v}_{1},\cdots ,\vect{v}_{n}\right\} $ is a basis, $
c_{i}=\hat{c}_{i}$ for each $i$. Hence 
\begin{equation*}
\sum_{i=1}^{n}c_{i}\vect{w}_{i}=\sum_{i=1}^{n}\hat{c}_{i}\vect{w}_{i}
\end{equation*}
and so the mapping is well defined. Also if $a,b$ are scalars, 
\begin{eqnarray*}
T\left( a\sum_{i=1}^{n}c_{i}\vect{v}_{i}+b\sum_{i=1}^{n}\hat{c}_{i}\vect{v}%
_{i}\right) &=&T\left( \sum_{i=1}^{n}\left( ac_{i}+b\hat{c}_{i}\right) \vect{v%
}_{i}\right) =\sum_{i=1}^{n}\left( ac_{i}+b\hat{c}_{i}\right) \vect{w}_{i} \\
&=&a\sum_{i=1}^{n}c_{i}\vect{w}_{i}+b\sum_{i=1}^{n}\hat{c}_{i}\vect{w}_{i} \\
&=&aT\left( \sum_{i=1}^{n}c_{i}\vect{v}_{i}\right) +bT\left( \sum_{i=1}^{n}%
\hat{c}_{i}\vect{v}_{i}\right)
\end{eqnarray*}
Thus $T$ is a linear transformation. 

Now if 
\begin{equation*}
T\left( \sum_{i=1}^{n}c_{i}\vect{v}_{i}\right) =\sum_{i=1}^{n}c_{i}\vect{w}%
_{i}=\vect{0},
\end{equation*}
then since the $\left\{ \vect{w}_{1},\cdots ,\vect{w}_{n}\right\} $ are
independent, each $c_{i}=0$ and so $\sum_{i=1}^{n}c_{i}\vect{v}_{i}=\vect{0}$
also. Hence $T$ is one to one. If $\sum_{i=1}^{n}c_{i}\vect{w}_{i}$ is a
vector in $W,$ then it equals 
\begin{equation*}
\sum_{i=1}^{n}c_{i}T(\vect{v}_{i})=T\left( \sum_{i=1}^{n}c_{i}\vect{v}_{i}\right)
\end{equation*}
showing that $T$ is also onto. Hence $T$ is an isomorphism and so $V$ and $W$
are isomorphic.

Next suppose $T:V \mapsto W$ is an isomorphism, so these two subspaces are isomorphic. Then for $\left\{ \vect{v}_{1},\cdots ,\vect{v}_{n}\right\} $ a
basis for $V$, it follows that a basis for $W$
is $\left\{ T(\vect{v}_{1}),\cdots ,T(\vect{v}_{n})\right\} $ showing that the two
subspaces have the same dimension.

Now suppose the two subspaces have the same dimension. Consider the three
claimed equivalences.

First consider the claim that $1.)\Rightarrow 2.).$ If $T$ is one to one and if $\left\{ \vect{v}_{1},\cdots ,\vect{v}
_{n}\right\} $ is a basis for $V,$ then $\left\{ T(\vect{v}_{1}),\cdots ,T(\vect{v
}_{n})\right\} $ is linearly independent. If it is not a basis, then it must
fail to span $W$. But then there would exist $\vect{w}\notin \func{span}
\left\{ T(\vect{v}_{1}),\cdots ,T(\vect{v}_{n})\right\} $ and it follows that $\left\{ T(\vect{v}_{1}),\cdots ,T(\vect{v}_{n}),\vect{w}
\right\} $ would be linearly independent which is impossible because there exists a basis for $W$ of $n$ vectors.

Hence $\func{span}\left\{ T(\vect{v}_{1}),\cdots ,T(\vect{v}_{n})\right\} =W$ and
so $\left\{ T(\vect{v}_{1}),\cdots ,T(\vect{v}_{n})\right\} $ is a basis. If $\vect{w}\in W,$ there exist scalars $c_{i}$ such that 
\begin{equation*}
\vect{w}=\sum_{i=1}^{n}c_{i}T(\vect{v}_{i})=T\left( \sum_{i=1}^{n}c_{i}\vect{v}
_{i}\right)
\end{equation*}
showing that $T$ is onto. This shows that $1.)\Rightarrow 2.).$

Next consider the claim that $2.)\Rightarrow 3.).$ Since $2.)$ holds, it
follows that $T$ is onto. It remains to verify that $T$ is one to one. Since 
$T$ is onto, there exists a basis of the form $\left\{ T(\vect{v}_{i}),\cdots ,T(\vect{v}_{n})\right\} .$ Then it follows that $\left\{ \vect{v}_{1},\cdots ,
\vect{v}_{n}\right\} $ is linearly independent. Suppose 
\begin{equation*}
\sum_{i=1}^{n}c_{i}\vect{v}_{i}=\vect{0}
\end{equation*}
Then 
\begin{equation*}
\sum_{i=1}^{n}c_{i}T(\vect{v}_{i})=\vect{0}
\end{equation*}
Hence each $c_{i}=0$ and so, $\left\{ \vect{v}_{1},\cdots ,\vect{v}
_{n}\right\} $ is a basis for $V$. Now it follows that a typical vector in $
V $ is of the form $\sum_{i=1}^{n}c_{i}\vect{v}_{i}$. If $T\left(
\sum_{i=1}^{n}c_{i}\vect{v}_{i}\right) =\vect{0},$ it follows that 
\begin{equation*}
\sum_{i=1}^{n}c_{i}T(\vect{v}_{i})=\vect{0}
\end{equation*}
and so, since $\left\{ T(\vect{v}_{i}),\cdots ,T(\vect{v}_{n})\right\} $ is
independent, it follows each $c_{i}=0$ and hence $\sum_{i=1}^{n}c_{i}\vect{v}
_{i}=\vect{0}$. Thus $T$ is one to one as well as onto and so it is an
isomorphism.

If $T$ is an isomorphism, it is both one to one and onto by definition so $
3.)$ implies both $1.)$ and $2.)$.
\end{proof}

Note the interesting way of defining a linear transformation in the first
part of the argument by describing what it does to a basis and then
``extending it linearly'' to the entire subspace.
\index{linear map!defining on a basis}

\begin{example}{Isomorphic Subspaces}{}
Let $V=\mathbb{R}^{3}$ and let $W$ denote 
\begin{equation*}
\func{span}\left\{ \leftB 
\begin{array}{r}
1 \\ 
2 \\ 
1 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{r}
0 \\ 
1 \\ 
0 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{r}
1 \\ 
1 \\ 
2 \\ 
0
\end{array}
\rightB \right\}
\end{equation*}
Show that $V$ and $W$ are isomorphic. 
\end{example}

\begin{solution}
First observe that these subspaces are both of dimension 3 and so they are isomorphic by Theorem \ref{thm:isomorphicsubspaces}. The
three vectors which span $W$ are easily seen to be linearly independent by
making them the columns of a matrix and row reducing to the \rref.

You can exhibit an isomorphism of these two spaces as follows. 
\begin{equation*}
T(\vect{e}_{1})=\leftB 
\begin{array}{c}
1 \\ 
2 \\ 
1 \\ 
1
\end{array}
\rightB, T(\vect{e}_{2})=\leftB 
\begin{array}{c}
0 \\ 
1 \\ 
0 \\ 
1
\end{array}
\rightB, T(\vect{e}_{3})=\leftB 
\begin{array}{c}
1 \\ 
1 \\ 
2 \\ 
0
\end{array}
\rightB
\end{equation*}
and extend linearly. Recall that the matrix of this linear transformation is
just the matrix having these vectors as columns. Thus the matrix of this
isomorphism is 
\begin{equation*}
\leftB 
\begin{array}{rrr}
1 & 0 & 1 \\ 
2 & 1 & 1 \\ 
1 & 0 & 2 \\ 
1 & 1 & 0
\end{array}
\rightB
\end{equation*}
You should check that multiplication on the left by this matrix does
reproduce the claimed effect resulting from an application by $T$.
\end{solution}

Consider the following example. 

\begin{example}{Finding the Matrix of an Isomorphism}{matrixofisomorphism}
Let $V=\mathbb{R}^{3}$ and let $W$ denote 
\begin{equation*}
\func{span}\left\{ \leftB 
\begin{array}{c}
1 \\ 
2 \\ 
1 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{c}
0 \\ 
1 \\ 
0 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{c}
1 \\ 
1 \\ 
2 \\ 
0
\end{array}
\rightB \right\}
\end{equation*}

Let $T: V \mapsto W$ be defined as follows. 
\begin{equation*}
T\leftB 
\begin{array}{c}
1 \\ 
1 \\ 
0
\end{array}
\rightB =\leftB 
\begin{array}{c}
1 \\ 
2 \\ 
1 \\ 
1
\end{array}
\rightB ,T\leftB 
\begin{array}{c}
0 \\ 
1 \\ 
1
\end{array}
\rightB =\leftB 
\begin{array}{c}
0 \\ 
1 \\ 
0 \\ 
1
\end{array}
\rightB ,T\leftB 
\begin{array}{c}
1 \\ 
1 \\ 
1
\end{array}
\rightB =\leftB 
\begin{array}{c}
1 \\ 
1 \\ 
2 \\ 
0
\end{array}
\rightB
\end{equation*}
Find the matrix of this isomorphism $T$.
\end{example}

\begin{solution}
 First note that the vectors 
\begin{equation*}
\leftB
\begin{array}{c}
1 \\ 
1 \\ 
0
\end{array}
\rightB ,\leftB 
\begin{array}{c}
0 \\ 
1 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{c}
1 \\ 
1 \\ 
1
\end{array}
\rightB
\end{equation*}
are indeed a basis for $\mathbb{R}^{3}$ as can be seen by making them the
columns of a matrix and using the \rref.

Now recall the matrix of $T$ is a $4\times 3$ matrix $A$ which gives the same
effect as $T.$ Thus, from the way we multiply matrices, 
\begin{equation*}
A\leftB
\begin{array}{rrr}
1 & 0 & 1 \\ 
1 & 1 & 1 \\ 
0 & 1 & 1
\end{array}
\rightB =\leftB 
\begin{array}{rrr}
1 & 0 & 1 \\ 
2 & 1 & 1 \\ 
1 & 0 & 2 \\ 
1 & 1 & 0
\end{array}
\rightB
\end{equation*}
Hence, 
\begin{equation*}
A=\leftB 
\begin{array}{rrr}
1 & 0 & 1 \\ 
2 & 1 & 1 \\ 
1 & 0 & 2 \\ 
1 & 1 & 0
\end{array}
\rightB \leftB 
\begin{array}{rrr}
1 & 0 & 1 \\ 
1 & 1 & 1 \\ 
0 & 1 & 1
\end{array}
\rightB ^{-1}=\leftB 
\begin{array}{rrr}
1 & 0 & 0 \\ 
0 & 2 & -1 \\ 
2 & -1 & 1 \\ 
-1 & 2 & -1
\end{array}
\rightB
\end{equation*}
Note how the span of the columns of this new matrix must be the same as the
span of the vectors defining $W$.
\end{solution}

This idea of defining a linear transformation by what it does on a basis
works for linear maps which are not necessarily isomorphisms.

\begin{example}{Finding the Matrix of an Isomorphism}{}
Let $V=\mathbb{R}^{3}$ and let $W$ denote 
\begin{equation*}
\func{span}\left\{ \leftB
\begin{array}{c}
1 \\ 
0 \\ 
1 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{c}
0 \\ 
1 \\ 
0 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{c}
1 \\ 
1 \\ 
1 \\ 
2
\end{array}
\rightB \right\}
\end{equation*}
Let $T: V \mapsto W$ be defined as follows. 
\begin{equation*}
T\leftB 
\begin{array}{c}
1 \\ 
1 \\ 
0
\end{array}
\rightB = \leftB 
\begin{array}{c}
1 \\ 
0 \\ 
1 \\ 
1
\end{array}
\rightB ,T\leftB 
\begin{array}{c}
0 \\ 
1 \\ 
1
\end{array}
\rightB =\leftB 
\begin{array}{c}
0 \\ 
1 \\ 
0 \\ 
1
\end{array}
\rightB ,T\leftB 
\begin{array}{c}
1 \\ 
1 \\ 
1
\end{array}
\rightB =\leftB 
\begin{array}{c}
1 \\ 
1 \\ 
1 \\ 
2
\end{array}
\rightB
\end{equation*}
 Find the matrix of this linear transformation.
\end{example}

\begin{solution}
Note that in this case, the three vectors which span $W$ are not linearly independent. Nevertheless the above procedure will still work.
The reasoning is the same as before. If $A$ is this matrix, then 
\begin{equation*}
A\leftB
\begin{array}{rrr}
1 & 0 & 1 \\ 
1 & 1 & 1 \\ 
0 & 1 & 1
\end{array}
\rightB =\leftB 
\begin{array}{rrr}
1 & 0 & 1 \\ 
0 & 1 & 1 \\ 
1 & 0 & 1 \\ 
1 & 1 & 2
\end{array}
\rightB
\end{equation*}
and so 
\begin{equation*}
A=\leftB
\begin{array}{rrr}
1 & 0 & 1 \\ 
0 & 1 & 1 \\ 
1 & 0 & 1 \\ 
1 & 1 & 2
\end{array}
\rightB \leftB 
\begin{array}{rrr}
1 & 0 & 1 \\ 
1 & 1 & 1 \\ 
0 & 1 & 1
\end{array}
\rightB ^{-1}=\leftB 
\begin{array}{rrr}
1 & 0 & 0 \\ 
0 & 0 & 1 \\ 
1 & 0 & 0 \\ 
1 & 0 & 1
\end{array}
\rightB
\end{equation*}
The columns of this last matrix are obviously not linearly independent.
\end{solution}