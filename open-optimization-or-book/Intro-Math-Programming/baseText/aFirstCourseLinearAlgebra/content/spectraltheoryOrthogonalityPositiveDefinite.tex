\subsection{Positive Definite Matrices}

Positive definite matrices are often encountered in applications such mechanics and statistics.

We begin with a definition.

\begin{definition}{Positive Definite Matrix}{positivedefinitematrix}
Let $A$ be an $n \times n$ symmetric matrix. Then $A$ is positive definite if all of its eigenvalues are positive.
\index{positive definite}
\end{definition}

The relationship between a negative definite matrix and positive definite matrix is as follows. 

\begin{lemma}{Negative Definite Matrix}{negativedefinitematrix}
An $n\times n$ matrix $A$ is negative definite if and only if $-A$ is
positive definite.
\end{lemma}

%%\begin{proof}
%%Suppose $A$ is negative definite. For all $\vect{x},$ 
%%\begin{equation*}
%%\vect{x}^{T}A\vect{x}\leq -\delta \vectlength \vect{x} \vectlength ^{2}
%\end{equation*}
%%Then multiply both sides by $-1$ to find that $-A$ is positive definite.
%%Next suppose for all $\vect{x},$
%%\begin{equation*}
%%\vect{x}^{T}\left( -A\right) \vect{x}\geq \delta \vectlength \vect{x}\vectlength
%%^{2}.
%%\end{equation*}
%%Then multiply by $-1$ to see that $A$ is negative definite.
%%\end{proof}
%%
%%\medskip

%%There is an equivalent characterization of a positive definite matrix in
%%terms of eigenvalues.
%%
%%\medskip
%%
%%\begin{theorem}{Positive Definite Matrix and Eigenvalues}{posdefeigen}  %%\label{3oct3f}
%%A symmetric matrix $A$ is positive definite if and only if all
%%of its eigenvalues are positive. (Recall that all of the eigenvalues of a symmetric matrix are real.)
%%\index{positive definite!positive eigenvalues}
%%\end{theorem}
%%
%%\medskip
%%
%%\begin{proof}Suppose first that $A$ is positive definite. Let $\lambda $
%%is an eigenvalue with eigenvector $
%%\vect{u}.$ Then, since $\lambda $ is real, 
%%\begin{equation*}
%%\vect{u}^{T}A\vect{u}=\lambda \vect{u}^{T}\vect{u}=\lambda \vectlength
%%u\vectlength ^{2}\geq \delta \vectlength \vect{u}\vectlength ^{2}
%%\end{equation*}
%%Hence $0<\delta \leq \lambda ,$ this for any eigenvalue. Hence all
%%eigenvalues are positive.
%%
%%Next suppose that all eigenvalues are positive. Then letting $\delta $ be
%%the smallest eigenvalue and $U$ a real orthogonal matrix such that $
%%U^{T}AU=D,$ a diagonal matrix having the eigenvalues of $A$ on the main
%%diagonal, 
%%\begin{equation*}
%%\vect{x}^{T}A\vect{x}=\vect{x}^{T}UDU^{T}\vect{x}=\left( U^{T}\vect{x}\right)
%%^{T}DU^{T}\vect{x}=\sum_{i=1}^{n}\left( U^{T}\vect{x}\right) _{i}^{2}\lambda
%%_{i}\geq \delta \sum_{i=1}^{n}\left( U^{T}\vect{x}\right) _{i}^{2}=\delta
%%\vectlength U^{T}\vect{x}\vectlength ^{2}
%%\end{equation*}
%%Now $\vectlength U^{T}\vect{x}\vectlength ^{2}=\left( U^{T}\vect{x}\right)
%%^{T}U^{T}\vect{x}=\vect{x}^{T}UU^{T}\vect{x}=\vect{x}^{T}\vect{x}=\vectlength \vect{x}\vectlength ^{2}$ and so 
%%\begin{equation*}
%%\vect{x}^{T}A\vect{x}\geq \delta \vectlength \vect{x}\vectlength ^{2}
%%\end{equation*}
%%and so $A$ is positive definite.
%%\end{proof}
%%
%%\medskip

Consider the following lemma.

\index{positive definite!invertible}
\begin{lemma}{Positive Definite Matrix and Invertibility}{posdefiniteinvertible}
If $A$ is positive definite, then it is invertible.
\end{lemma}

\begin{proof}
If $A\vect{v}=\vect{0},$ then $0$ is an eigenvalue if $\vect{v}$ is nonzero, which
does not happen for a positive definite matrix. Hence $\vect{v}=\vect{0}$ and
so $A$ is one to one. This is sufficient to conclude that it is invertible.
\end{proof}

Notice that this lemma implies that if a matrix $A$ is positive definite, then $\det(A) > 0$. 

The following theorem provides another characterization of positive definite matrices. It gives a useful test for verifying if a matrix is positive definite. 

\begin{theorem}{Positive Definite Matrix}{positivedefinite}
Let $A$ be a symmetric matrix. Then $A$ is positive definite if and
only if $\vect{x}^T A \vect{x} $ is positive for all nonzero $\vect{x}
\in \mathbb{R}^n$.
\end{theorem}

\begin{proof}
Since $A$ is symmetric, there exists an orthogonal matrix $U$ so that

\[ U^{T}AU=\func{diag}(\lambda_1,\lambda_2,\ldots,\lambda_n)=D,\]

where $\lambda_1,\lambda_2,\ldots,\lambda_n$ are the (not necessarily
distinct) eigenvalues of $A$.
Let $\vect{x}\in\mathbb{R}^n$, $\vect{x}\neq \vect{0}$, and define
$\vect{y}=U^T\vect{x}$.
Then
\[ \vect{x}^TA\vect{x}=\vect{x}^T(UDU^T)\vect{x}
= (\vect{x}^TU)D(U^T\vect{x})
=\vect{y}^TD\vect{y}.\]

Writing $\vect{y}^T=\leftB\begin{array}{cccc}
y_1 & y_2 & \cdots & y_n\end{array}\rightB$, 

\begin{eqnarray*}
\vect{x}^TA\vect{x} & = & 
\leftB\begin{array}{cccc} y_1 & y_2 & \cdots & y_n\end{array}\rightB
\func{diag}(\lambda_1,\lambda_2,\ldots,\lambda_n)
\leftB\begin{array}{c} y_1 \\ y_2 \\ \vdots \\ y_n\end{array}\rightB\\
& = & \lambda_1 y_1^2 + \lambda_2 y_2^2 + \cdots \lambda_n y_n^2.
\end{eqnarray*}

$(\Rightarrow)$ First we will assume that $A$ is positive definite and prove that $\vect{x}^T A \vect{x} $ is positive. 

Suppose $A$ is positive definite, and $\vect{x}\in\mathbb{R}^n$,
$\vect{x}\neq\vect{0}$.
Since $U^T$ is invertible, $\vect{y}=U^T\vect{x}\neq \vect{0}$,
and thus $y_j\neq 0$ for some $j$, implying $y_j^2>0$
for some $j$.
Furthermore, since all eigenvalues of $A$ are positive,
$\lambda_i y_i^2\geq 0$ for all $i$ and $\lambda_jy_j^2>0$.
Therefore, $\vect{x}^TA\vect{x}>0$.


$(\Leftarrow)$ Now we will assume $\vect{x}^T A \vect{x} $ is positive and show that $A$ is positive definite.

If $\vect{x}^TA\vect{x}>0$ whenever $\vect{x}\neq \vect{0}$,
choose $\vect{x}=U\vect{e}_j$, where $\vect{e}_j$ is the $j^{\mbox{th}}$
column of $I_n$.
Since $U$ is invertible, $\vect{x}\neq\vect{0}$, 
and thus
\[ \vect{y}=U^T\vect{x}=U^T(U\vect{e}_j) =\vect{e}_j.\]
Thus $y_j=1$ and $y_i=0$ when $i\neq j$, so
\[ \lambda_1 y_1^2 + \lambda_2 y_2^2 + \cdots \lambda_n y_n^2
=\lambda_j,\]
i.e., $\lambda_j=\vect{x}^TA\vect{x}>0$.
Therefore, $A$ is positive definite.
\end{proof}

There are some other very interesting consequences which result from a
matrix being positive definite. First one can note that the property of
being positive definite is transferred to each of the principal submatrices which we will now define.

\begin{definition}{The Submatrix $A_k$}{submatrixAk}
Let $A$ be an $n\times n$ matrix. Denote by $A_{k}$ the $k\times k$ matrix
obtained by deleting the $k+1,\cdots ,n$ columns and the $k+1,\cdots ,n$
rows from $A.$ Thus $A_{n}=A$ and $A_{k}$ is the $k\times k$ submatrix of $A$
which occupies the upper left corner of $A.$
\index{principal submatrices}
\end{definition}

\begin{lemma}{Positive Definite and Submatrices}{positivematrixsubmatrix}
Let $A$ be an $n\times n$ positive definite matrix.  Then each submatrix $A_{k}$ is also positive definite.
\end{lemma}

\begin{proof}
This follows right away from the above definition. Let $\vect{x}\in \mathbb{R}^{k}$ be nonzero. Then 
\begin{equation*}
\vect{x}^{T}A_{k}\vect{x}=\leftB 
\begin{array}{cc}
\vect{x}^{T} & 0
\end{array}
\rightB A\leftB 
\begin{array}{c}
\vect{x} \\ 
0
\end{array}
\rightB >  0
\end{equation*}
by the assumption that $A$ is positive definite.
\end{proof}

There is yet another way to recognize whether a matrix is positive definite
which is described in terms of these submatrices. We state the result,  the proof
of which can be found in more advanced texts.

\begin{theorem}{Positive Matrix and Determinant of $A_k$}{positivematrixdeterminantAk}   
Let $A$ be a symmetric matrix. Then $A$ is positive definite if
and only if $\det \left( A_{k}\right)$ is greater than $0$ for every submatrix $A_{k}$,  $k=1,\cdots ,n$. 
\index{principal submatrices!positive definite}
\index{positive definite!principal submatrices}
\end{theorem}

\begin{proof}
We prove this theorem by induction on $n.$ It is clearly
true if $n=1.$ Suppose then that it is true for $n-1$ where $n\geq 2$. Since 
$\det \left( A\right) =\det \left( A_{n}\right) >0,$ it follows that all the
eigenvalues are nonzero. We need to show that they are all positive. Suppose
not. Then there is some even number of them which are negative, even because
the product of all the eigenvalues is known to be positive, equaling $\det
\left( A\right) $. Pick two, $\lambda _{1}$ and $\lambda _{2}$ and let $A
\vect{u}_{i}=\lambda _{i}\vect{u}_{i}$ where $\vect{u}_{i}\neq \vect{0}$ for $
i=1,2$ and $\vect{u}_{1}\dotprod \vect{u}_{2}=0.$ Now if $\vect{y}\equiv \alpha
_{1}\vect{u}_{1}+\alpha _{2}\vect{u}_{2}$ is an element of $\func{span}\left\{ 
\vect{u}_{1},\vect{u}_{2}\right\} ,$ then since these are eigenvalues and $\ 
\vect{u}_{1}\dotprod \vect{u}_{2}=0,$ a short computation shows 
\begin{equation*}
\left( \alpha _{1}\vect{u}_{1}+\alpha _{2}\vect{u}_{2}\right) ^{T}A\left(
\alpha _{1}\vect{u}_{1}+\alpha _{2}\vect{u}_{2}\right)
\end{equation*}
\begin{equation*}
=\left\vert \alpha _{1}\right\vert ^{2}\lambda _{1}\vectlength \vect{u}
_{1}\vectlength ^{2}+\left\vert \alpha _{2}\right\vert ^{2}\lambda
_{2}\vectlength \vect{u}_{2}\vectlength ^{2}<0.
\end{equation*}
Now letting $\vect{x}\in \mathbb{R}^{n-1},$ we can use the induction
hypothesis to write 
\begin{equation*}
\leftB
\begin{array}{cc}
x^{T} & 0
\end{array}
\rightB A\leftB 
\begin{array}{c}
\vect{x} \\ 
0
\end{array}
\rightB =\vect{x}^{T}A_{n-1}\vect{x}>0.
\end{equation*}
Now the dimension of $\left\{ \vect{z}\in \mathbb{R}^{n}:z_{n}=0\right\} $ is 
$n-1$ and the dimension of $\func{span}\left\{ \vect{u}_{1},\vect{u}
_{2}\right\} =2$ and so there must be some nonzero $\vect{x}\in \mathbb{R}
^{n} $ which is in both of these subspaces of $\mathbb{R}^{n}$. However, the
first computation would require that $\vect{x}^{T}A\vect{x}<0$ while the
second would require that $\vect{x}^{T}A\vect{x}>0.$ This contradiction shows
that all the eigenvalues must be positive. This proves the if part of the
theorem. The converse can also be shown to be correct, but it is the
direction which was just shown which is of most interest. 
\end{proof}

\begin{corollary}{Symmetric and Negative Definite Matrix}{symmetricdefinitematrix} %\label{pd3} 
Let $A$ be symmetric. Then $A$ is negative definite if and only
if 
\begin{equation*}
\left( -1\right) ^{k} \det \left( A_{k}\right) >0
\end{equation*}
for every $k=1,\cdots ,n$.
\end{corollary}

\begin{proof}This is immediate from the above theorem when we notice,
that $A$ is negative definite if and only if $-A$ is positive definite.
Therefore, if $\det \left( -A_{k}\right) >0$ for all $k=1,\cdots ,n,$ it
follows that $A$ is negative definite. However, $\det \left( -A_{k}\right)
=\left( -1\right) ^{k}\det \left( A_{k}\right) .$ 
\end{proof}