\subsection{Subspaces and Basis}

The goal of this section is to develop an understanding of a subspace of $\mathbb{R}^n$. Before a precise definition is considered, we first examine the subspace test given below.

\begin{theorem}{Subspace Test}{subspacetest}
A subset $V$ of $\mathbb{R}^n$ is a subspace of $\mathbb{R}^n$ if 
\begin{enumerate}
\item the zero vector of $\mathbb{R}^n$, $\vect{0}_n$, is in $V$;
\item $V$ is closed under addition, i.e., for all $\vect{u},\vect{w}\in V$, $\vect{u}+\vect{w}\in V$;
\item $V$ is closed under scalar multiplication, i.e., for all $\vect{u}\in V$
and $k\in\mathbb{R}$, $k\vect{u}\in V$.
\end{enumerate}
\end{theorem}

This test allows us to determine if a given set is a subspace of $\mathbb{R}^n$. Notice that the subset $V = \left\{ \vect{0} \right\}$ is a subspace of $\mathbb{R}^n$ (called the zero subspace \index{zero subspace}), as is $\mathbb{R}^n$ itself. A subspace which is not the zero subspace of $\mathbb{R}^n$ is referred to as a proper subspace\index{proper subspace}.

 A subspace is simply a set of vectors with the property that linear
combinations of these vectors remain in the set. Geometrically in
$\mathbb{R}^{3}$, it turns out that a subspace can be represented by
either the origin as a single point, lines and planes which contain
the origin, or the entire space $\mathbb{R}^{3}$. 

Consider the following example of a line in $\mathbb{R}^3$. 

\begin{example}{Subspace of $\mathbb{R}^3$}{linesubspace}
In $\mathbb{R}^3$, the line $L$ through the origin that is
parallel to the vector
${\vect{d}}= \leftB \begin{array}{r} -5 \\ 1 \\ -4 \end{array}\rightB$ 
has (vector) equation
$\leftB \begin{array}{r} x \\ y \\ z \end{array}\rightB
=t\leftB \begin{array}{r} -5 \\ 1 \\ -4 \end{array}\rightB, t\in\mathbb{R}$,
%If ${\bm u}=\leftB \begin{array}{r} x \\ y \\ z \end{array}\rightB$, 
so 
\[ L=\left\{ t{\vect{d}} ~|~ t\in\mathbb{R}\right\}.\] Then $L$ is a subspace of $\mathbb{R}^3$.  
\end{example}

\begin{solution}
Using the subspace test given above we can verify that $L$ is a subspace of $\mathbb{R}^3$. 
\begin{itemize}
\item First: $\vect{0}_3\in L$ since $0\vect{d}=\vect{0}_3$.
\item Suppose $\vect{u},\vect{v}\in L$.
Then by definition, $\vect{u}=s\vect{d}$ and $\vect{v}=t\vect{d}$, 
for some $s,t\in\mathbb{R}$.
Thus
\[ \vect{u}+\vect{v} = s\vect{d}+t\vect{d} = (s+t)\vect{d}.\]
Since $s+t\in\mathbb{R}$, $\vect{u}+\vect{v}\in L$;
i.e., $L$ is closed under addition.
\item Suppose $\vect{u}\in L$ and $k\in\mathbb{R}$ ($k$ is a scalar).
Then $\vect{u}=t\vect{d}$, for some $t\in\mathbb{R}$, so
\[ k\vect{u}=k(t\vect{d})=(kt)\vect{d}.\]
Since $kt\in\mathbb{R}$, $k\vect{u}\in L$;
i.e., $L$ is closed under scalar multiplication.
\end{itemize}
Since $L$ satisfies all conditions of the subspace test, it follows that $L$ is a subspace. 
\end{solution}

Note that there is nothing special about the vector $\vect{d}$ used
in this example; the same proof works for any nonzero
vector $\vect{d}\in\mathbb{R}^3$, so any line through the origin is
a subspace of $\mathbb{R}^3$.

We are now prepared to examine the precise definition of a subspace as follows.

\begin{definition}{Subspace}{subspace}
Let $V$ be a nonempty collection of vectors in $\mathbb{R}^{n}.$ Then
$V$ is called a subspace \index{subspace} if whenever $a$ and $b$  are scalars and $\vect{u}$ and $\vect{v}$
are vectors in $V,$ the linear combination $a \vect{u}+ b \vect{v}$ is also in $V$.
\index{subspace} 
\end{definition}

More generally this means that a subspace contains the span of any
finite collection vectors in that subspace. It turns out that in
$\mathbb{R}^{n}$, a subspace is exactly the span of finitely many of
its vectors.

\begin{theorem}{Subspaces are Spans}{subspacesarespans}
Let $V$ be a nonempty collection of vectors in $\mathbb{R}^{n}.$ Then $V$ is a subspace of $\mathbb{R}^{n}$ if and only if there
exist vectors $\left\{ \vect{u}_{1},\cdots ,\vect{u}_{k}\right\}$ in $V$ such that 
\[
V= \func{span}\left\{ \vect{u}_{1},\cdots ,\vect{u}_{k}\right\} 
\]
\index{subspace!span} 
Furthermore, let $W$ be another subspace of $\mathbb{R}^n$ and suppose $\left\{ \vect{u}_{1},\cdots ,\vect{u}_{k}\right\} \in W$. Then it follows that $V$ is a subset of $W$. 
\end{theorem}

Note that since $W$ is arbitrary, the statement that $V \subseteq W$ means that any other subspace of $\mathbb{R}^n$ that contains these vectors will also contain $V$. 

\begin{proof}
We first show that if $V$ is a subspace, then it can be written as $V= \func{span}\left\{ \vect{u}_{1},\cdots ,\vect{u}_{k}\right\}$. Pick a vector $\vect{u}_{1}$ in $V$. If $V=
\func{span}\left\{ \vect{u}_{1}\right\} ,$ then you have found your
list of vectors and are done. If $V\neq \func{span}\left\{ \vect{u}_{1}\right\} ,$ then
there exists $\vect{u}_{2}$ a vector of $V$ which is not in $
\func{span}\left\{ \vect{u}_{1}\right\} .$ Consider $\func{span}\left\{ 
\vect{u}_{1},\vect{u}_{2}\right\}.$ 
If $V=\func{span}\left\{ \vect{u}_{1},\vect{u}_{2}\right\}$, we are
done. Otherwise, pick $\vect{u}_{3}$ not in $\func{span}\left\{ \vect{u}_{1},\vect{u}_{2}\right\} .$ Continue this way.
Note that since $V $ is a subspace, these spans are each contained in
$V$.  The process must stop with $\vect{u}_{k}$ for some $k\leq n$
by Corollary \ref{cor:lineardependenceRn}, and thus $V=\func{span}\left\{ \vect{u}_{1},\cdots ,
\vect{u}_{k}\right\}$.

Now suppose $V=\func{span}\left\{ \vect{u}_{1},\cdots ,
\vect{u}_{k}\right\}$, we must show this is a subspace. So let $\sum_{i=1}^{k}c_{i}\vect{u}_{i}$ and $
\sum_{i=1}^{k}d_{i}\vect{u}_{i}$ be two vectors in $V$, and let $a$
and $b$ be two scalars. Then 
\begin{equation*}
a \sum_{i=1}^{k}c_{i}\vect{u}_{i}+ b \sum_{i=1}^{k}d_{i}\vect{u}_{i}=
 \sum_{i=1}^{k}\left( a c_{i}+b  d_{i}\right) \vect{u}_{i}
\end{equation*}
which is one of the vectors in $\func{span}\left\{ \vect{u}_{1},\cdots ,
\vect{u}_{k}\right\}$ and is therefore contained in $V$. This shows that $\func{span}\left\{ \vect{u}_{1},\cdots ,\vect{u}_{k}\right\} $ has the properties of a subspace. 

To prove that $V \subseteq W$, we prove that if
$\vect{u}_i\in V$, then $\vect{u}_i \in W$.

Suppose $\vect{u}\in V$. 
Then $\vect{u}=a_1\vect{u}_1 + a_2\vect{u}_2 + \cdots + a_k\vect{u}_k$
for some $a_i\in\mathbb{R}$, $1\leq i\leq k$.
Since $W$ contain each $\vect{u}_i$ and $W$ is a vector space, it follows that $ a_1\vect{u}_1 + a_2\vect{u}_2 + \cdots + a_k\vect{u}_k \in W$. 
\end{proof}

Since the vectors $\vect{u}_i$ we constructed in the proof above are not in the span
of the previous vectors (by definition), they must be linearly independent and thus we
obtain the following corollary.

\begin{corollary}{Subspaces are Spans of Independent Vectors}{subspacesarespansindependentvectors}
If $V$ is a subspace of $\mathbb{R}^{n},$ then there exist linearly independent 
vectors $\left\{ \vect{u}_{1},\cdots ,\vect{u}_{k}\right\}$ in $V$ such
that $V=\func{span}\left\{ \vect{u}_{1},\cdots ,\vect{u}_{k}\right\} $.
\end{corollary}

In summary, subspaces of $\mathbb{R}^{n}$ consist of spans of finite,
linearly independent collections of vectors of $\mathbb{R}^{n}$.  Such
a collection of vectors is called a basis. \index{basis}

\begin{definition}{Basis of a Subspace}{subspacebasis}
Let $V$ be a subspace of $\mathbb{R}^{n}$. Then $\left\{
\vect{u}_{1},\cdots ,\vect{u}_{k}\right\} $ is a \textbf{basis} for $V$ if the following two conditions hold.\index{basis}\index{vectors!basis}

\begin{enumerate}
\item $\func{span}\left\{ \vect{u}_{1},\cdots ,\vect{u}_{k}\right\} =V$
\item $\left\{ \vect{u}_{1},\cdots ,\vect{u}_{k}\right\} $ is linearly
independent
\end{enumerate}

Note the plural of basis is \textbf{bases}.
\index{subspace!basis} 
\end{definition}

The following is a simple but very useful example of a basis, called the standard basis.

\begin{definition}{Standard Basis of $\mathbb{R}^n$}{standardbasis}
Let $\vect{e}_i$ be the vector in $\mathbb{R}^n$ which has a $1$ in the $i^{th}$ entry and zeros elsewhere, that is the $i^{th}$ column of the identity matrix. Then the collection $\left\{\vect{e}_1, \vect{e}_2, \cdots, \vect{e}_n
\right\}$ is a basis for
$\mathbb{R}^n$ and is called the standard basis of $\mathbb{R}^n$.
\index{standard basis}
\end{definition}

The main theorem about bases is not only they exist, but that they
must be of the same size. To show this, we will need the the following
fundamental result, called the Exchange Theorem.

\begin{theorem}{Exchange Theorem}{exchangetheorem}
Suppose $\left\{ \vect{u}_{1},\cdots ,\vect{u}_{r}\right\} $ is a
linearly independent set of vectors in $\mathbb{R}^n$, and each
$\vect{u}_{k}$ is contained in $\func{span}\left\{ \vect{v}_{1},\cdots
,\vect{v}_{s}\right\}$ Then $s\geq r.$ \\
In words, spanning \index{exchange theorem} sets have at least as many vectors as linearly
independent sets.
\end{theorem}

\begin{proof}
Since each $\vect{u}_j$ is in $\func{span}\left\{ \vect{v}_{1},\cdots ,\vect{v}_{s}\right\} 
$, there exist scalars $a_{ij}$ such that 
\begin{equation*}
\vect{u}_{j}=\sum_{i=1}^{s}a_{ij}\vect{v}_{i}
\end{equation*}
Suppose for a contradiction that $s<r$. Then the matrix $A = \leftB
a_{ij} \rightB$ has fewer rows, $s$ than columns, $r$. Then the system
$AX=0$ has %Theorem \ref{thm:rankhomogeneoussolutions} 
a non trivial solution $\vect{d}$, that is there is a  $\vect{d}\neq \vect{0}$ such that  $A\vect{d}=\vect{0}$. In other
words, 
\begin{equation*}
\sum_{j=1}^{r}a_{ij}d_{j}=0,\;i=1,2,\cdots ,s
\end{equation*}
Therefore, 
\begin{eqnarray*}
\sum_{j=1}^{r}d_{j}\vect{u}_{j} &=&\sum_{j=1}^{r}d_{j}\sum_{i=1}^{s}a_{ij}
\vect{v}_{i} \\
&=&\sum_{i=1}^{s}\left( \sum_{j=1}^{r}a_{ij}d_{j}\right) \vect{v}
_{i}=\sum_{i=1}^{s}0\vect{v}_{i}=0
\end{eqnarray*}
which contradicts the assumption that $\left\{ \vect{u}_{1},\cdots ,\vect{u}_{r}\right\} $
is linearly independent, because not all the $d_{j}$ are zero. Thus this contradiction indicates that $s\geq r$. 
\end{proof}

We are now ready to show that any two bases are of the same size. 

\begin{theorem}{Bases of $\mathbb{R}^{n}$ are of the Same Size}{basessamesize}
Let $V$ be a subspace of $\mathbb{R}^{n}$ with two bases $B_1$ and $B_2$. Suppose $B_1$ contains $s$ vectors and $B_2$ contains $r$ vectors. Then $s=r.$
\end{theorem}

\begin{proof}
This follows right away from Theorem \ref{thm:exchangetheorem}. Indeed
observe that $B_1 = \left\{
\vect{u}_{1},\cdots ,\vect{u}_{s}\right\} $ is a spanning set for $V$ while $
B_2 = \left\{ \vect{v}_{1},\cdots ,\vect{v}_{r}\right\} $ is linearly
independent, so $s \geq r.$ Similarly $B_2 = \left\{ \vect{v}_{1},\cdots ,\vect{v}
_{r}\right\} $ is a spanning set for $V$ while $B_1 = \left\{ \vect{u}_{1},\cdots ,
\vect{u}_{s}\right\} $ is linearly independent,  so $r\geq s$.
\end{proof}

The following definition can now be stated.

\begin{definition}{Dimension of a Subspace}{dimension}
Let $V$ be a subspace of $\mathbb{R}^{n}$. Then the \textbf{dimension }of $V$, written $\func{dim}(V)$
is defined to be the number of vectors in a basis. \index{dimension} \index{subspace!dimension}
\end{definition}

The next result follows.

\begin{corollary}{Dimension of $\mathbb{R}^n$}{dimensionRn}
The dimension of $\mathbb{R}^{n}$ is $n.$ 
\end{corollary}

\begin{proof}
You only need to exhibit a basis for $\mathbb{R}^{n}$ which
has $n$ vectors. Such a basis is the standard basis $\left\{ \vect{e}_{1},\cdots , \vect{e}_{n}\right\} $.
\end{proof}

Consider the following example.

\begin{example}{Basis of Subspace}{basissubspace}
Let 
\[ V=\left\{
\leftB\begin{array}{c} a\\ b\\ c\\ d\end{array}\rightB\in\mathbb{R}^4
~:~ a-b=d-c \right\}.\]
Show that $V$ is a subspace of $\mathbb{R}^4$,
find a basis of $V$, and find $\dim(V)$.
\end{example}

\begin{solution}
The condition $a-b=d-c$ is equivalent to the condition
$a=b-c+d$, so we may write

\[ V =\left\{
\leftB\begin{array}{c} b-c+d\\ b\\ c\\ d\end{array}\rightB ~:~b,c,d  \in\mathbb{R}
\right\}
= \left\{
b\leftB\begin{array}{c} 1\\ 1\\ 0\\ 0\end{array}\rightB
+c\leftB\begin{array}{c} -1\\ 0\\ 1\\ 0\end{array}\rightB
+d\leftB\begin{array}{c} 1\\ 0\\ 0\\ 1\end{array}\rightB
~:~ b,c,d\in\mathbb{R} \right\}
\]

This shows that $V$ is a subspace of $\mathbb{R}^4$,
since $V=\func{span}\{ \vect{u}_1, \vect{u}_2, \vect{u}_3 \}$ where

\begin{equation*}
\vect{u}_1  =  \leftB\begin{array}{r} 
1 \\
1 \\
0 \\
0 \end{array}\rightB, \vect{u}_2  =  \leftB\begin{array}{r} 
-1 \\
 0 \\
 1 \\
 0 \end{array}\rightB, \vect{u}_3  =  \leftB\begin{array}{r} 
1 \\
 0 \\
 0 \\
 1 \end{array}\rightB
\end{equation*}

Furthermore,

\[\left\{ 
\leftB\begin{array}{c} 1\\ 1\\ 0\\ 0\end{array}\rightB,
\leftB\begin{array}{c} -1\\ 0\\ 1\\ 0\end{array}\rightB,
\leftB\begin{array}{c} 1\\ 0\\ 0\\ 1\end{array}\rightB \right\}\]
is linearly independent, as can be seen by taking the
\rref\; of the matrix whose columns are
$\vect{u}_1, \vect{u}_2$ and $\vect{u}_3$.

\[ \leftB\begin{array}{rrr}
1 & -1 & 1 \\
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \end{array}\rightB
\rightarrow
\leftB\begin{array}{rrr}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
0 & 0 & 0 \end{array}\rightB
\]

Since every column of the \rref\; matrix has a leading one,
the columns are linearly independent.

Therefore $\{ \vect{u}_1, \vect{u}_2, \vect{u}_3 \}$ is linearly
independent and spans $V$, so is a basis of $V$. Hence 
$V$ has dimension three.
\end{solution}

We continue by stating further properties of a set of vectors in  $\mathbb{R}^{n}$.

\begin{corollary}{Linearly Independent and Spanning Sets in  $\mathbb{R}^{n}$}{independentspanningRn}
The following properties hold in $\mathbb{R}^{n}$:
\begin{itemize}
\item Suppose $\left\{ \vect{u}_{1},\cdots ,\vect{u}_{n}\right\} $ is linearly independent. Then $\left\{ \vect{u}_{1},\cdots ,\vect{u}_{n}\right\} $ is a basis for $\mathbb{R}^{n}$. 
\item Suppose $\left\{ \vect{u}_{1},\cdots ,\vect{u}_{m}\right\} $ spans $\mathbb{R}^{n}.$ Then $m\geq n.$
\item If $\left\{ \vect{u}_{1},\cdots ,\vect{u}_{n}\right\} $ spans $\mathbb{R}^{n},$ then $\left\{ \vect{u}_{1},\cdots ,\vect{u}_{n}\right\} $ is
linearly independent.
\end{itemize}
\end{corollary}

\begin{proof}
Assume first that $\left\{ \vect{u}_{1},\cdots ,\vect{u}_{n}\right\} $
is linearly independent, and we need to show that this set spans
$\mathbb{R}^{n}$. To do so, let $\vect{v}$ be a vector of
$\mathbb{R}^{n}$, and we need to write $\vect{v}$ as a linear combination of $\vect{u}_i$'s. 
Consider the matrix $A$ having the vectors $\vect{u}_i$  as
columns:
\begin{equation*}
A = 
\leftB
\begin{array}{rrr}
\vect{u}_{1} & \cdots & \vect{u}_{n} 
\end{array}
\rightB
\end{equation*}
By linear independence of the $\vect{u}_i$'s, the \rref \;of $A$ is
the identity matrix.  Therefore the system $A\vect{x}=
\vect{v}$ has a (unique) solution, so $\vect{v}$ is a linear combination
of the $\vect{u}_i$'s.

To establish the second claim, suppose that $m<n.$ Then letting
$\vect{u}_{i_{1}},\cdots ,\vect{u}_{i_{k}}$ be the pivot columns of the
matrix
\begin{equation*}
\leftB
\begin{array}{ccc}
\vect{u}_{1} & \cdots & \vect{u}_{m}
\end{array}
\rightB
\end{equation*}
it follows $k\leq m<n$ and these $k$ pivot columns would be a basis
for $\mathbb{R}^{n}$ having fewer than $n$ vectors, contrary to
Corollary \ref{cor:dimensionRn}.

Finally consider the third claim. If $\left\{ \vect{u}_{1},\cdots
,\vect{u}_{n}\right\} $ is not linearly independent, then replace this
list with $\left\{ \vect{u}_{i_{1}},\cdots ,\vect{u}_{i_{k}}\right\} $ where these
are the pivot columns of the matrix 
\begin{equation*}
\leftB
\begin{array}{ccc}
\vect{u}_{1} & \cdots & \vect{u}_{n}
\end{array}
\rightB
\end{equation*}
Then $\left\{ \vect{u}_{i_{1}},\cdots ,\vect{u}_{i_{k}}\right\} $ spans
$\mathbb{R}^{n}$ and is linearly independent, so it is a basis having
less than $n$ vectors again contrary to Corollary \ref{cor:dimensionRn}.
\end{proof}

The next theorem follows from the above claim.

\begin{theorem}{Existence of Basis}{existencebasis}
Let $V$ be a subspace of $\mathbb{R}^n$. Then there exists a basis of $V$ with 
 $\dim(V)\leq n$.
\end{theorem}

Consider Corollary \ref{cor:independentspanningRn} together with Theorem \ref{thm:existencebasis}. Let $\dim(V) = r$. Suppose there exists an independent set of vectors in $V$. If this set contains $r$ vectors, then it is a basis for $V$. If it contains less than $r$ vectors, then vectors can be added to the set to create a basis of $V$. Similarly, any spanning set of $V$ which contains more than $r$ vectors can have vectors removed to create a basis of $V$.

We illustrate this concept in the next example.

\begin{example}{Extending an Independent Set}{extendindependent}
Consider the set $U$ given by 
\[ U=\left\{ \left.\leftB\begin{array}{c} a\\ b\\ c\\ d\end{array}\rightB
\in\mathbb{R}^4 ~\right|~ a-b=d-c \right\}\]
Then $U$ is a subspace of $\mathbb{R}^4$ and $\dim(U)=3$.

Then
\[S=\left\{ 
\leftB\begin{array}{c} 1\\ 1\\ 1\\ 1\end{array}\rightB,
\leftB\begin{array}{c} 2\\ 3\\ 3\\ 2\end{array}\rightB \right\},\]
is an independent subset of $U$.
Therefore $S$ can be extended to a basis of $U$.
\end{example}

\begin{solution}
To extend $S$ to a basis of $U$, find a vector in $U$ that is {\bf not} in
$\func{span}(S)$.
\[
\leftB\begin{array}{rrr}
1 & 2 & ? \\
1 & 3 & ? \\
1 & 3 & ? \\
1 & 2 & ? 
\end{array}\rightB
\]

\[
\leftB\begin{array}{rrr}
1 & 2 & 1 \\
1 & 3 & 0 \\
1 & 3 & -1 \\
1 & 2 & 0 
\end{array}\rightB
\rightarrow
\leftB\begin{array}{rrr}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
0 & 0 & 0 
\end{array}\rightB
\]

Therefore, $S$ can be extended to the following basis of $U$:
\[\left\{ 
\leftB\begin{array}{r} 1\\ 1\\ 1\\ 1\end{array}\rightB,
\leftB\begin{array}{r} 2\\ 3\\ 3\\ 2\end{array}\rightB,
\leftB\begin{array}{r} 1\\ 0\\ -1\\ 0\end{array}\rightB
\right\},\]
\end{solution}

Next we consider the case of removing vectors from a spanning set to result in a basis. 

\begin{theorem}{Finding a Basis from a Span}{}
Let $W$ be a subspace. Also suppose that $W=\limfunc{span}\left\{ \vect{w}
_{1},\cdots ,\vect{w}_{m}\right\} $. Then there exists a subset of $\left\{ 
\vect{w}_{1},\cdots ,\vect{w}_{m}\right\} $ which is a basis for $W$.
\index{spanning set!basis}
\end{theorem}

\begin{proof}
Let $S$ denote the set of positive integers such that for $
k\in S,$ there exists a subset of $\left\{ \vect{w}_{1},\cdots ,\vect{w}_{m}\right\} $ consisting of exactly $k$ vectors
which is a spanning set for $W$. Thus $m\in S$. Pick the smallest positive
integer in $S$. Call it $k$. Then there exists $\left\{ \vect{u}_{1},\cdots ,
\vect{u}_{k}\right\} \subseteq \left\{ \vect{w}_{1},\cdots ,\vect{w}
_{m}\right\} $ such that $\limfunc{span}\left\{ \vect{u}_{1},\cdots ,\vect{u}
_{k}\right\} =W.$ If 
\begin{equation*}
\sum_{i=1}^{k}c_{i}\vect{w}_{i}=\vect{0}
\end{equation*}
and not all of the $c_{i}=0,$ then you could pick $c_{j}\neq 0$, divide by
it and solve for $\vect{u}_{j}$ in terms of the others, 
\begin{equation*}
\vect{w}_{j}=\sum_{i\neq j}\left( -\frac{c_{i}}{c_{j}}\right) \vect{w}_{i}
\end{equation*}
Then you could delete $\vect{w}_{j}$ from the list and have the same span. Any linear combination involving $\vect{w}_{j}$ would equal one in which $\vect{w}_{j}$ is replaced with the
above sum, showing that it could have been obtained as a linear combination
of $\vect{w}_{i}$ for $i\neq j$. Thus $k-1\in S$ contrary to the choice of $k$
. Hence each $c_{i}=0$ and so $\left\{ \vect{u}_{1},\cdots ,\vect{u}
_{k}\right\} $ is a basis for $W$ consisting of vectors of $\left\{ \vect{w}
_{1},\cdots ,\vect{w}_{m}\right\} $.
\end{proof}

The following example illustrates how to carry out this shrinking process which will obtain a subset of a span
of vectors which is linearly independent.

\begin{example}{Subset of a Span}{subsetbasis}
Let $W$ be the subspace 
\begin{equation*}
\limfunc{span}\left\{ \leftB 
\begin{array}{r}
1 \\ 
2 \\ 
-1 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{r}
1 \\ 
3 \\ 
-1 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{r}
8 \\ 
19 \\ 
-8 \\ 
8
\end{array}
\rightB ,\leftB 
\begin{array}{r}
-6 \\ 
-15 \\ 
6 \\ 
-6
\end{array}
\rightB ,\leftB 
\begin{array}{r}
1 \\ 
3 \\ 
0 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{r}
1 \\ 
5 \\ 
0 \\ 
1
\end{array}
\rightB \right\}
\end{equation*}
Find a basis for $W$ which consists of a subset of the given vectors.
\end{example}

\begin{solution}
You can use the \rref\; to accomplish this reduction. Form
the matrix which has the given vectors as columns. 
\begin{equation*}
\leftB
\begin{array}{rrrrrr}
1 & 1 & 8 & -6 & 1 & 1 \\ 
2 & 3 & 19 & -15 & 3 & 5 \\ 
-1 & -1 & -8 & 6 & 0 & 0 \\ 
1 & 1 & 8 & -6 & 1 & 1
\end{array}
\rightB
\end{equation*}
Then take the \rref\; 
\begin{equation*}
\leftB
\begin{array}{rrrrrr}
1 & 0 & 5 & -3 & 0 & -2 \\ 
0 & 1 & 3 & -3 & 0 & 2 \\ 
0 & 0 & 0 & 0 & 1 & 1 \\ 
0 & 0 & 0 & 0 & 0 & 0
\end{array}
\rightB
\end{equation*}
It follows that a basis for $W$ is 
\begin{equation*}
\left\{ \leftB 
\begin{array}{r}
1 \\ 
2 \\ 
-1 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{r}
1 \\ 
3 \\ 
-1 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{c}
1 \\ 
3 \\ 
0 \\ 
1
\end{array}
\rightB \right\}
\end{equation*}
Since the first, second, and fifth columns are
obviously a basis for the column space of the \rref, the same
is true for the matrix having the given vectors as columns. 
\end{solution}

Consider the following theorems regarding a subspace contained in another subspace. 

\begin{theorem}{Subset of a Subspace}{subsetdimension}
Let $V$ and $W$ be subspaces of $\mathbb{R}^n$, and suppose that $W\subseteq V$.
Then  $\dim(W) \leq \dim(V)$ with equality when $W=V$.
\end{theorem}

\begin{theorem}{Extending a Basis}{extendingbasis}
Let $W$ be any non-zero subspace $\mathbb{R}^{n}$ and let $W\subseteq V$
where $V$ is also a subspace of $\mathbb{R}^{n}$. Then every basis of $W$
can be extended to a basis for $V$.
\index{extending a basis}
\end{theorem}

The proof is left as an exercise but proceeds as follows. Begin with a basis for $W,\left\{ \vect{w}_{1},\cdots ,\vect{w}_{s}\right\} $ and add in vectors from $V$ until you obtain a basis for $V$.
Not that the process will stop because the dimension of $V$ is no more than $n$. 

Consider the following example.

\begin{example}{Extending a Basis}{extendingbasis}
Let $V=\mathbb{R}^{4}$ and let 
\begin{equation*}
W=\func{span}\left\{ \leftB
\begin{array}{c}
1 \\ 
0 \\ 
1 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{c}
0 \\ 
1 \\ 
0 \\ 
1
\end{array}
\rightB \right\}
\end{equation*}
Extend this basis of $W$ to a basis of $\mathbb{R}^{n}$.
\end{example}

\begin{solution}
An easy way to do this is to take the \rref\; of the matrix 
\begin{equation}
\leftB
\begin{array}{cccccc}
1 & 0 & 1 & 0 & 0 & 0 \\ 
0 & 1 & 0 & 1 & 0 & 0 \\ 
1 & 0 & 0 & 0 & 1 & 0 \\ 
1 & 1 & 0 & 0 & 0 & 1
\end{array}
\rightB  \label{basiseq1}
\end{equation}
Note how the given vectors were placed as the first two columns and then the matrix
was extended in such a way that it is clear that the span of the columns of
this matrix yield all of $\mathbb{R}^{4}$. Now determine the pivot columns.
The \rref\; is 
\begin{equation}
\leftB
\begin{array}{rrrrrr}
1 & 0 & 0 & 0 & 1 & 0 \\ 
0 & 1 & 0 & 0 & -1 & 1 \\ 
0 & 0 & 1 & 0 & -1 & 0 \\ 
0 & 0 & 0 & 1 & 1 & -1
\end{array}
\rightB  \label{basiseq2}
\end{equation}
Therefore the pivot columns are 
\begin{equation*}
\leftB
\begin{array}{c}
1 \\ 
0 \\ 
1 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{c}
0 \\ 
1 \\ 
0 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{c}
1 \\ 
0 \\ 
0 \\ 
0
\end{array}
\rightB ,\leftB 
\begin{array}{c}
0 \\ 
1 \\ 
0 \\ 
0
\end{array}
\rightB
\end{equation*}
and now this is an extension of the given basis for $W$ to a basis for $
\mathbb{R}^{4}$.

Why does this work? The columns of \ref{basiseq1} obviously span $\mathbb{R
}^{4}$. In fact the span of the first four is the same as the span of all
six. 
\end{solution}

Consider another example.

\begin{example}{Extending a Basis}{}
Let $W$ be the span of $\leftB
\begin{array}{c}
1 \\ 
0 \\ 
1 \\ 
0
\end{array}
\rightB $ in $\mathbb{R}^{4}$. Let $V$ consist of the span of the vectors 
\begin{equation*}
\leftB
\begin{array}{c}
1 \\ 
0 \\ 
1 \\ 
0
\end{array}
\rightB ,\leftB 
\begin{array}{c}
0 \\ 
1 \\ 
1 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{r}
7 \\ 
-6 \\ 
1 \\ 
-6
\end{array}
\rightB ,\leftB 
\begin{array}{r}
-5 \\ 
7 \\ 
2 \\ 
7
\end{array}
\rightB ,\leftB 
\begin{array}{c}
0 \\ 
0 \\ 
0 \\ 
1
\end{array}
\rightB
\end{equation*}
Find a basis for $V$ which extends the basis for $W$.
\end{example}

\begin{solution}
Note that the above vectors are not linearly independent, but their span,
denoted as $V$ is a subspace which does include the subspace $W$. 

Using the process outlined in the previous example, form the following matrix 
\begin{equation*}
\leftB
\begin{array}{rrrrr}
1 & 0 & 7 & -5 & 0 \\ 
0 & 1 & -6 & 7 & 0 \\ 
1 & 1 & 1 & 2 & 0 \\ 
0 & 1 & -6 & 7 & 1
\end{array}
\rightB
\end{equation*}
Next find its \rref\; 
\begin{equation*}
\leftB
\begin{array}{rrrrr}
1 & 0 & 7 & -5 & 0 \\ 
0 & 1 & -6 & 7 & 0 \\ 
0 & 0 & 0 & 0 & 1 \\ 
0 & 0 & 0 & 0 & 0
\end{array}
\rightB
\end{equation*}
It follows that a basis for $V$ consists of the first two vectors and the
last. 
\begin{equation*}
\left\{ \leftB
\begin{array}{c}
1 \\ 
0 \\ 
1 \\ 
0
\end{array}
\rightB ,\leftB 
\begin{array}{c}
0 \\ 
1 \\ 
1 \\ 
1
\end{array}
\rightB ,\leftB 
\begin{array}{c}
0 \\ 
0 \\ 
0 \\ 
1
\end{array}
\rightB \right\}
\end{equation*}
Thus $V$ is of dimension 3 and it has a basis which extends the basis for $W$.
\end{solution}