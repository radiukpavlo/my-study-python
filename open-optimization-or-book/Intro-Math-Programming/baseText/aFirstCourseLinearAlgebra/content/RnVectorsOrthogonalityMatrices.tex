\subsection{Orthogonal Matrices}

Recall that the process to find the inverse of a matrix was often cumbersome. 
In contrast, it was very easy to take the transpose of a matrix. Luckily for some special
matrices, the transpose equals the inverse. When an $n \times n$ matrix has all real
entries and its transpose equals its inverse, the matrix is called an \textbf{orthogonal matrix}. 

The precise definition is as follows. 

\begin{definition}{Orthogonal Matrices}{OrthoMatrix}
A real $n\times n$ matrix $U$ is called an
\index{matrix!orthogonal} \textbf{orthogonal} matrix if $UU^{T}=U^{T}U=I.$
\end{definition}

Note since $U$ is assumed to be a square matrix, it suffices to verify
only one of these equalities $UU^{T}=I$ or $U^{T}U=I$ holds to
guarantee that $U^T$ is the inverse of $U$.

Consider the following example. 

\begin{example}{Orthogonal Matrix}{}
Show the matrix
\begin{equation*}
U=\leftB
\begin{array}{rr}
\vspace{0.05in}\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\
\vspace{0.05in}\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}
\end{array}
\rightB
\end{equation*}
is orthogonal.
\end{example}

\begin{solution}
All we need to do is verify (one of the equations from) the requirements of Definition \ref{def:OrthoMatrix}.

\begin{equation*}
UU^{T}=\leftB 
\begin{array}{rr}
\vspace{0.05in}\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ 
\vspace{0.05in}\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}
\end{array}
\rightB \leftB 
\begin{array}{rr}
\vspace{0.05in}\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ 
\vspace{0.05in}\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}
\end{array}
\rightB =\allowbreak \leftB 
\begin{array}{cc}
1 & 0 \\ 
0 & 1
\end{array}
\rightB 
\end{equation*}

Since $UU^{T} = I$, this matrix is orthogonal.
\end{solution}

Here is another example.

\begin{example}{Orthogonal Matrix}{}
Let $U=\leftB
\begin{array}{rrr}
1 & 0 & 0 \\
0 & 0 & -1 \\
0 & -1 & 0
\end{array}
\rightB .$ Is $U$ orthogonal?
\end{example}

\begin{solution}
Again the answer is yes and this can be verified simply by showing that $U^{T}U=I$: 

\begin{eqnarray*}
U^{T}U&=&\leftB 
\begin{array}{rrr}
1 & 0 & 0 \\ 
0 & 0 & -1 \\ 
0 & -1 & 0
\end{array}
\rightB ^{T}\leftB 
\begin{array}{rrr}
1 & 0 & 0 \\ 
0 & 0 & -1 \\ 
0 & -1 & 0
\end{array}
\rightB \\
&=&\leftB 
\begin{array}{rrr}
1 & 0 & 0 \\ 
0 & 0 & -1 \\ 
0 & -1 & 0
\end{array}
\rightB \leftB 
\begin{array}{rrr}
1 & 0 & 0 \\ 
0 & 0 & -1 \\ 
0 & -1 & 0
\end{array}
\rightB \\
&=&\leftB 
\begin{array}{rrr}
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 1
\end{array}
\rightB
\end{eqnarray*}
\end{solution}

When we say that $U$ is orthogonal, we are saying that $UU^T=I$, meaning that
\begin{equation*}
\sum_{j}u_{ij}u_{jk}^{T}=\sum_{j}u_{ij}u_{kj}=\delta _{ik}
\end{equation*}
where $\delta _{ij}$ is the \textbf{Kronecker symbol}
defined
\index{Kronecker symbol} by
\begin{equation*}
\delta _{ij}=\left\{
\begin{array}{c}
1
\text{ if }i=j \\
0\text{ if }i\neq j
\end{array}
\right.
\end{equation*}

In words, the product of the $i^{th}$ row of $U$ with the $k^{th}$ row
gives $1$ if $i=k$ and $0$ if $i\neq k.$ The same is true of the columns because 
$U^{T}U=I$ also. Therefore, 
\begin{equation*}
\sum_{j}u_{ij}^{T}u_{jk}=\sum_{j}u_{ji}u_{jk}=\delta _{ik}
\end{equation*}
which says that the product of one column with another column gives $1$ if the two
columns are the same and $0$ if the two columns are different.

More succinctly, this states that if $\vect{u}_{1},\cdots ,\vect{u}_{n}$
are the columns of $U,$ an orthogonal matrix, then 
\[
\vect{u}_{i}\dotprod \vect{u}_{j}=\delta _{ij} = \left\{ 
\begin{array}{c}
1\text{ if }i=j \\ 
0\text{ if }i\neq j
\end{array}
\right.  
\]

We will say that the columns form an orthonormal set of vectors, and similarly for the rows. Thus a matrix is \textbf{orthogonal} if its rows (or columns) form an
\textbf{orthonormal} set of vectors. Notice that the convention is to call such a matrix orthogonal rather than orthonormal (although this may make more sense!). 

\begin{proposition}{Orthonormal Basis}{orthbasis}
The rows of an $n \times n$ orthogonal matrix form an orthonormal
basis of $\mathbb{R}^n$. Further, any orthonormal basis of
$\mathbb{R}^n$ can be used to construct an $n \times n$ orthogonal
matrix.
\end{proposition}

\begin{proof}
Recall from Theorem \ref{thm:orthbasis} that an orthonormal set is
linearly independent and forms a basis for its span. Since the rows of
an $n \times n$ orthogonal matrix form an orthonormal set, they must
be linearly independent. Now we have $n$ linearly independent vectors,
and it follows that their span equals $\mathbb{R}^n$. Therefore these
vectors form an orthonormal basis for $\mathbb{R}^n$.

Suppose now that we have an orthonormal basis for $\mathbb{R}^n$. Since the
basis will contain $n$ vectors, these can be used to construct an $n
\times n$ matrix, with each vector becoming a row. Therefore the
matrix is composed of orthonormal rows, which by our above discussion,
means that the matrix is orthogonal. Note we could also have construct
a matrix with each vector becoming a column instead, and this would
again be an orthogonal matrix. In fact this is simply the transpose of
the previous matrix.
\end{proof}

Consider the following proposition.

\begin{proposition}{Determinant of Orthogonal Matrices}{orthoDet}
Suppose $U$ is an orthogonal matrix. Then $\det \left( U\right) = \pm 1.$ 
\end{proposition}

\begin{proof}
This result follows from the properties of determinants. Recall that
for any matrix $A$, $\det(A)^T = \det(A)$. Now if $U$ is orthogonal, then:
\begin{equation*}
(\det \left( U\right)) ^{2}=\det \left( U^{T}\right) \det \left( U\right)
=\det \left( U^{T}U\right) =\det \left( I\right) =1
\end{equation*}

Therefore $(\det (U))^2 = 1$ and it follows that $\det \left( U\right) = \pm 1$. 
\end{proof}

Orthogonal matrices are divided into two classes, proper and improper.
\index{matrix!proper}\index{matrix!improper}
The proper orthogonal matrices are those whose determinant equals 1
and the improper ones are those whose determinant equals $-1$. The
reason for the distinction is that the improper orthogonal matrices
are sometimes considered to have no physical significance. These
matrices cause a change in orientation which would correspond to
material passing through itself in a non physical manner. Thus in
considering which coordinate systems must be considered in certain
applications, you only need to consider those which are related by a
proper orthogonal transformation. Geometrically, the linear
transformations determined by the proper orthogonal matrices
correspond to the composition of rotations.

We conclude this section with two useful properties of orthogonal matrices. 

\begin{example}{Product and Inverse of Orthogonal Matrices}{productinverseorthogonal}
Suppose $A$ and $B$ are orthogonal matrices. Then $AB$ and $A^{-1}$ both exist and are orthogonal.
\end{example}

\begin{solution}
First we examine the product $AB$. 
\[ (AB)(B^TA^T)=A(BB^T)A^T =AA^T=I \]
Since $AB$ is square, $B^TA^T=(AB)^T$ is the inverse of
$AB$, so $AB$ is invertible, and $(AB)^{-1}=(AB)^T$
Therefore, $AB$ is orthogonal.

Next we show that $A^{-1}=A^T$ is also orthogonal. 
\[ (A^{-1})^{-1} = A = (A^T)^{T}
=(A^{-1})^{T} \]
Therefore $A^{-1}$ is also orthogonal.
\end{solution}