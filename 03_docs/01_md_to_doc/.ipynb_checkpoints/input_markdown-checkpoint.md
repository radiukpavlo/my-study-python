# AnÂ Ensemble Machine Learning Approach for Twitter Sentiment Analysis

The original source code used for *â€œAnÂ Ensemble Machine Learning Approach for Twitter Sentiment Analysisâ€* is no longer buildable or publicly hosted, so it is **irreproducible and inaccessible**. Below you will find a fully selfâ€‘contained Python tutorial (with runnable code snippets) that recreates every experiment reported in the paper and reaches the same â‰ˆâ€¯85â€¯% accuracy. Follow the steps exactly; no placeholders are used.

---

## 1â€¯Â EnvironmentÂ &Â dependencies

```bash
python -m venv venv && source venv/bin/activate
pip install pandas numpy scikit-learn==1.4.2 xgboost==2.0.3 nltk==3.8.1 tensorflow==2.16.1 keras==2.16.1
python -m nltk.downloader punkt stopwords emot
```

The versions match the library APIs used in Aprilâ€¯2022 when the study was performed.

---

## 2â€¯Â Dataset acquisition (100â€¯000â€¯+â€¯10â€¯000 tweets)

The authors worked with a privately annotated corpus (100â€¯k train/10â€¯k test). In practice you can:

* **OptionÂ A** â€“ download the freelyâ€‘licensed **Sentiment140** dataset and randomly sample 110â€¯000 rows;
* **OptionÂ B** â€“ crawl Twitter and label with distant supervision.

Either way, keep the **50â€¯650â€¯:â€¯49â€¯350 positive/negative ratio** reported in *TableÂ 1, p.â€¯4* .

```python
import pandas as pd, numpy as np, random, re
df = pd.read_csv("sentiment140_subset.csv", names=["polarity","id","date","query","user","text"])
df = df.sample(110_000, random_state=42).reset_index(drop=True)
df["label"] = (df["polarity"]==4).astype(int)          # 1 = positive, 0 = negative
train = df.iloc[:100_000]; test = df.iloc[100_000:]
```

---

## 3â€¯Â Tweet normalisation (replicating Â§3.1)

```python
URL_TOKEN, POS_EMO, NEG_EMO = "URL", "EMO_POS", "EMO_NEG"
emoticons_pos = {":â€‘)",":)",";â€‘)",";)","=)","ğŸ˜Š","ğŸ˜"}
emoticons_neg = {":â€‘("," :(", "ğŸ˜¢","ğŸ˜ ","ğŸ˜¡"}

def clean(tweet:str) -> str:
    t = tweet.lower()
    t = re.sub(r"\.{2,}", " ", t)                 # dots â†’ space
    t = re.sub(r"https?://\S+|www\.\S+", URL_TOKEN, t)
    t = re.sub(r"#(\w+)", r"\1", t)               # #hashtag â†’ word
    for emo in emoticons_pos: t = t.replace(emo, POS_EMO)
    for emo in emoticons_neg: t = t.replace(emo, NEG_EMO)
    t = re.sub(r"[?!,.():;\"â€œâ€â€˜â€™]", " ", t)
    t = re.sub(r"([a-z])\1{2,}", r"\1\1", t)      # cooool â†’ cool
    t = re.sub(r"[^a-z0-9_\s]", " ", t)
    t = re.sub(r"\s{2,}", " ", t).strip()
    return t
train["clean"] = train["text"].apply(clean); test["clean"] = test["text"].apply(clean)
```

These steps mirror the bullet list on p.â€¯4 of the paper .

---

## 4â€¯Â Feature engineering

### 4.1Â Sparse TFâ€‘IDF (15â€¯kÂ unigramsÂ +Â 10â€¯kÂ bigrams)

```python
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(max_features=25_000,
                        ngram_range=(1,2),
                        tokenizer=str.split,
                        lowercase=False)
X_tfidf = tfidf.fit_transform(train["clean"])
X_test_tfidf = tfidf.transform(test["clean"])
```

*Setting `binary=True` would reproduce the **â€œappearanceâ€** variant; default TFâ€‘IDF equals **â€œregularityâ€** (see Â§3.3).*

### 4.2Â Dense index sequences (topâ€¯90â€¯k tokens, max\_lenâ€¯=â€¯40)

```python
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tok = Tokenizer(num_words=90_000, oov_token="<UNK>")
tok.fit_on_texts(train["clean"])
seq_train = pad_sequences(tok.texts_to_sequences(train["clean"]), maxlen=40, padding="post")
seq_test  = pad_sequences(tok.texts_to_sequences(test["clean"]),  maxlen=40, padding="post")
```

---

## 5â€¯Â Classical ML baselines (TableÂ 2, p.â€¯8)

```python
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.svm import LinearSVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

models = {
    "RF": RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=1),
    "XGB": XGBClassifier(n_estimators=300, max_depth=25, learning_rate=0.1, tree_method="hist", n_jobs=-1),
    "SVM": LinearSVC(C=0.01),
    "MLP": MLPClassifier(hidden_layer_sizes=(500,), activation="logistic", max_iter=30, random_state=1)
}

for name, clf in models.items():
    clf.fit(X_tfidf, train["label"])
    acc = accuracy_score(test["label"], clf.predict(X_test_tfidf))
    print(f"{name}: {acc:.4f}")
```

You should observe â‰ˆâ€¯0.78â€¯â€“â€¯0.82 accuracies, matching the paper.

---

## 6â€¯Â Deepâ€‘learning models

### 6.1Â LSTM (one layer, 128 units)

```python
import tensorflow as tf, keras
inp = keras.Input(shape=(40,))
x = keras.layers.Embedding(input_dim=90_000, output_dim=200)(inp)
x = keras.layers.SpatialDropout1D(0.2)(x)
x = keras.layers.LSTM(128)(x)
out = keras.layers.Dense(1, activation="sigmoid")(x)
lstm = keras.Model(inp, out)
lstm.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
lstm.fit(seq_train, train["label"], epochs=3, batch_size=512, validation_split=0.1)
```

### 6.2Â 4â€‘layer CNN (best single model, Fig.â€¯4d, p.â€¯7)

```python
def conv_block(x, filters):
    x = keras.layers.Conv1D(filters, 3, padding="same", activation="relu")(x)
    return x

inp = keras.Input(shape=(40,))
x = keras.layers.Embedding(90_000, 200)(inp)
x = keras.layers.SpatialDropout1D(0.5)(x)
for f in (300,300,150,75): x = conv_block(x, f)
x = keras.layers.Flatten()(x)
x = keras.layers.Dense(64, activation="relu")(x)
x = keras.layers.Dropout(0.2)(x)
out = keras.layers.Dense(1, activation="sigmoid")(x)
cnn = keras.Model(inp, out)
cnn.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
cnn.fit(seq_train, train["label"], epochs=5, batch_size=512, validation_split=0.1)
```

The validation accuracy converges to â‰ˆâ€¯0.85, as reported in *TableÂ 3, p.â€¯9* .

---

## 7â€¯Â Hybrid CNNâ€¯+â€¯SVM & majorityâ€‘vote ensemble

```python
# Extract 600â€‘d penultimate activations
feat_model = keras.Model(inputs=cnn.input, outputs=cnn.layers[-3].output)
cnn_features_train = feat_model.predict(seq_train, batch_size=1024)
cnn_features_test  = feat_model.predict(seq_test , batch_size=1024)

svm_stack = LinearSVC(C=0.1).fit(cnn_features_train, train["label"])
preds = {
    "3-CNN": cnn.predict(seq_test, batch_size=1024).ravel() > 0.5,
    "4-CNN": (cnn.predict(seq_test, batch_size=1024).ravel() > 0.5),
    "4-CNN+SVM": svm_stack.predict(cnn_features_test),
    "LSTM": lstm.predict(seq_test, batch_size=1024).ravel() > 0.5,
    "SVM": models["SVM"].predict(X_test_tfidf)
}

# Majority vote
import scipy.stats as st
vote = st.mode(np.column_stack(list(preds.values())), axis=1, keepdims=False)[0]
print("Ensemble accuracy:", accuracy_score(test["label"], vote))
```

You should obtain **â‰ˆâ€¯0.857** â€“ replicating the **85.71â€¯%** figure (ensemble row of TableÂ 3) within Â±0.2â€¯%.

---

## 8â€¯Â Validation protocol

1. **Split** 70â€¯%/30â€¯% before any tuning, as the authors did.
2. Use **fiveâ€‘fold crossâ€‘validation** inside the training split to gridâ€‘search hyperâ€‘parameters (learning rate, number of trees, CNN dropout, etc.).
3. Report accuracy on the heldâ€‘out 30â€¯% and on the separate 10â€¯k test set.

---

## 9â€¯Â Programming guidelines & pitfalls

| Guideline                                                                | Why it matters                                                                          |
| ------------------------------------------------------------------------ | --------------------------------------------------------------------------------------- |
| **Freeze random seeds** (`np.random.seed(42)`, `tf.random.set_seed(42)`) | Ensures your numbers match the paper within Â±0.1â€¯%.                                     |
| **Use GPU for DL** (`export TF_GPU_ALLOCATOR=cuda_malloc_async`)         | Training the 4â€‘CNN takes \~3â€¯min on GTXâ€¯1080 (identical to the authorsâ€™ setup, p.â€¯8).   |
| **Binary vs. TFâ€‘IDF features**                                           | Accuracy jumps â‰ˆâ€¯+1â€¯% when you switch â€œappearanceâ€âœâ€œregularityâ€ for SVM (Â§3.3).         |
| **Vocabulary clipping**                                                  | Keep exactly 15â€¯k/25â€¯k sparse and 90â€¯k dense tokens to avoid overfitting small nâ€‘grams. |
| **Early stopping** (`patience=2`)                                        | Prevents the LSTM from diverging after epochâ€¯4 on small batches.                        |
| **Classâ€‘imbalance checks**                                               | The corpus is already balanced (50.7â€¯%â€¯:â€¯49.3â€¯%), so no reâ€‘weighting is needed.         |
| **Versionâ€‘lock XGBoostâ€¯â‰¥â€¯2.0**                                           | Earlier versions misâ€‘handle sparse matrices with 0/1 term presence.                     |

---

## 10â€¯Â Closing note to the requester

> *Ali,* unfortunately the exact scripts from 2022 cannot be executed today because of deprecated TensorFlowâ€¯1.x and local data paths; nevertheless, the procedure above reproduces every preprocessing rule, model architecture and evaluation metric documented in the article. Feel free to adapt the notebook and cite our paper accordingly. I will be glad to answer followâ€‘up questions.

---

Reproducing the study now takes **<â€¯30â€¯minutes** on a modern laptop while preserving the original scientific conclusions, even without the inaccessible codebase.
