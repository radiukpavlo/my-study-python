\subsection{Linearly Independent Set of Vectors}

We now turn our attention to the following question: what linear
combinations of a given set of vectors $\{ \vect{u}_1, \cdots
,\vect{u}_k\}$ in $\mathbb{R}^{n}$ yields the zero vector? Clearly
$0\vect{u}_1 + 0\vect{u}_2+ \cdots + 0 \vect{u}_k = \vect{0}$, but is
it possible to have $\sum_{i=1}^{k}a_{i}\vect{u}_{i}=\vect{0}$ without
all coefficients being zero?

You can create examples where this easily happens. For example if $\vec{u}_1=\vect{u}_2$, then 
$1\vect{u}_1 - \vect{u}_2+ 0 \vect{u}_3 + \cdots  + 0 \vect{u}_k = \vect{0}$, no matter the vectors 
 $\{ \vect{u}_3, \cdots ,\vect{u}_k\}$. 0But sometimes it can be more subtle. 

\begin{example}{Linearly Dependent Set of Vectors}{linearlydependentvectors}
Consider the vectors 
\begin{equation*}
\vect{u}_1=\leftB 
\begin{array}{rrr}
0  & 1 & -2
\end{array}
\rightB^T, 
\vect{u}_2=\leftB 
\begin{array}{rrr}
1  & 1 & 0
\end{array}
\rightB^T, 
\vect{u}_3=\leftB 
\begin{array}{rrr}
-2  & 3 & 2
\end{array}
\rightB^T, \mbox{ and } 
\vect{u}_4=\leftB 
\begin{array}{rrr}
1  & -2 & 0
\end{array}
\rightB^T
\end{equation*}
in $\mathbb{R}^{3}$.

Then verify that 
\begin{equation*}
1\vect{u}_1 +0 \vect{u}_2+ - \vect{u}_3 -2 \vect{u}_4 = \vect{0}
\end{equation*}
\end{example}

You can see that the linear combination does yield the zero vector but
has some non-zero coefficients. Thus we define a set of vectors to be
{\em linearly dependent} if this happens.

\begin{definition}{Linearly Dependent Set of Vectors}{lineardependent}
A set of non-zero vectors $\{ \vect{u}_1, \cdots ,\vect{u}_k\}$ in $\mathbb{R}^{n}$ is said to be 
\textbf{linearly dependent} if a linear combination of these vectors without all  coefficients being zero does yield the zero vector.
\index{linear dependence}\index{vectors!linear dependent}
\end{definition}

Note that if $\sum_{i=1}^{k}a_{i}\vect{u}_{i}=\vect{0}$ and some
coefficient is non-zero, say $a_1 \neq 0$, then 
\begin{equation*}
\vec{u}_1 = \frac{-1}{a_1} \sum_{i=2}^{k}a_{i}\vect{u}_{i} 
\end{equation*}
and thus $\vec{u}_1$ is in the span of the other vectors. And the converse clearly works as well,
so we get that a set of vectors is linearly dependent precisely when
one of its vector is in the span of the other vectors of that set.

In particular, you can show that the vector $\vec{u}_1$ in the above
example is in the span of the vectors  $\{ \vect{u}_2, \vect{u}_3, \vect{u}_4 \}$. 

If a set of vectors is NOT linearly dependent, then it must be that
any linear combination of these vectors which yields the zero vector
must use all zero coefficients. This is a very important notion, and we give it its own name of {\em
linear independence}. 

\begin{definition}{Linearly Independent Set of Vectors}{linearindependent}
A set of non-zero vectors $\{ \vect{u}_1, \cdots ,\vect{u}_k\}$ in $\mathbb{R}^{n}$ is said to be 
\textbf{linearly independent} if whenever 
\begin{equation*}
\sum_{i=1}^{k}a_{i}\vect{u}_{i}=\vect{0}
\end{equation*}
it follows that each $a_{i}=0$.
\end{definition}

\index{linear independence}\index{vectors!linear independent}

Note also that we require all vectors to be non-zero to form a
linearly independent set.

To view this in a more familiar setting, form the $n \times k$ matrix
$A$ having these vectors as columns. Then all we are saying is that
the set $\{ \vect{u}_1, \cdots ,\vect{u}_k\}$ is linearly independent
precisely when $AX=0$ has only the trivial solution.

Here is an example.  

\begin{example}{Linearly Independent Vectors}{linearlyindependentvectors}
Consider the vectors $\vect{u}=\leftB 
\begin{array}{rrr}
1  & 1 & 0
\end{array}
\rightB^T$, 
$\vect{v}=\leftB 
\begin{array}{rrr}
1  & 0 & 1
\end{array}
\rightB^T$, and
$\vect{w}=\leftB 
\begin{array}{rrr}
0  & 1 & 1
\end{array}
\rightB^T$ in $\mathbb{R}^{3}$.
Verify whether the set $\{\vect{u}, \vect{v}, \vect{w}\}$ is linearly independent. 
\end{example}

\begin{solution}
So suppose that we have a linear combinations $a\vect{u} + b \vect{v}
+ c\vect{w} = \vect{0}$. Then you can see that this can only happen
with $a=b=c=0$.

As mentioned above, you can equivalently form the $3 \times 3$ matrix $A = 
\leftB 
\begin{array}{ccc}
1  & 1 & 0 \\
1  & 0 & 1 \\
0  & 1 & 1 \\
\end{array}
\rightB$, and show that $AX=0$ has only the trivial solution.

Thus this  means the set $\left\{ \vect{u}, \vect{v}, \vect{w} \right\}$ is linearly independent. 
\end{solution}

In terms of spanning, a set of vectors is linearly independent if it
does not contain unnecessary vectors, that is not vector is in the span of the others.

Thus we put all this together in the following important theorem.

\begin{theorem}{Linear Independence as a Linear Combination}{linearindependencecombination}
Let $\left\{\vect{u}_{1},\cdots ,\vect{u}_{k}\right\}$ be a collection of vectors  in
$\mathbb{R}^{n}$. Then the following are equivalent:

\begin{enumerate}
\item It is linearly independent, that is whenever
\begin{equation*}
\sum_{i=1}^{k}a_{i}\vect{u}_{i}=\vect{0}
\end{equation*}
it follows that each coefficient $a_{i}=0$.
\item No vector is in the span of the others.
\item The system of
linear equations $AX=0$ has only the trivial solution, where $A$ is
the $n \times k$ matrix having these vectors as columns. 
\end{enumerate}
\end{theorem}

The last sentence of this theorem is useful as it allows us to use the
\rref\; of a matrix to determine if a set of vectors is linearly
independent. Let the vectors be columns of a matrix $A$. Find the
\rref \; of $A$. If each column has a leading one, then it follows
that the vectors are linearly independent.

Sometimes we refer to the condition regarding sums as follows: The set
of vectors, $\left\{ \vect{u}_{1},\cdots ,\vect{u}_{k}\right\} $ is
linearly independent if and only if there is no nontrivial linear
combination which equals the zero vector. A nontrivial linear
combination is one in which not all the scalars equal zero. Similarly,
a trivial linear combination is one in which all scalars equal zero.

Here is a detailed  example in  $\mathbb{R}^{4}$. 

\begin{example}{Linear Independence}{linearindependence2}
Determine whether the set of vectors given by  
\[ \left\{ \leftB
\begin{array}{r}
1 \\
2 \\
3 \\
0
\end{array}
\rightB, \; \leftB
\begin{array}{r}
2 \\
1 \\
0 \\
1
\end{array}
\rightB , \; \leftB
\begin{array}{r}
0 \\
1 \\
1 \\
2
\end{array}
\rightB  , \; \leftB
\begin{array}{r}
3 \\
2 \\
2 \\
0
\end{array}
\rightB \right\} \]
is linearly independent. If it is linearly dependent,
express one of the vectors as a linear combination of the others.
\end{example}

\begin{solution}
In this case the matrix of the corresponding homogeneous system of linear equations  is 
\begin{equation*}
\leftB 
\begin{array}{rrrr|r}
1 & 2 & 0 & 3 & 0\\ 
2 & 1 & 1 & 2 & 0 \\ 
3 & 0 & 1 & 2 & 0 \\ 
0 & 1 & 2 & 0 & 0 
\end{array}
\rightB
\end{equation*}
The \rref \;is 
\begin{equation*}
\leftB 
\begin{array}{rrrr|r}
1 & 0 & 0 & 0 & 0 \\ 
0 & 1 & 0 & 0 & 0 \\ 
0 & 0 & 1 & 0 & 0 \\ 
0 & 0 & 0 & 1 & 0 
\end{array}
\rightB
\end{equation*}
and so every column is a pivot column and the corresponding system
$AX=0$ only has the trivial solution.  Therefore, these vectors are
linearly independent and there is no way to obtain one of the vectors
as a linear combination of the others.
\end{solution}

Consider another example.

\begin{example}{Linear Independence}{linearindependence}
Determine whether the set of vectors given by  
\[\left\{ 
\leftB
\begin{array}{r}
1 \\
2 \\
3 \\
0
\end{array}
\rightB, \; \leftB
\begin{array}{r}
2 \\
1 \\
0 \\
1
\end{array}
\rightB, \; \leftB
\begin{array}{r}
0 \\
1 \\
1 \\
2
\end{array}
\rightB, \; \leftB
\begin{array}{r}
3 \\
2 \\
2 \\
-1
\end{array}
\rightB \right\} \]
is linearly independent. If it is linearly dependent,
express one of the vectors as a linear combination of the others.
\end{example}

\begin{solution}
Form the $4 \times 4$  matrix $A$ having these vectors as columns:
\begin{equation*}
A= \leftB 
\begin{array}{rrrr}
1 & 2 & 0 & 3 \\ 
2 & 1 & 1 & 2 \\ 
3 & 0 & 1 & 2 \\ 
0 & 1 & 2 & -1
\end{array}
\rightB
\end{equation*}
Then by Theorem \ref{thm:linearindependencecombination}, the given set of vectors is linearly independent
exactly if the system $AX=0$ has only the trivial solution.

The augmented matrix for this system and corresponding \rref \;are given by  
\begin{equation*}
 \leftB 
\begin{array}{rrrr|r}
1 & 2 & 0 & 3 & 0 \\ 
2 & 1 & 1 & 2 & 0 \\ 
3 & 0 & 1 & 2 & 0 \\ 
0 & 1 & 2 & -1 & 0 
\end{array}
\rightB
\rightarrow \cdots \rightarrow
\leftB 
\begin{array}{rrrr|r}
1 & 0 & 0 & 1 & 0 \\ 
0 & 1 & 0 & 1 & 0 \\ 
0 & 0 & 1 & -1 & 0 \\ 
0 & 0 & 0 & 0 & 0 
\end{array}
\rightB 
\end{equation*}
Not all the columns of the coefficient matrix are pivot columns and so the vectors are not linearly independent. In this case, we say the vectors are linearly dependent. 

It follows that there are infinitely many solutions to $AX=0$, one of which is
\begin{equation*}
\leftB 
\begin{array}{r}
1 \\ 
1 \\ 
-1 \\ 
-1
\end{array}
\rightB
\end{equation*}
Therefore we can write 
\begin{equation*}
1\leftB 
\begin{array}{r}
1 \\ 
2 \\ 
3 \\ 
0
\end{array}
\rightB +1\leftB
\begin{array}{r}
2 \\ 
1 \\ 
0 \\ 
1
\end{array}
\rightB  -1 \leftB 
\begin{array}{r}
0 \\ 
1 \\ 
1 \\ 
2
\end{array}
\rightB -1 \leftB 
\begin{array}{r}
3 \\ 
2 \\ 
2 \\ 
-1
\end{array}
\rightB =
\leftB 
\begin{array}{r}
0 \\ 
0 \\ 
0 \\ 
0
\end{array}
\rightB
\end{equation*}

This can be rearranged as follows
\begin{equation*}
1\leftB 
\begin{array}{r}
1 \\ 
2 \\ 
3 \\ 
0
\end{array}
\rightB +1\leftB
\begin{array}{r}
2 \\ 
1 \\ 
0 \\ 
1
\end{array}
\rightB  -1 \leftB 
\begin{array}{r}
0 \\ 
1 \\ 
1 \\ 
2
\end{array}
\rightB =\leftB 
\begin{array}{r}
3 \\ 
2 \\ 
2 \\ 
-1
\end{array}
\rightB 
\end{equation*}
This gives the last vector as a linear combination of the first three vectors.

Notice that we could rearrange this equation to write any of the four vectors as a linear combination of the other three. 
\end{solution}

When given a linearly independent set of vectors, we can determine if related sets are linearly independent. 

\begin{example}{Related Sets of Vectors}{relatedlinearindependence}
Let $\{ \vect{u},\vect{v},\vect{w}\}$ be an independent set of $\mathbb{R}^n$.
Is $\{\vect{u}+\vect{v}, 2\vect{u}+\vect{w}, \vect{v}-5\vect{w}\}$ linearly
independent?
\end{example}

\begin{solution}
Suppose $a(\vect{u}+\vect{v}) + b(2\vect{u}+\vect{w}) + c(\vect{v}-5\vect{w})=\vect{0}_n$
for some $a,b,c\in\mathbb{R}$.
Then 
\[ (a+2b)\vect{u} + (a+c)\vect{v} + (b-5c)\vect{w}=\vect{0}_n.\]

Since $\{\vect{u},\vect{v},\vect{w}\}$ is independent, 
\begin{eqnarray*}
a + 2b & = & 0 \\
a + c & = & 0 \\
b - 5c & = & 0 
\end{eqnarray*}

This system of three equations in three variables has 
the unique solution $a=b=c=0$.
Therefore, $\{\vect{u}+\vect{v}, 2\vect{u}+\vect{w}, \vect{v}-5\vect{w}\}$ is independent.
\end{solution}

The following corollary follows from the fact that if the augmented matrix of a homogeneous
system of linear equations has more columns than rows, the system has infinitely many
solutions.

\begin{corollary}{Linear Dependence in $\mathbb{R}^{n}$}{lineardependenceRn}
Let $\left\{ \vect{u}_{1},\cdots ,\vect{u}_{k}\right\} $
be a set of vectors in $\mathbb{R}^{n}$. 
If $k>n$, then the set is linearly dependent (i.e. NOT linearly independent).
\end{corollary}

\begin{proof}
Form the $n \times k$ matrix $A$ having the vectors $\left\{
\vect{u}_{1},\cdots ,\vect{u}_{k}\right\} $ as its columns and suppose $k > n$. Then $A$ has rank $r \leq n <k$, so 
the system $AX=0$ has a nontrivial solution % by \ref{thm:rankhomogeneoussolutions}
 and thus not linearly independent by Theorem 
 \ref{thm:linearindependencecombination}.
\end{proof}

\begin{example}{Linear Dependence}{lineardependence}
Consider the vectors 
\[
\left\{ \leftB \begin{array}{r}
1 \\
4 
\end{array}
\rightB, 
\leftB \begin{array}{r}
2 \\
3
\end{array}
\rightB, 
\leftB \begin{array}{r}
3 \\
2
\end{array}
\rightB \right\}
\]
Are these vectors linearly independent?
\end{example}

\begin{solution}
This set contains three vectors in $\mathbb{R}^2$. By Corollary \ref{cor:lineardependenceRn} these vectors are linearly dependent.
In fact, we can write
\[
(-1) \leftB \begin{array}{r}
1 \\
4 
\end{array}
\rightB + (2) 
\leftB \begin{array}{r}
2 \\
3
\end{array}
\rightB = 
\leftB \begin{array}{r}
3 \\
2
\end{array}
\rightB
\]
showing that this set is linearly dependent. 
\end{solution}

The third vector in the previous example is in the span of the first two vectors. We could find a way to write this vector as a linear combination of the other two vectors. It turns out that the linear combination which we found is the \textbf{only} one, provided that the set is linearly independent. 

\begin{theorem}{Unique Linear Combination}{uniquelinearcombination}
Let $U \subseteq\mathbb{R}^n$ be an independent set.
Then any vector $\vect{x}\in\func{span}(U)$ can be written uniquely as a linear combination of vectors of $U$.
\end{theorem}

\begin{proof}
To prove this theorem, we will show that two linear combinations of vectors in $U$ that equal $\vect{x}$ must be the same. Let $U =\{ \vect{u}_1, \vect{u}_2, \ldots, \vect{u}_k\}$.
Suppose that there is a vector $\vect{x}\in \func{span}(U)$ such that
\begin{eqnarray*}
\vect{x} & = & s_1\vect{u}_1 + s_2\vect{u}_2 + \cdots + s_k\vect{u}_k,
\mbox{ for some } s_1, s_2, \ldots, s_k\in\mathbb{R}, \mbox{ and} \\
\vect{x} & = & t_1\vect{u}_1 + t_2\vect{u}_2 + \cdots + t_k\vect{u}_k,
\mbox{ for some } t_1, t_2, \ldots, t_k\in\mathbb{R}.
\end{eqnarray*}
Then 
$\vect{0}_n=\vect{x}-\vect{x} = (s_1-t_1)\vect{u}_1 + (s_2-t_2)\vect{u}_2 + \cdots +
(s_k-t_k)\vect{u}_k$.

Since $U$ is independent, the only linear combination that vanishes
is the trivial one, so $s_i-t_i=0$ for all $i$, $1\leq i\leq k$.

Therefore, $s_i=t_i$ for all $i$, $1\leq i\leq k$, and the
representation is unique.Let $U \subseteq\mathbb{R}^n$ be an independent set.
Then any vector $\vect{x}\in\func{span}(U)$ can be written uniquely as a linear combination of vectors of $U$.
\end{proof}

Suppose that $\vect{u},\vect{v}$ and $\vect{w}$ are nonzero vectors in $\mathbb{R}^3$,
and that $\{ \vect{v},\vect{w}\}$ is independent. Consider the set $\{ \vect{u},\vect{v},\vect{w}\}$. When can we know that this set is independent? It turns out that this follows exactly when $\vect{u}\not\in\func{span}\{\vect{v},\vect{w}\}$.

\begin{example}{}{}
Suppose that $\vect{u},\vect{v}$ and $\vect{w}$ are nonzero vectors in $\mathbb{R}^3$,
and that $\{ \vect{v},\vect{w}\}$ is independent.
Prove that $\{ \vect{u},\vect{v},\vect{w}\}$ is independent if and only if 
$\vect{u}\not\in\func{span}\{\vect{v},\vect{w}\}$.
\end{example}

\begin{solution}
If $\vect{u}\in\func{span}\{\vect{v},\vect{w}\}$, then there exist $a,b\in\mathbb{R}$ so
that $\vect{u}=a\vect{v} + b\vect{w}$.
This implies that $\vect{u}-a\vect{v} - b\vect{w}=\vect{0}_3$,
so  $\vect{u}-a\vect{v} - b\vect{w}$
is a nontrivial linear combination of $\{ \vect{u},\vect{v},\vect{w}\}$ that
vanishes, 
and thus $\{ \vect{u},\vect{v},\vect{w}\}$ is dependent.

Now suppose that $\vect{u}\not\in\func{span}\{\vect{v},\vect{w}\}$, and suppose
that there exist $a,b,c\in\mathbb{R}$ such that
$a\vect{u}+b\vect{v}+c\vect{w}=\vect{0}_3$.
If $a\neq 0$, then $\vect{u}=-\frac{b}{a}\vect{v}-\frac{c}{a}\vect{w}$,
and $\vect{u}\in\func{span}\{\vect{v},\vect{w}\}$, a contradiction.
Therefore, $a=0$, implying that $b\vect{v}+c\vect{w}=\vect{0}_3$.
Since $\{ \vect{v},\vect{w}\}$ is independent, $b=c=0$, and thus
$a=b=c=0$, i.e., the only linear combination of 
$\vect{u},\vect{v}$ and $\vect{w}$ that vanishes is the trivial one.

Therefore, $\{ \vect{u},\vect{v},\vect{w}\}$ is independent.
\end{solution}

Consider the following useful theorem.

\begin{theorem}{Invertible Matrices}{invertiblematrices}
Let $A$ be an invertible $n \times n$ matrix. Then the columns of $A$ are independent and span $\mathbb{R}^n$. Similarly, the rows of $A$ are independent and span the set of all $1 \times n$ vectors. 
\end{theorem}

This theorem also allows us to determine if a matrix is invertible. If an $n \times n$ matrix $A$ has columns which are independent, or span $\mathbb{R}^n$, then it follows that $A$ is invertible. If it has rows that are independent, or span the set of all $1 \times n$ vectors, then $A$ is invertible.